{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "493fd120-59d3-4ae2-9f6a-7f43f07e5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, re, json\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#importing credentials\n",
    "with open('../../../Notes-General/config.txt', 'r') as f:\n",
    "    creds = f.read()\n",
    "creds = json.loads(creds)\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "browser_path = r\"..\\browsers\\geckodriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5d8a57-9f16-4b1f-a64b-ba76f4397b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column structures for stat projections\n",
    "projection_columns = [\"outlet\",\"date\", \"playerId\", \"name\", \"shortName\", \"pos\", \"team\", 'GamesPlayed',\n",
    " 'PassAttempts','PassCompletions','PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    " 'InterceptionsThrown', 'PasserRating',\n",
    " 'RushingAttempts','RushingYards', 'AverageYardsPerRush', 'RushingTouchdowns',\n",
    " 'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception','ReceivingTouchdowns',\n",
    " 'FumblesLost',\n",
    " 'FieldGoalsMade','FieldGoalAttempts','LongestFieldGoal','FieldGoals119Yards','FieldGoals119YardAttempts',\n",
    " 'FieldGoals2029Yards','FieldGoals2029YardAttempts','FieldGoals3039Yards','FieldGoals3039YardAttempts',\n",
    " 'FieldGoals4049Yards','FieldGoals4049YardAttempts','FieldGoals50Yards','FieldGoals50YardsAttempts',\n",
    " 'ExtraPointsMade','ExtraPointsAttempted',\n",
    " 'Interceptions','Safeties','Sacks','Tackles','DefensiveFumblesRecovered','ForcedFumbles','DefensiveTouchdowns', \n",
    " 'ReturnTouchdowns','PointsAllowed','PointsAllowedPerGame','NetPassingYardsAllowed','RushingYardsAllowed',\n",
    " 'TotalYardsAllowed', 'YardsAgainstPerGame','FantasyPoints','FantasyPointsPerGame']\n",
    "\n",
    "ranking_columns = [\"outlet\", \"date\", \"group\", \"expert\", \"rank\",\"name\",\"playerId\",\"team\",\"pos\"]\n",
    "\n",
    "adp_columns = ['outlet', 'date', 'playerId', 'name', 'shortName' , 'pos', 'team', 'adp', 'high', 'low']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c85bb-f992-462e-a2d2-8c385eac036e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- CBS - PPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5c759-c181-4c54-81e1-44b5d37e24e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### cbs projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07100377-0987-4797-a14e-98ff2b71d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_projections = \"https://www.cbssports.com/fantasy/football/stats/{pos}/{year}/restofseason/projections/ppr/\"\n",
    "year = 2024\n",
    "positions = [\"QB\", \"RB\", \"WR\", \"TE\", \"K\", \"DST\"]\n",
    "tableClass = \"TableBase-table\"  \n",
    "tableHeader = \"TableBase-headTr\"\n",
    "headerClass = \"Tablebase-tooltipInner\"\n",
    "tableRow = \"TableBase-bodyTr\"\n",
    "tableD = \"TableBase-bodyTd\"\n",
    "\n",
    "df_cbs_proj = pd.DataFrame(columns=projection_columns)\n",
    "\n",
    "# loop through each position to retrieve HTML and convert to df\n",
    "for p in range(len(positions)):\n",
    "    \n",
    "    time.sleep(3)\n",
    "    #updating URL for each position\n",
    "    url_formatted = url_projections.format(pos=positions[p], year=year)\n",
    "\n",
    "    # retreiving HTML and converting it to soup\n",
    "    r = requests.get(url_formatted)\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # accessing table with the data\n",
    "    table = soup.find(\"table\", class_= tableClass)\n",
    "\n",
    "    \n",
    "    # accounting for the difference in DEF headers\n",
    "    if positions[p] == \"DST\":\n",
    "        cols = [\"pos\", \"team\",\"name\"]\n",
    "    else:\n",
    "        cols = [\"playerId\", \"name\", \"shortName\", \"pos\", \"team\"]\n",
    "    \n",
    "    ### grabbing column names from the thead for the position. These will be used to create the temp. pos dataframe\n",
    "    #retrieving column names from the HTML\n",
    "    for i in table.find_all(\"div\", class_=headerClass):\n",
    "        cols.append(''.join(filter(str.isalnum, i.text)))\n",
    "        \n",
    "    # accessing the data in the body\n",
    "    body = table.find(\"tbody\")\n",
    "    # looping through rows\n",
    "    data = []\n",
    "    for tr in body.find_all(\"tr\", class_=tableRow):\n",
    "        # accounting for DST and populating pos as DST since it is not provided\n",
    "        if positions[p] == \"DST\":\n",
    "            player_data = [\"DST\"]\n",
    "        else:\n",
    "            player_data = []\n",
    "        \n",
    "        for td in tr.find_all(\"td\", class_=tableD):\n",
    "            \n",
    "            if positions[p] == \"DST\":\n",
    "                \n",
    "                span = td.find_all(\"span\",class_=\"CellLogoNameLockup\")\n",
    "                \n",
    "                if span:\n",
    "                    \n",
    "                    for s in span:\n",
    "                        player_data.append(s.find(\"a\")[\"href\"].split(\"/\")[3])\n",
    "                        player_data.append(str.strip(td.text))\n",
    "                \n",
    "                # non-span <Td>\n",
    "                else:\n",
    "                    player_data.append(str.strip(td.text))\n",
    "                    \n",
    "            # processing table body for all pos except DST\n",
    "            else:\n",
    "                #the player name, id, pos, and team are all in spans. the spans are not present in the stat <td>'s\n",
    "                span_short = td.find_all(\"span\",class_=\"CellPlayerName--short\")\n",
    "                span_long = td.find_all(\"span\",class_=\"CellPlayerName--long\")\n",
    "\n",
    "                # if the <td> has a span, the player info will be extracted\n",
    "                if span_long:\n",
    "\n",
    "                    for s in span_long:\n",
    "                        # player Id from the href url\n",
    "                        player_data.append(s.find(\"a\")[\"href\"].split(\"/\")[3])\n",
    "                        # player full name\n",
    "                        player_data.append(str.strip(s.find(\"a\").text).replace(\".\", \"\"))\n",
    "\n",
    "                    for s in span_short:\n",
    "                        # player short name\n",
    "                        player_data.append(s.find(\"a\").text.replace(\".\", \"\"))\n",
    "                        #player position\n",
    "                        player_data.append(str.strip(s.find(\"span\", class_=\"CellPlayerName-position\").text))\n",
    "                        #player nfl team\n",
    "                        player_data.append(str.strip(s.find(\"span\", class_=\"CellPlayerName-team\").text))\n",
    "            \n",
    "                # non-span <Td>\n",
    "                else:\n",
    "                    player_data.append(str.strip(td.text))\n",
    "        \n",
    "        # creates the list of players, each player is a list with stats\n",
    "        data.append(player_data)\n",
    "    \n",
    "    # converts list of list to data frame with the applicable columns pulled earlier\n",
    "    pos_df = pd.DataFrame(data, columns=cols)\n",
    "    \n",
    "    # concats all of the position data to the master df\n",
    "    df_cbs_proj = pd.concat([df_cbs_proj, pos_df], axis=0, ignore_index=True)\n",
    "\n",
    "df_cbs_proj.loc[:,'outlet'] = \"cbs\"\n",
    "df_cbs_proj.loc[:,'date'] = today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8000a90e-b09f-4a07-b0d6-14ef6bfdd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cbs_proj.to_excel(\"../data/projection/offseason/cbs_proj_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a430b-f61b-4e8b-8ba8-759a2ca73a19",
   "metadata": {
    "tags": []
   },
   "source": [
    "### cbs draft rankings ****NOT WORKING SINCE 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc4a5817-09b5-4d22-a8f6-8a0ee0a7702b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_rankings = \"https://www.cbssports.com/fantasy/football/rankings/ppr/{pos}/\"\n",
    "positions = [\"top200\",\"QB\", \"RB\", \"WR\", \"TE\", \"K\", \"DST\"]\n",
    "\n",
    "# key class names that will be targeted\n",
    "parentDivClass = \"rankings-table multi-authors  \"  # contains all expert rankings (3 tables)\n",
    "individualRankingDivClass = \"\"\" \t\t\t\t\texperts-column  \t\t\t\t\t\ttriple\t\"\"\"  # 3 of these for their 3 experts  \n",
    "authorNameAClass = \"author-name\"\n",
    "playersDivClass = \"player-wrapper\"  # the divs of interest are in here but it also includes data that is not needed \n",
    "\n",
    "df_cbs_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "for pos in positions:\n",
    "    time.sleep(3)    \n",
    "    # retreiving HTML and converting it to soup\n",
    "    url_formatted = url_rankings.format(pos=pos)\n",
    "    r = requests.get(url_formatted)\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # finding the tables with rankings\n",
    "    rankingTables = soup.find_all(\"div\", class_=individualRankingDivClass)\n",
    "    \n",
    "    # looping through the 3 expert ranks that are in their own tables\n",
    "    player_ranking_data = []\n",
    "    if pos == \"top200\":\n",
    "        continue\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "\n",
    "                temp = [\"cbs\", today, pos, expert]\n",
    "                try:\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"player-name\").text).replace(\".\", \"\"))  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[0])) # contains the player nfl team \n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[1])) # contains the player nfl position \n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    elif pos == \"DST\":\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, pos, expert]\n",
    "                try:\n",
    "                    team = str.strip(p.find(\"span\", class_=\"player-name\").text)\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(team)  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(team) # contains the player nfl team \n",
    "                    temp.append(pos) # contains the player nfl position \n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    else:\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, pos, expert]\n",
    "                try:\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"player-name\").text).replace(\".\", \"\"))  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[0])) # contains the player nfl team \n",
    "                    temp.append(pos) # contains the player nfl team \n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    # creating temp dataframe that includes all 3 expert rankings for a grouping to add to the master df \n",
    "    temp_df = pd.DataFrame(player_ranking_data, columns=ranking_columns)        \n",
    "    df_cbs_ranking = pd.concat([df_cbs_ranking, temp_df], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c079ff-817a-437b-a2fe-8d008eb391d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cbs_ranking.to_excel(\"Data/projection/offseason/cbs_rank_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74982928-e05f-4002-8e20-a5fc8b2b0a77",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CBS ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002b0efc-b0be-4dbf-88c5-76f5422f2abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cbs_adp_url = \"https://www.cbssports.com/fantasy/football/draft/averages/\"\n",
    "\n",
    "r = requests.get(cbs_adp_url)\n",
    "soup = bs(r.text)\n",
    "\n",
    "table = soup.find(\"table\", class_=\"TableBase-table\")\n",
    "body = table.find(\"tbody\")\n",
    "\n",
    "adps = []\n",
    "for tr in body.find_all(\"tr\"):\n",
    "    temp = []\n",
    "    \n",
    "    data = tr.find_all(\"td\")\n",
    "    \n",
    "    playerId = data[1].find(\"a\")[\"href\"].split(\"/\")[3]\n",
    "    shortName =  data[1].find(\"span\", class_=\"CellPlayerName--short\").text.split(\"\\n\")[0].replace(\".\", \"\")\n",
    "    fullName =  data[1].find(\"span\", class_=\"CellPlayerName--long\").text.split(\"\\n\")[0].replace(\".\", \"\")\n",
    "    pos = data[1].find(\"span\", class_=\"CellPlayerName-position\").text.strip()\n",
    "    team =  data[1].find(\"span\", class_=\"CellPlayerName-team\").text.strip()\n",
    "    \n",
    "    adp = data[3].text.strip()\n",
    "    \n",
    "    highLow = data[4].text.split(\"/\")\n",
    "    high = highLow[0].strip()\n",
    "    low = highLow[1]\n",
    "    \n",
    "    temp = [\"cbs\", today, playerId, fullName, shortName, pos, team, adp, high, low]\n",
    "    adps.append(temp)\n",
    "    \n",
    "df_cbs_adp = pd.DataFrame(adps, columns = adp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b1c50e-b529-44b4-879e-862629bb7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cbs_adp.to_excel(\"../data/projection/offseason/cbs_adp_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4be72-4592-4191-8e08-0cf285af7865",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- Fantasy Pros - HPPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288c4e0-fc76-42a4-9965-5d8228911798",
   "metadata": {},
   "source": [
    "They use CBS and ESPN for season stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adcebc-ed8b-424e-be41-bb0730ece2da",
   "metadata": {},
   "source": [
    "### Fantasy Pros Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d9ebc2-35d7-4afb-8384-f24357aa6b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_fp_rankings = r\"https://www.fantasypros.com/nfl/fantasy-football-rankings/{pos}.php\"\n",
    "fp_url_positions = {\"top500\":\"half-point-ppr-overall\", \"QB\":\"qb\", \"RB\":\"half-point-ppr-rb\", \n",
    "                    \"WR\":\"half-point-ppr-wr\", \"TE\":\"half-point-ppr-te\", \"K\":\"k\", \"DST\":\"dst\"}\n",
    "\n",
    "all_ranks = []    \n",
    "for k,v in fp_url_positions.items():  \n",
    "    time.sleep(3)\n",
    "    url_fp_formatted = url_fp_rankings.format(pos=v)\n",
    "    r = requests.get(url_fp_formatted)\n",
    "    soup = bs(r.text)\n",
    "    \n",
    "    # getting expert name and rank date\n",
    "    experts = []\n",
    "    for a in soup.find_all(\"th\", class_=\"expert__th\"):\n",
    "        temp = []\n",
    "        temp.append(a['data-sort-label'])   # expert name\n",
    "        temp.append(str.strip(a.find(\"div\", class_=\"expert__publish-date\").text))  #ranking publish date\n",
    "        experts.append(temp)\n",
    "\n",
    "    # getting player info and ranks\n",
    "    if k == \"DST\":\n",
    "        for p in soup.find_all(\"tr\", class_=\"player-row mpb-player__tr\"):\n",
    "            playerId = p[\"data-pid\"]  # fp playerid\n",
    "            shortName = p.find(\"span\", class_=\"mobile-only\").text.split()[0] # fp short name\n",
    "            #fullName = p.find(\"span\", class_=\"everything-but-mobile js-sort-field\").text # fp full name\n",
    "            TEAM = p.find(\"span\", class_=\"player__team\").text  # player nfl team\n",
    "            POS = p.find(\"span\", class_=\"player__position\").text  # player position\n",
    "\n",
    "            html_ranks = p.find_all(\"td\", attrs={'class': None})\n",
    "            for r in range(len(html_ranks)):\n",
    "                temp_fp_ranking = [shortName, playerId, TEAM, POS] \n",
    "\n",
    "                temp_fp_ranking.insert(0, html_ranks[r].text) # ranking\n",
    "                temp_fp_ranking.insert(0, experts[r][0]) # expert\n",
    "                temp_fp_ranking.insert(0, k) # group ranking set\n",
    "                temp_fp_ranking.insert(0, experts[r][1]) # date\n",
    "                temp_fp_ranking.insert(0, \"fantasyPros\") # outlet\n",
    "\n",
    "                all_ranks.append(temp_fp_ranking)\n",
    "    else:\n",
    "        for p in soup.find_all(\"tr\", class_=\"player-row mpb-player__tr\"):\n",
    "            playerId = p[\"data-pid\"]  # fp playerid\n",
    "            shortName = p.find(\"span\", class_=\"mobile-only\").text.replace(\".\", \"\") # fp short name\n",
    "            #fullName = p.find(\"span\", class_=\"everything-but-mobile js-sort-field\").text # fp full name\n",
    "            TEAM = p.find(\"span\", class_=\"player__team\").text  # player nfl team\n",
    "            POS = p.find(\"span\", class_=\"player__position\").text  # player position\n",
    "\n",
    "            html_ranks = p.find_all(\"td\", attrs={'class': None})\n",
    "            for r in range(len(html_ranks)):\n",
    "                temp_fp_ranking = [shortName, playerId, TEAM, POS] \n",
    "\n",
    "                temp_fp_ranking.insert(0, html_ranks[r].text) # ranking\n",
    "                temp_fp_ranking.insert(0, experts[r][0]) # expert\n",
    "                temp_fp_ranking.insert(0, k) # group ranking set\n",
    "                temp_fp_ranking.insert(0, experts[r][1]) # date\n",
    "                temp_fp_ranking.insert(0, \"fantasyPros\") # outlet\n",
    "\n",
    "                all_ranks.append(temp_fp_ranking)\n",
    "                \n",
    "df_fp_ranking = pd.DataFrame(all_ranks, columns=ranking_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e583f8f-d9c1-4d80-ac05-75eb64a0af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_ranking.to_excel(\"../data/projection/offseason/fp_rank_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab963ead-0784-4b5c-9b81-a6fdb71d7289",
   "metadata": {},
   "source": [
    "### Fantasy Pros ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2350b612-1096-4fd1-8428-18462157a8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp_adp_url = \"https://www.fantasypros.com/nfl/adp/half-point-ppr-overall.php\"\n",
    "\n",
    "r = requests.get(fp_adp_url)\n",
    "soup = bs(r.text)\n",
    "\n",
    "table = soup.find_all(\"table\")[0].find(\"tbody\")\n",
    "\n",
    "adps = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    \n",
    "    temp = []\n",
    "    data = tr.find_all(\"td\")\n",
    "    \n",
    "    fullName = data[1].find(\"a\", class_=\"player-name\").text.replace(\".\", \"\")\n",
    "    playerId = data[1].find(\"a\", class_=\"fp-player-link\")\n",
    "    \n",
    "    for c in data[1].find_all(class_=True):\n",
    "        classes = c['class']\n",
    "        if len(classes) > 1:\n",
    "            for i in classes:\n",
    "                if \"id\" in i:\n",
    "                    playerId = i.split(\"id-\")[1]\n",
    "                    \n",
    "    \n",
    "        \n",
    "    pos = re.search(pattern = r\"\\D*\", string=data[2].text)[0]\n",
    "    \n",
    "    if pos == 'DST':\n",
    "        team = fullName\n",
    "    else:\n",
    "        try:\n",
    "            team = data[1].find(\"small\").text\n",
    "        except:\n",
    "            team = \"FA\"\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # adding an entry for each sites adp. they are their own records\n",
    "    yahoo = data[3].text\n",
    "    temp = [\"yahoo\", today, playerId, fullName, np.nan, pos, team, yahoo, np.nan, np.nan]\n",
    "    adps.append(temp)\n",
    "    \n",
    "    fantrax = data[4].text\n",
    "    temp = [\"fantrax\", today, playerId, fullName, np.nan, pos, team, fantrax, np.nan, np.nan]\n",
    "    adps.append(temp)\n",
    "    \n",
    "    ffc = data[5].text\n",
    "    temp = [\"ffc\", today, playerId, fullName, np.nan, pos, team, ffc, np.nan, np.nan]\n",
    "    adps.append(temp)\n",
    "    \n",
    "    sleeper = data[6].text\n",
    "    temp = [\"sleeper\", today, playerId, fullName, np.nan, pos, team, sleeper, np.nan, np.nan]\n",
    "    adps.append(temp)\n",
    "    \n",
    "    #avg = data[7].text\n",
    "    \n",
    "    \n",
    "df_fp_adp = pd.DataFrame(adps, columns=adp_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da7051b-0d15-4e58-9c2f-52569143497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_adp.to_excel(\"../data/projection/offseason/fp_adp_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d912a6-b546-4b95-8b7c-0cce17a9d432",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- ESPN - HPPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1d7c0-c74f-4776-be69-ee20695b8476",
   "metadata": {},
   "source": [
    "### ESPN Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab5ff7-522e-4bc9-b70b-76b152527760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = Service(browser_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# open the initial projection page\n",
    "url_espn_proj = \"https://fantasy.espn.com/football/players/projections\"\n",
    "driver.get(url_espn_proj) \n",
    "# sleep to let the html load\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "try:\n",
    "    # changing to the desired projection view\n",
    "    button = driver.find_element(By.XPATH, \"//button[@class='Button Button--filter player--filters__projections-button']\")\n",
    "    button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # grabs the entire pages html\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "    \n",
    "    # grabbing the number of pages there are in the projections\n",
    "    pagenation_list = soup.find(\"div\", class_=\"Pagination__wrap overflow-x-auto\")\n",
    "    pages = pagenation_list.find_all(\"li\")\n",
    "    last_page = pages[-1].text\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "    \n",
    "\n",
    "espn_player_proj_player = []\n",
    "page_count1 = 0\n",
    "page_count2 = 0\n",
    "\n",
    "for page in range(1, int(last_page)+1):\n",
    "    try:\n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        # grabbing the projection tables\n",
    "        tables = soup.find_all(\"table\")\n",
    "        \n",
    "        # the player info table\n",
    "        for tr in tables[0].find_all(\"tr\"):\n",
    "            for td in tr:\n",
    "                if td.find(\"a\", class_=\"AnchorLink link clr-link pointer\"):\n",
    "                    #grabs the ESPN player id from the image url\n",
    "                    playerId = td.find(\"img\")['src'].split(\"/\")[-1].split(\".\")[0]\n",
    "                    name = td.find(\"a\", class_=\"AnchorLink link clr-link pointer\").text.replace(\".\", \"\")\n",
    "                    position = td.find(\"span\", class_=\"playerinfo__playerpos ttu\").text\n",
    "                    team = td.find(\"span\", class_=\"playerinfo__playerteam\").text\n",
    "\n",
    "                    espn_player_proj_player.append([\"espn\", today, playerId, name, np.nan, position, team, np.nan])\n",
    "\n",
    "\n",
    "        # the stat projection table\n",
    "        for tr in tables[1].find_all(\"tr\",class_=\"Table__TR Table__TR--lg Table__odd\"):\n",
    "            comp_att = tr.find(\"div\", {\"title\":\"Each Pass Completed & Each Pass Attempted\"}).text.split(\"/\")\n",
    "            pass_comps = comp_att[0]\n",
    "            pass_atts = comp_att[1]\n",
    "            pass_yds = tr.find(\"div\", {\"title\":\"Passing Yards\"}).text\n",
    "            pass_tds = tr.find(\"div\", {\"title\":\"TD Pass\"}).text\n",
    "            ints = tr.find(\"div\", {\"title\":\"Interceptions Thrown\"}).text\n",
    "            rush_atts = tr.find(\"div\", {\"title\":\"Rushing Attempts\"}).text\n",
    "            rush_yds = tr.find(\"div\", {\"title\":\"Rushing Yards\"}).text\n",
    "            rush_tds = tr.find(\"div\", {\"title\":\"TD Rush\"}).text\n",
    "            rec = tr.find(\"div\", {\"title\":\"Each reception\"}).text\n",
    "            rec_yds = tr.find(\"div\", {\"title\":\"Receiving Yards\"}).text\n",
    "            rec_tds = tr.find(\"div\", {\"title\":\"TD Reception\"}).text\n",
    "            rec_trgts = tr.find(\"div\", {\"title\":\"Receiving Target\"}).text\n",
    "            \n",
    "            espn_player_proj_player[page_count1].extend([pass_atts, pass_comps,pass_yds, 0, pass_tds,\n",
    "                                                         ints, 0, rush_atts,rush_yds,0, rush_tds,rec_trgts,rec,rec_yds,0,0,rec_tds,\n",
    "                                                        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "            page_count1 += 1\n",
    "       \n",
    "        # the fantasy points table\n",
    "        for tr in tables[2].find_all(\"tr\",class_=\"Table__TR Table__TR--lg Table__odd\"):\n",
    "            for div in tr.find_all(\"div\"):\n",
    "                # some of the free agents/retired players don't have div[\"title\"] need to catch them with try\n",
    "                try:\n",
    "                    if 'point' in div['title']:\n",
    "                        total_ff_pts = div.find(\"span\").text\n",
    "                    else:\n",
    "                        avg_ff_pts = div.find(\"span\").text\n",
    "                except:\n",
    "                    total_ff_pts = 0\n",
    "                    avg_ff_pts = 0\n",
    "\n",
    "            espn_player_proj_player[page_count2].extend([total_ff_pts, avg_ff_pts])\n",
    "            page_count2 += 1\n",
    "        \n",
    "        #checks for last page\n",
    "        \n",
    "        if page < int(last_page):\n",
    "            # jumping to the next page\n",
    "            nextButton = driver.find_element(By.XPATH, \"//button[@class='Button Button--default Button--icon-noLabel Pagination__Button Pagination__Button--next']\")\n",
    "            nextButton.click()\n",
    "            time.sleep(10)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        driver.close()\n",
    "\n",
    "try:\n",
    "    driver.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# creating df from gathered data to merge into final df that matches the cbs structure\n",
    "temp_proj = pd.DataFrame(espn_player_proj_player, columns = projection_columns)\n",
    "                        \n",
    "\n",
    "df_espn_proj = pd.DataFrame(columns = projection_columns)\n",
    "\n",
    "#final espn projections data\n",
    "df_espn_proj = pd.concat([df_espn_proj, temp_proj]).replace(\"--\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b0f49b6-b1fc-4d77-98f0-83f5bb7abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_espn_proj.to_excel(\"../data/projection/offseason/espn_proj_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2d618-1994-4c74-a640-78b30f3e1710",
   "metadata": {},
   "source": [
    "### ESPN Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61bcd408-9f70-40af-9739-e73fadc676b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "espn_ranking_urls = {\n",
    "\"QB\":\"https://www.espn.com/fantasy/football/story/_/id/36312955/nfl-fantasy-football-rankings-2024-qb-quarterback\",\n",
    "\"RB\":\"https://www.espn.com/fantasy/football/story/_/id/36313077/nfl-fantasy-football-rankings-2024-rb-running-back-ppr\",\n",
    "\"WR\":\"https://www.espn.com/fantasy/football/story/_/id/36313408/nfl-fantasy-football-rankings-2024-wr-wide-receiver-ppr\",\n",
    "\"TE\":\"https://www.espn.com/fantasy/football/story/_/id/36313475/nfl-fantasy-football-rankings-2024-te-tight-end-ppr\",\n",
    "\"K\":\"https://www.espn.com/fantasy/football/story/_/id/36313520/nfl-fantasy-football-rankings-2024-kicker-k\",\n",
    "\"DST\":\"https://www.espn.com/fantasy/football/story/_/id/36313516/nfl-fantasy-football-rankings-2024-dst-defense\",\n",
    "\"IDP\":\"https://www.espn.com/fantasy/football/story/_/id/36334764/2023-fantasy-football-rankings-idp-defense-defensive-line-linebacker-defensive-back\",\n",
    "\"top200\":\"https://www.espn.com/fantasy/football/story/_/id/36135778/fantasy-football-ppr-rankings-2024-quarterback-running-back-wide-receiver-tight-end-top-200\"\n",
    "}\n",
    "\n",
    "# final dataframe structure to hosue all the rankings\n",
    "df_espn_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "service = Service(browser_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# looping through the urls to aggregate the rankings\n",
    "for group, url in espn_ranking_urls.items():\n",
    "    # opening the webpage and allowing the scripts to load for the HTML to be accessed\n",
    "    \n",
    "    url_espn_formatted = url\n",
    "    driver.get(url_espn_formatted)\n",
    "    time.sleep(10)\n",
    "\n",
    "    # grabs the entire pages html\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "    \n",
    "    # this will hold a list of list. One list will be a players rank for a single expert\n",
    "    player_ranks = []\n",
    "\n",
    "    # the top 100 and 200 have different html structure\n",
    "    if group == \"top100\" or group == \"top200\":\n",
    "        \n",
    "        # grabbing the desired html chunk\n",
    "        ranking_table = soup.find(\"div\", class_=\"article-body\").find_all(\"p\")\n",
    "        #driver.close()\n",
    "        \n",
    "        \n",
    "        if group == \"top100\":\n",
    "            expert = \"Berry\"\n",
    "        else:\n",
    "            expert = \"Cockcroft\"\n",
    "        \n",
    "        # the <p> have <a> in them for each rank, this is looping through both to gather the ranks\n",
    "        rank = 1\n",
    "        for p in ranking_table[2:]:\n",
    "\n",
    "            test = p.find_all(\"a\")\n",
    "            if len(test) >= 10:\n",
    "                for a in test:\n",
    "                    fullName = a.text.replace(\".\", \"\")\n",
    "                    playerId = a['href'].split(\"/\")[7]\n",
    "\n",
    "                    player_ranks.append([\"espn\", today, group, expert, rank, fullName, playerId, np.nan,  np.nan])\n",
    "                    rank += 1\n",
    "    \n",
    "    # IDP page has 3 separate tables for positions instead of a single position on the page and a single table handled in the else below\n",
    "    elif group == \"IDP\":\n",
    "        ranking_tables = soup.find_all(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "        count = 0 # hard coded the positions based on the which table the site holds them in\n",
    "        \n",
    "        # 3 tables for the 3 IDPs  DL, LB, DB\n",
    "        for ranking_table in ranking_tables:\n",
    "\n",
    "            # retrieves the expert names and the order they are listed\n",
    "            expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "            expert_names = []\n",
    "            for tr in range(2, len(expert_names_html)):\n",
    "                expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "            player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "            for tr in player_ranks_html:\n",
    "\n",
    "                tds = tr.find_all(\"td\")\n",
    "\n",
    "                playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "                name = tds[0].find(\"a\").text.replace(\".\", \"\")\n",
    "\n",
    "                if count == 0:\n",
    "                    POS = \"DL\"\n",
    "                elif count == 1:\n",
    "                    POS = \"LB\"\n",
    "                elif count == 2:\n",
    "                    POS = \"DB\"\n",
    "\n",
    "                # try block to handle injury designations that the site puts in the same text as the team name\n",
    "                try:\n",
    "                    #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "                    injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "                    if len(injury) > 1:  # Accounts for suspended tag \"SSPD\"\n",
    "                        team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-4]\n",
    "                    else:\n",
    "                        team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-1]\n",
    "\n",
    "                except:\n",
    "                    team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "                for i in range(len(expert_names)):\n",
    "\n",
    "                    # expert name from the list generated from thead\n",
    "                    expert = expert_names[i]\n",
    "                    # position of the expert ranking column in tbody\n",
    "                    idx = i + 2\n",
    "\n",
    "                    # retrieves the expert rank from tbody rows\n",
    "                    exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "                    player_ranks.append([\"espn\", today, group, expert, exRank, name, playerId, team,  POS])\n",
    "\n",
    "            count += 1\n",
    "    \n",
    "    \n",
    "    # for position specific rankings\n",
    "    else:\n",
    "    \n",
    "        ranking_table = soup.find(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "        #driver.close()\n",
    "        \n",
    "        # retrieves the expert names and the order they are listed\n",
    "        expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "        expert_names = []\n",
    "        for tr in range(2, len(expert_names_html)):\n",
    "            expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "        player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "        for tr in player_ranks_html:\n",
    "\n",
    "            tds = tr.find_all(\"td\")\n",
    "\n",
    "            playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "            if group == \"DST\":\n",
    "                name = tds[0].find(\"a\").text.split()[0]\n",
    "            else:\n",
    "                name = tds[0].find(\"a\").text.replace(\".\", \"\")\n",
    "                \n",
    "            POS = group\n",
    "            \n",
    "            #team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "            # try block to handle injury designations that the site puts in the same text as the team name\n",
    "            try:\n",
    "                #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "                injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "                if len(injury) > 1:  # Accounts for suspended tag \"SSPD\"\n",
    "                    team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-4]\n",
    "                else:\n",
    "                    team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-1]\n",
    "\n",
    "            except:\n",
    "                team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "            for i in range(len(expert_names)-1):\n",
    "                \n",
    "                # expert name from the list generated from thead\n",
    "                expert = expert_names[i]\n",
    "                # position of the expert ranking column in tbody\n",
    "                idx = i + 2\n",
    "\n",
    "                # retrieves the expert rank from tbody rows\n",
    "                exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "                player_ranks.append([\"espn\", today, group, expert, exRank, name, playerId, team,  POS])\n",
    "\n",
    "    \n",
    "    temp_df = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "    df_espn_ranking = pd.concat([df_espn_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "driver.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b922ae-0c4b-44b3-a14a-9c2a6ad4d6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_espn_ranking.to_excel(\"../data/projection/offseason/espn_rank_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd44e8-2d9f-4012-bd81-0d32467805df",
   "metadata": {},
   "source": [
    "### ESPN ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b72f38-29eb-45ed-8d0c-533d93e1a5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = Service(browser_path)\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# open the initial adp page\n",
    "espn_adp_url = \"https://fantasy.espn.com/football/livedraftresults\"\n",
    "driver.get(espn_adp_url) \n",
    "# sleep to let the html load\n",
    "time.sleep(10)\n",
    "\n",
    "html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "soup = bs(html)\n",
    "\n",
    "table = soup.find(\"tbody\", class_=\"Table__TBODY\")\n",
    "\n",
    "adps = []\n",
    "\n",
    "for n in range(10):\n",
    "    \n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        temp = []\n",
    "        data = tr.find_all(\"td\")\n",
    "\n",
    "        fullName = data[1].find(\"a\", class_=\"AnchorLink link clr-link pointer\").text.replace(\".\", \"\")\n",
    "        pos = data[1].find(\"span\", class_=\"playerinfo__playerpos\").text.replace(\"/\",\"\")\n",
    "        try:\n",
    "            team = data[1].find(\"span\", class_=\"playerinfo__playerteam\").text\n",
    "        except:\n",
    "            team = \"FA\"\n",
    "        adp = data[2].text\n",
    "        \n",
    "        if pos == \"DST\":\n",
    "            playerId = \"\"\n",
    "        else:\n",
    "            playerId  = data[1].find('img', src=True)['src'].split(\"/\")[10].split(\".\")[0]\n",
    "\n",
    "        temp = [\"espn\", today, playerId, fullName, np.nan, pos, team, adp, np.nan, np.nan]\n",
    "        adps.append(temp)\n",
    "    \n",
    "    # looping over the pages for ADP\n",
    "    button = driver.find_element(By.XPATH, \"//button[@class='Button Button--default Button--icon-noLabel Pagination__Button Pagination__Button--next']\")\n",
    "    button.click()\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # grabs the entire pages html for the new page and sets it for the next scrap iteration\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "    \n",
    "    table = soup.find(\"tbody\", class_=\"Table__TBODY\")\n",
    "\n",
    "driver.close()\n",
    "df_espn_adp = pd.DataFrame(adps, columns=adp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e54a7a9d-d334-4576-8b59-55241308ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_espn_adp.to_excel(\"../data/projection/offseason/espn_adp_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ba735-05c5-468d-b4a1-e6aef0481c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ------------ NFL --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d90e5-0fae-485b-9960-34d458d51f47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NFL projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f062d3-340a-4dc9-bc85-ae588209cb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_2340\\1279835996.py:176: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_nfl_proj = pd.DataFrame(player_data, columns=projection_columns).replace(\"-\",0)\n"
     ]
    }
   ],
   "source": [
    "# position=  0:QB,RB,WR,TE  7:Kicker, 8:D\n",
    "nfl_proj_url = [\n",
    "\"https://fantasy.nfl.com/research/projections?offset={}&position=0&statCategory=projectedStats&statSeason=2024&statType=seasonProjectedStats&statWeek=1\",\n",
    "\"https://fantasy.nfl.com/research/projections?offset={}&position=7&statCategory=projectedStats&statSeason=2024&statType=seasonProjectedStats&statWeek=1\",    \n",
    "\"https://fantasy.nfl.com/research/projections?offset={}&position=8&statCategory=projectedStats&statSeason=2024&statType=seasonProjectedStats&statWeek=1\"\n",
    "]\n",
    "\n",
    "df_nfl_proj = pd.DataFrame(columns=projection_columns)\n",
    "player_data = []\n",
    "# count will be updated to the player count after the first page load \n",
    "# this is being used to avoid loading more pages than needed\n",
    "count = 3000\n",
    "\n",
    "#looping through the 3 URLs, the site has QB,RB,WR,TE combined in a single list and then K and D on their own pages\n",
    "for i in range(3):\n",
    "    if i == 0:  # this will handle the offensive players\n",
    "        while count > 25:\n",
    "            if count == 3000:\n",
    "                time.sleep(1)\n",
    "                r = requests.get(nfl_proj_url[0].format(1))\n",
    "                soup = bs(r.text)\n",
    "\n",
    "                # grabs the number of players with projections on the site. pagenated at 25 a page\n",
    "                player_count = int(soup.find(\"span\", class_=\"paginationTitle\").text.split(\"of\")[-1].strip())\n",
    "                count = player_count\n",
    "\n",
    "                table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "\n",
    "                body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                for tr in body_trs:\n",
    "                    data = tr.find_all(\"td\")\n",
    "\n",
    "                    firstColA = data[0].find('a')\n",
    "                    playerId = firstColA['href'].split(\"=\")[2]\n",
    "                    fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                    posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                    pos = posAndTeam[0].strip()\n",
    "                    try:\n",
    "                        team = posAndTeam[1].strip()\n",
    "                    except:\n",
    "                        team = \"FA\"\n",
    "\n",
    "                    gp = data[2].text\n",
    "                    PassingYards = data[3].text\n",
    "                    TouchdownsPasses = data[4].text\n",
    "                    InterceptionsThrown = data[5].text\n",
    "                    RushingYards = data[6].text\n",
    "                    RushingTouchdowns = data[7].text\n",
    "                    Receptions = data[8].text\n",
    "                    ReceivingYards = data[9].text\n",
    "                    ReceivingTouchdowns = data[10].text\n",
    "                    retTd = data[11].text\n",
    "                    fumTd = data[12].text\n",
    "                    twoPt= data[13].text\n",
    "                    FumblesLost = data[14].text\n",
    "                    FantasyPoints = data[15].text\n",
    "\n",
    "                    temp = [\"nfl\", today, playerId,fullName,np.nan,pos,team,gp,0,0,PassingYards,0,TouchdownsPasses, InterceptionsThrown,\n",
    "                            0,0,RushingYards,0,RushingTouchdowns,0,Receptions,ReceivingYards,0,0,ReceivingTouchdowns,\n",
    "                            FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,FantasyPoints,0]\n",
    "                    player_data.append(temp)\n",
    "\n",
    "            else:\n",
    "                for j in range(26, player_count, 25):\n",
    "                    time.sleep(1)\n",
    "                    r = requests.get(nfl_proj_url[0].format(j))\n",
    "                    soup = bs(r.text)\n",
    "                    table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "                    body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                    for tr in body_trs:\n",
    "                        data = tr.find_all(\"td\")\n",
    "\n",
    "                        firstColA = data[0].find('a')\n",
    "                        playerId = firstColA['href'].split(\"=\")[2]\n",
    "                        fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                        posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                        pos = posAndTeam[0].strip()\n",
    "                        try:\n",
    "                            team = posAndTeam[1].strip()\n",
    "                        except:\n",
    "                            team = \"FA\"\n",
    "\n",
    "                        gp = data[2].text\n",
    "                        PassingYards = data[3].text\n",
    "                        TouchdownsPasses = data[4].text\n",
    "                        InterceptionsThrown = data[5].text\n",
    "                        RushingYards = data[6].text\n",
    "                        RushingTouchdowns = data[7].text\n",
    "                        Receptions = data[8].text\n",
    "                        ReceivingYards = data[9].text\n",
    "                        ReceivingTouchdowns = data[10].text\n",
    "                        retTd = data[11].text\n",
    "                        fumTd = data[12].text\n",
    "                        twoPt= data[13].text\n",
    "                        FumblesLost = data[14].text\n",
    "                        FantasyPoints = data[15].text\n",
    "\n",
    "                        temp = [\"nfl\", today, playerId,fullName,np.nan,pos,team,gp,0,0,PassingYards,0,TouchdownsPasses, InterceptionsThrown,\n",
    "                                0,0,RushingYards,0,RushingTouchdowns,0,Receptions,ReceivingYards,0,0,ReceivingTouchdowns,\n",
    "                                FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,FantasyPoints,0]\n",
    "                        player_data.append(temp)\n",
    "\n",
    "                    count -= 25\n",
    "                    \n",
    "    else: # this will handle K and D\n",
    "        for j in range(2):  \n",
    "            \n",
    "            time.sleep(1)\n",
    "            r = requests.get(nfl_proj_url[i].format(j*25+1))  # k and d only have 2 pages, j *25 + 1 handles the url offset that pagenates\n",
    "            soup = bs(r.text)\n",
    "\n",
    "            table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "\n",
    "            body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "            for tr in body_trs:\n",
    "                data = tr.find_all(\"td\")\n",
    "                temp = []\n",
    "                \n",
    "                firstColA = data[0].find('a')\n",
    "                playerId = firstColA['href'].split(\"=\")[2]\n",
    "                fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                pos = posAndTeam[0].strip()\n",
    "                \n",
    "                if i == 1:  # K url\n",
    "                    try:\n",
    "                        team = posAndTeam[1].strip()\n",
    "                    except:\n",
    "                        team = \"FA\"\n",
    "                        \n",
    "                    gp = data[2].text\n",
    "                    xpMade = data[3].text\n",
    "                    made0_19 = data[4].text\n",
    "                    made20_29 = data[5].text\n",
    "                    made30_39 = data[6].text\n",
    "                    made40_49 = data[7].text\n",
    "                    made50 = data[8].text\n",
    "                    fgMade = made0_19 + made20_29 + made30_39 + made40_49 + made50\n",
    "                    FantasyPoints = data[9].text\n",
    "\n",
    "                    temp = [\"nfl\", today, playerId,fullName,np.nan,pos,team,gp,0,0,0,0,0, 0,\n",
    "                                    0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "                                    fgMade,0,0,made0_19,0,made20_29,0,made30_39,0,made40_49,0,made50,0,xpMade,0,0,\n",
    "                                     0,0,0,0,0,0,0,0,0,0,0,0,0,FantasyPoints,0]\n",
    "                    player_data.append(temp)    \n",
    "                        \n",
    "                else: # D url\n",
    "                    \n",
    "                    team = fullName\n",
    "                    gp = data[2].text\n",
    "                    sacks = data[3].text\n",
    "                    interceptions = data[4].text\n",
    "                    fum = data[5].text\n",
    "                    safety = data[6].text\n",
    "                    defTd = data[7].text\n",
    "                    twoPt = data[8].text\n",
    "                    retTd = data[9].text\n",
    "                    ptsAllowed= data[10].text\n",
    "                    fantasyPts= data[11].text\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    temp = [\"nfl\", today, playerId,np.nan, np.nan,pos,team,gp,0,0,0,0,0,0,\n",
    "                            0,0,0,0,0,0,0,0,0,0,0,\n",
    "                            FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,interceptions,safety,sacks,0,fum,0,\n",
    "                            defTd, retTd,ptsAllowed,0,0,0,0,0,fantasyPts,0]\n",
    "                    \n",
    "                    player_data.append(temp)\n",
    "        \n",
    "df_nfl_proj = pd.DataFrame(player_data, columns=projection_columns).replace(\"-\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0785e8c9-b294-44a8-82f6-2df4392579a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_proj.to_excel(\"../data/projection/offseason/nfl_proj_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72bb1a-23e1-42d0-b858-a87f7d5280be",
   "metadata": {},
   "source": [
    "### NFL rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "305c60d3-1ed1-4744-9f54-9ed6ab23e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_rank_url = {\n",
    "    \"QB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=QB&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"RB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=RB&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"WR\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=WR&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"TE\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=TE&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"K\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=K&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"DEF\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=DEF&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"DL\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=DL&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"LB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=LB&sort=1&statSeason=2024&statType=seasonStats\",\n",
    "    \"DB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=DB&sort=1&statSeason=2024&statType=seasonStats\"\n",
    "}\n",
    "\n",
    "df_nfl_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "for k,v in nfl_rank_url.items():\n",
    "    \n",
    "    time.sleep(1)\n",
    "    r = requests.get(nfl_rank_url[k])\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # grabs the number of players with projections on the site. pagenated at 25 a page\n",
    "    rank_table = soup.find(\"table\", class_=\"tableType-player noGroups\").find(\"tbody\")\n",
    "\n",
    "    player_ranks = []\n",
    "    for tr in rank_table.find_all(\"tr\"):\n",
    "        player_data = []\n",
    "        td = tr.find_all(\"td\")\n",
    "        \n",
    "        pos_rank = int(td[0].text)\n",
    "        playerId = int(td[1].find(\"a\")['href'].split(\"=\")[-1])\n",
    "        full_name = td[1].find(\"a\").text.replace(\".\", \"\")\n",
    "        \n",
    "        pos = td[1].find(\"em\").text.split(\"-\")[0].strip()\n",
    "        \n",
    "        if k == \"DEF\":\n",
    "            team = \"\"\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # no team name for FAs\n",
    "            try:\n",
    "                team = td[1].find(\"em\").text.split(\"-\")[1].strip()\n",
    "            except:\n",
    "                team = \"FA\"\n",
    "            \n",
    "        ovr_rank = int(td[-1].text)\n",
    "\n",
    "        player_data = [\"nfl\", today, k, \"nfl\", pos_rank, full_name, playerId, team, pos ]\n",
    "        player_ranks.append(player_data)\n",
    "\n",
    "    temp_df = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "    df_nfl_ranking = pd.concat([df_nfl_ranking, temp_df], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dab273ae-b631-44ab-86f4-a7a690845559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_ranking.to_excel(\"../data/projection/offseason/nfl_rank_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043567c6-54a4-4054-af36-2e135ee6c181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# bettingpros PLAYER TOTAL PROPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c6630-cdae-4529-a72c-6bc4fecfbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#######  THIS DIDN\"T WORK AFTER AUGUST 2022\n",
    "#############################################\n",
    "#########\n",
    "################################################\n",
    "\n",
    "service = Service(r\"C:\\Users\\jrbrz\\Desktop\\projects\\projects\\ffDraft\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "overUnder_urls = {\n",
    "    \"RByds\":\"https://www.bettingpros.com/nfl/picks/prop-bets/bet/rushing-yards/\",\n",
    "    \"RBtds\":\"https://www.bettingpros.com/nfl/picks/prop-bets/bet/rushing-touchdowns/\",\n",
    "    \"WRyds\":\"https://www.bettingpros.com/nfl/picks/prop-bets/bet/receiving-yards/\",\n",
    "    \"WRtds\":\"https://www.bettingpros.com/nfl/picks/prop-bets/bet/receiving-touchdowns/\"\n",
    "}\n",
    "\n",
    "overUnders = []\n",
    "for k,v in overUnder_urls.items():\n",
    "    # open the initial adp page\n",
    "    driver.get(v) \n",
    "    # sleep to let the html load\n",
    "    time.sleep(10)\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "    \n",
    "    table = soup.find(\"table\").find(\"tbody\")\n",
    "\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        temp = []\n",
    "        \n",
    "        data = tr.find_all(\"td\")\n",
    "\n",
    "        pos = data[1].find(\"div\", class_=\"yearbook-block__description\").text.strip().split(\" - \")[1].strip()\n",
    "\n",
    "        if ((pos == \"RB\") and ((k == \"RByds\") or (k == \"RBtds\"))) or ((pos == \"WR\") and ((k == \"WRyds\") or (k == \"WRtds\"))):\n",
    "\n",
    "            full = data[1].find(\"span\", class_=\"yearbook-block__title--block player-name player-name--desktop\").text.strip().replace(\".\", \"\")\n",
    "            short = data[1].find(\"span\", class_=\"yearbook-block__title--block player-name player-name--mobile\").text.strip().replace(\".\", \"\")\n",
    "            last = data[1].find(\"span\", class_=\"yearbook-block__title--block player-name\").text.strip().replace(\".\", \"\")\n",
    "            \n",
    "            for c in data[1].find_all(class_=True):\n",
    "                classes = c['class']\n",
    "                if len(classes) > 1:\n",
    "                    for i in classes:\n",
    "                        if \"id-\" in i:\n",
    "                            playerId = i.split(\"id-\")[1]\n",
    "                            \n",
    "            \n",
    "            fullName = full + \" \" + last\n",
    "            shortName = short + \" \" + last\n",
    "\n",
    "            team = data[1].find(\"div\", class_=\"yearbook-block__description\").text.strip().split(\" - \")[0].strip()\n",
    "\n",
    "            overUnder = data[4].text.strip().split()[0]\n",
    "            projectedTotal = data[5].text\n",
    "            \n",
    "            temp = [\"bettingPros\", today, k, playerId, fullName, shortName, pos, team, overUnder, projectedTotal]\n",
    "            overUnders.append(temp)\n",
    "\n",
    "driver.close()\n",
    "df_overUnders = pd.DataFrame(overUnders, columns=[\"outlet\", \"date\", \"category\", \"playerId\", \"name\", \"shortName\", \"pos\", \"team\", \"ou\", \"projected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f810570-8e95-4cd0-b34a-8093e54cbbe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_overUnders.to_excel(\"Data/overUnder_{}.xlsx\".format(str(today)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e126a9c-f967-482c-8a69-a884ab6abd79",
   "metadata": {},
   "source": [
    "# <<< SCRATCH >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680a31b-cf25-442f-9a66-315a0b8a0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 2022\n",
    "week = 1\n",
    "service = Service(r\"C:\\Users\\jrbrz\\Desktop\\projects\\projects\\ffDraft\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "urls = {\"overunder\": r'https://www.bettingpros.com/nfl/odds/spread/?season={season}&week={week}',\n",
    "\"moneyline\":r\"https://www.bettingpros.com/nfl/odds/moneyline/?season={season}&week={week}\",\n",
    "\"totals\":r\"https://www.bettingpros.com/nfl/odds/total/?season={season}&week={week}\"}\n",
    "           \n",
    "lines = []\n",
    "for k,v in overUnder_urls.items():\n",
    "    driver.get(ou_url.format(season=season, week=week)) \n",
    "    # sleep to let the html load\n",
    "    time.sleep(10)\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "\n",
    "    data = soup.find_all(\"div\", class_=\"flex odds-offer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27ab05-f80b-4f1e-ae37-5088bc652c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ea648-7781-4bbf-a907-1ea8a333c1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = Service(r\"C:\\Users\\jrbrz\\Desktop\\projects\\projects\\ffDraft\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "driver.get(\"https://www.espn.com/fantasy/football/story/_/id/33898295/fantasy-football-idp-rankings-2022-top-50-defensive-linemen-linebackers-defensive-backs\")\n",
    "time.sleep(10)\n",
    "\n",
    "# grabs the entire pages html\n",
    "html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "soup = bs(html)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52206c-c740-44a2-86d6-b67d1ad56a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player_ranks = []\n",
    "ranking_tables = soup.find_all(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "count = 0\n",
    "for ranking_table in ranking_tables:\n",
    "        \n",
    "    # retrieves the expert names and the order they are listed\n",
    "    expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "    expert_names = []\n",
    "    for tr in range(2, len(expert_names_html)):\n",
    "        expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "    player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "    for tr in player_ranks_html:\n",
    "\n",
    "        tds = tr.find_all(\"td\")\n",
    "\n",
    "        playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "        if group == \"DST\":\n",
    "            name = tds[0].find(\"a\").text.split()[0]\n",
    "        else:\n",
    "            name = tds[0].find(\"a\").text\n",
    "            \n",
    "        if count == 0:\n",
    "            POS = \"DL\"\n",
    "        elif count == 1:\n",
    "            POS = \"LB\"\n",
    "        elif count == 2:\n",
    "            POS = \"DB\"\n",
    "        \n",
    "        # try block to handle injury designations that the site puts in the same text as the team name\n",
    "        try:\n",
    "            #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "            injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "            team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper().replace(injury, \"\")\n",
    "\n",
    "        except:\n",
    "            team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "        for i in range(len(expert_names)):\n",
    "\n",
    "            # expert name from the list generated from thead\n",
    "            expert = expert_names[i]\n",
    "            # position of the expert ranking column in tbody\n",
    "            idx = i + 2\n",
    "\n",
    "            # retrieves the expert rank from tbody rows\n",
    "            exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "            player_ranks.append([today, group, expert, exRank, name, playerId, team,  POS])\n",
    "            \n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fafe6-c0a1-4c16-b7cf-a453a848aa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fantasyfootball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
