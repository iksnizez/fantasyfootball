{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bddad12-2646-4263-aada-1655c0c5d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, re, os, json\n",
    "import pymysql, pyodbc\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import date\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#importing credentials\n",
    "with open('../../../Notes-General/config.txt', 'r') as f:\n",
    "    creds = f.read()\n",
    "creds = json.loads(creds)\n",
    "league = 'nfl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf3688b-7dfc-4ef5-b989-ce31a711c5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-01 Week: 18\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "season = 2024\n",
    "week = int(input(\"What week is it >>>>>> ? \"))\n",
    "if week < 10:\n",
    "    strWeek = \"0\" + str(week)\n",
    "else:\n",
    "    strWeek = week\n",
    "print(today, \"Week:\", week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd2a5d2-4375-41f1-a832-8777b6abbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column structures for stat projections\n",
    "projection_columns = [\"outlet\",\"date\", \"season\", \"week\", \"playerId\", \"name\", \"shortName\", \"pos\", \"team\", 'GamesPlayed',\n",
    " 'PassAttempts','PassCompletions','PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    " 'InterceptionsThrown', 'PasserRating',\n",
    " 'RushingAttempts','RushingYards', 'AverageYardsPerRush', 'RushingTouchdowns',\n",
    " 'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception','ReceivingTouchdowns',\n",
    " 'FumblesLost',\n",
    " 'FieldGoalsMade','FieldGoalAttempts','LongestFieldGoal','FieldGoals119Yards','FieldGoals119YardAttempts',\n",
    " 'FieldGoals2029Yards','FieldGoals2029YardAttempts','FieldGoals3039Yards','FieldGoals3039YardAttempts',\n",
    " 'FieldGoals4049Yards','FieldGoals4049YardAttempts','FieldGoals50Yards','FieldGoals50YardsAttempts',\n",
    " 'ExtraPointsMade','ExtraPointsAttempted',\n",
    " 'Interceptions','Safeties','Sacks','Tackles','DefensiveFumblesRecovered','ForcedFumbles','DefensiveTouchdowns', \n",
    " 'ReturnTouchdowns','PointsAllowed','PointsAllowedPerGame','NetPassingYardsAllowed','RushingYardsAllowed', \n",
    " 'TotalYardsAllowed', 'YardsAgainstPerGame', 'twoPt','FantasyPoints','FantasyPointsPerGame']\n",
    "\n",
    "ranking_columns = [\"outlet\", \"date\", \"season\", \"week\", \"group\", \"expert\", \"rank\",\"name\",\"playerId\",\"team\",\"pos\", \"high\", \"low\"]\n",
    "\n",
    "team_map = {\n",
    "    \"Jacksonville Jaguars\":\"JAX\",\"Los Angeles Rams\":\"LA\",\"Philadelphia Eagles\":\"PHI\",\"Minnesota Vikings\":\"MIN\",\n",
    "    \"Houston Texans\":\"HOU\",\"Los Angeles Chargers\":\"LAC\",\"Baltimore Ravens\":\"BAL\",\"New England Patriots\":\"NE\",\n",
    "    \"Carolina Panthers\":\"CAR\",\"Denver Broncos\":\"DEN\",\"Arizona Cardinals\":\"ARI\",\"New Orleans Saints\":\"NO\",\n",
    "    \"Detroit Lions\":\"DET\",\"Pittsburgh Steelers\":\"PIT\",\"Chicago Bears\":\"CHI\",\"Seattle Seahawks\":\"SEA\",\n",
    "    \"Buffalo Bills\":\"BUF\",\"Tennessee Titans\":\"TEN\",\"Atlanta Falcons\":\"ATL\",\"Cincinnati Bengals\":\"CIN\",\n",
    "    \"Kansas City Chiefs\":\"KC\",\"Washington Redskins\":\"WAS\",\"Dallas Cowboys\":\"DAL\",\"Tampa Bay Buccaneers\":\"TB\",\n",
    "    \"Green Bay Packers\":\"GB\",\"New York Giants\":\"NYG\",\"San Francisco 49ers\":\"SF\",\"Cleveland Browns\":\"CLE\",\n",
    "    \"Oakland Raiders\":\"OAK\",\"Indianapolis Colts\":\"IND\",\"Miami Dolphins\":\"MIA\",\"New York Jets\":\"NYJ\"}\n",
    "team_mascot_map = {\n",
    "    \"Jaguars\":\"JAX\",\"Rams\":\"LA\",\"Eagles\":\"PHI\",\"Vikings\":\"MIN\",\n",
    "    \"Texans\":\"HOU\",\"Chargers\":\"LAC\",\"Ravens\":\"BAL\",\"Patriots\":\"NE\",\n",
    "    \"Panthers\":\"CAR\",\"Broncos\":\"DEN\",\"Cardinals\":\"ARI\",\"Saints\":\"NO\",\n",
    "    \"Lions\":\"DET\",\"Steelers\":\"PIT\",\"Bears\":\"CHI\",\"Seahawks\":\"SEA\",\n",
    "    \"Bills\":\"BUF\",\"Titans\":\"TEN\",\"Falcons\":\"ATL\",\"Bengals\":\"CIN\",\n",
    "    \"Chiefs\":\"KC\",\"Redskins\":\"WAS\",\"Cowboys\":\"DAL\",\"Buccaneers\":\"TB\",\n",
    "    \"Packers\":\"GB\",\"Giants\":\"NYG\",\"49ers\":\"SF\",\"Browns\":\"CLE\",\n",
    "    \"Raiders\":\"OAK\",\"Colts\":\"IND\",\"Dolphins\":\"MIA\",\"Jets\":\"NYJ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccecebe4-4180-47dc-8cfe-71525d2ad042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 18 18\n"
     ]
    }
   ],
   "source": [
    "print(season, week, strWeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c85bb-f992-462e-a2d2-8c385eac036e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- CBS - PPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5c759-c181-4c54-81e1-44b5d37e24e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### cbs projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07100377-0987-4797-a14e-98ff2b71d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_projections = \"https://www.cbssports.com/fantasy/football/stats/{pos}/{season}/{week}/projections/ppr/\"\n",
    "positions = [\"QB\", \"RB\", \"WR\", \"TE\", \"K\", \"DST\"]\n",
    "tableClass = \"TableBase-table\"  \n",
    "tableHeader = \"TableBase-headTr\"\n",
    "headerClass = \"Tablebase-tooltipInner\"\n",
    "tableRow = \"TableBase-bodyTr\"\n",
    "tableD = \"TableBase-bodyTd\"\n",
    "\n",
    "df_cbs_proj = pd.DataFrame(columns=projection_columns)\n",
    "\n",
    "# loop through each position to retrieve HTML and convert to df\n",
    "for p in range(len(positions)):\n",
    "    \n",
    "    time.sleep(3)\n",
    "    #updating URL for each position\n",
    "    url_formatted = url_projections.format(pos=positions[p], season=season, week=week)\n",
    "\n",
    "    # retreiving HTML and converting it to soup\n",
    "    r = requests.get(url_formatted)\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # accessing table with the data\n",
    "    table = soup.find(\"table\", class_= tableClass)\n",
    "\n",
    "    \n",
    "    # accounting for the difference in DEF headers\n",
    "    if positions[p] == \"DST\":\n",
    "        cols = [\"pos\", \"team\",\"name\"]\n",
    "    else:\n",
    "        cols = [\"playerId\", \"name\", \"shortName\", \"pos\", \"team\"]\n",
    "    \n",
    "    ### grabbing column names from the thead for the position. These will be used to create the temp. pos dataframe\n",
    "    #retrieving column names from the HTML\n",
    "    for i in table.find_all(\"div\", class_=headerClass):\n",
    "        cols.append(''.join(filter(str.isalnum, i.text)))\n",
    "        \n",
    "    # accessing the data in the body\n",
    "    body = table.find(\"tbody\")\n",
    "    # looping through rows\n",
    "    data = []\n",
    "    for tr in body.find_all(\"tr\", class_=tableRow):\n",
    "        # accounting for DST and populating pos as DST since it is not provided\n",
    "        if positions[p] == \"DST\":\n",
    "            player_data = [\"DST\"]\n",
    "        else:\n",
    "            player_data = []\n",
    "        \n",
    "        for td in tr.find_all(\"td\", class_=tableD):\n",
    "            \n",
    "            if positions[p] == \"DST\":\n",
    "            # pulling team name \n",
    "                span = td.find_all(\"span\",class_=\"CellLogoNameLockup\")\n",
    "                if span:   \n",
    "                    for s in span:\n",
    "                        player_data.append(s.find(\"a\")[\"href\"].split(\"/\")[3])\n",
    "                        player_data.append(str.strip(td.text))\n",
    "                \n",
    "                # non-span <Td>, \n",
    "                else:\n",
    "                    player_data.append(str.strip(td.text))\n",
    "                    \n",
    "            # processing table body for all pos except DST\n",
    "            else:\n",
    "                #the player name, id, pos, and team are all in spans. \n",
    "                #the spans are not present in the stat <td>'s\n",
    "                span_short = td.find_all(\"span\",class_=\"CellPlayerName--short\")\n",
    "                span_long = td.find_all(\"span\",class_=\"CellPlayerName--long\")\n",
    "\n",
    "                # if the <td> has a span, the player info will be extracted\n",
    "                if span_long:\n",
    "\n",
    "                    for s in span_long:\n",
    "                        # player Id from the href url\n",
    "                        player_data.append(s.find(\"a\")[\"href\"].split(\"/\")[3])\n",
    "                        # player full name\n",
    "                        player_data.append(str.strip(s.find(\"a\").text).replace(\".\", \"\"))\n",
    "\n",
    "                    for s in span_short:\n",
    "                        # player short name\n",
    "                        player_data.append(s.find(\"a\").text.replace(\".\", \"\"))\n",
    "                        #player position\n",
    "                        player_data.append(str.strip(s.find(\"span\", class_=\"CellPlayerName-position\").text))\n",
    "                        #player nfl team\n",
    "                        player_data.append(str.strip(s.find(\"span\", class_=\"CellPlayerName-team\").text).replace(\"JAC\", \"JAX\").replace(\"WAS\", \"WSH\"))\n",
    "            \n",
    "                # non-span <Td>\n",
    "                else:\n",
    "                    player_data.append(str.strip(td.text))\n",
    "        \n",
    "        # creates the list of players, each player is a list with stats\n",
    "        data.append(player_data)\n",
    "    \n",
    "    # converts list of list to data frame with the applicable columns pulled earlier\n",
    "    pos_df = pd.DataFrame(data, columns=cols)\n",
    "    \n",
    "    # concats all of the position data to the master df\n",
    "    df_cbs_proj = pd.concat([df_cbs_proj, pos_df], axis=0, ignore_index=True)\n",
    "\n",
    "df_cbs_proj.loc[:,'outlet'] = \"cbs\"\n",
    "df_cbs_proj.loc[:,'date'] = today\n",
    "df_cbs_proj.loc[:,'season'] = season\n",
    "df_cbs_proj.loc[:,'week'] = week\n",
    "df_cbs_proj.loc[:,'LongestFieldGoal'] = np.nan\n",
    "\n",
    "df_cbs_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8000a90e-b09f-4a07-b0d6-14ef6bfdd525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cbs_proj.to_csv(\"../data/projection/weekly/cbs_proj_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a430b-f61b-4e8b-8ba8-759a2ca73a19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### cbs rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4a5817-09b5-4d22-a8f6-8a0ee0a7702b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'span'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rt \u001b[38;5;129;01min\u001b[39;00m rankingTables:\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;66;03m#extracting expert name\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m         expert \u001b[38;5;241m=\u001b[39m \u001b[43mrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauthorNameAClass\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;66;03m#looping through the divs that contain all the player level ranking data\u001b[39;00m\n\u001b[0;32m     73\u001b[0m         ranks \u001b[38;5;241m=\u001b[39m rt\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39mplayersDivClass)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'span'"
     ]
    }
   ],
   "source": [
    "url_rankings = \"https://www.cbssports.com/fantasy/football/rankings/ppr/{pos}/\"#/weekly/\"\n",
    "\n",
    "positions = [\"QB\", \"RB\", \"WR\", \"TE\", \"K\", \"DST\", \"FLEX\"]\n",
    "\n",
    "# key class names that will be targeted\n",
    "parentDivClass = \"rankings-table multi-authors hide-player-stats\"  # contains all expert rankings (3 tables)\n",
    "individualRankingDivClass = \"experts-column triple\"  # 3 of these for their 3 experts  \n",
    "authorNameAClass = \"author-name\"\n",
    "playersDivClass = \"player-wrapper\"  # the divs of interest are in here but it also includes data that is not needed \n",
    "\n",
    "df_cbs_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "for pos in positions:\n",
    "    time.sleep(3)    \n",
    "    # retreiving HTML and converting it to soup\n",
    "    url_formatted = url_rankings.format(pos=pos)\n",
    "    r = requests.get(url_formatted)\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # finding the tables with rankings\n",
    "    rankingTables = soup.find_all(\"div\", class_=individualRankingDivClass)\n",
    "\n",
    "    # looping through the 3 expert ranks that are in their own tables\n",
    "    player_ranking_data = []\n",
    "    if pos == \"FLEX\":\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, season, week, pos, expert]\n",
    "                try:\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"player-name\").text).replace(\".\", \"\"))  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[0])) # contains the player nfl team \n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[1])) # contains the player nfl position\n",
    "                    temp.append(np.nan)\n",
    "                    temp.append(np.nan)\n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    elif pos == \"DST\":\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, season, week, pos, expert]\n",
    "                try:\n",
    "                    team = str.strip(p.find(\"span\", class_=\"player-name\").text)\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(team)  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(team) # contains the player nfl team \n",
    "                    temp.append(pos) # contains the player nfl position\n",
    "                    temp.append(np.nan)\n",
    "                    temp.append(np.nan)\n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    else:\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, season, week, pos, expert]\n",
    "                try:\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"player-name\").text).replace(\".\", \"\"))  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[3])) # cbs playerId is in the url\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[0])) # contains the player nfl team \n",
    "                    temp.append(pos) # contains the player nfl team\n",
    "                    temp.append(np.nan)\n",
    "                    temp.append(np.nan)\n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    # creating temp dataframe that includes all 3 expert rankings for a grouping to add to the master df \n",
    "    temp_df = pd.DataFrame(player_ranking_data, columns=ranking_columns)        \n",
    "    df_cbs_ranking = pd.concat([df_cbs_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "    \n",
    "df_cbs_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c079ff-817a-437b-a2fe-8d008eb391d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cbs_ranking.to_csv(\"../data/ranking/weekly/cbs_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4be72-4592-4191-8e08-0cf285af7865",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- Fantasy Pros - HPPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288c4e0-fc76-42a4-9965-5d8228911798",
   "metadata": {},
   "source": [
    "They use CBS and ESPN for season stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adcebc-ed8b-424e-be41-bb0730ece2da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fantasy Pros Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f16806-64a0-459c-8faf-9996b8fb0a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fantasy Pros ECRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916dea94-2c3e-448a-84b7-0234bed10961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1909, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "waitTime = 15\n",
    "\n",
    "df_ecr_ranks = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "urls = [\n",
    "    r\"https://www.fantasypros.com/nfl/rankings/half-point-ppr-{}\",\n",
    "    r\"https://www.fantasypros.com/nfl/rankings/{}\"\n",
    "] \n",
    "# try to close driver if there are any errors\n",
    "try:\n",
    "    for i in range(len(urls)):\n",
    "        if i == 0:\n",
    "            pos = ['SUPERFLEX', 'FLEX', 'TE', 'WR', 'RB']\n",
    "            for j in pos:\n",
    "                player_ranks = []\n",
    "                driver.get(urls[i].format(j.lower()))\n",
    "\n",
    "                # Accepting cookies if there is a popup\n",
    "                try:\n",
    "                    WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[contains(text(), 'Accept Cookies')]\"))\n",
    "                    cookies = driver.find_element(\"xpath\", '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "                    cookies.click()\n",
    "\n",
    "                except: pass\n",
    "\n",
    "                #driver.execute_script('videos = document.querySelectorAll(\"video\"); for(video of videos) {video.pause()}')\n",
    "\n",
    "                # select drop down that defaults to Overview and selecting Ranks\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                # grab all html\n",
    "                html = driver.page_source\n",
    "                soup = bs(html, 'lxml')  #parse the html\n",
    "\n",
    "                table = soup.find(\"table\", id='ranking-table').find(\"tbody\")\n",
    "                ranks = table.find_all(\"tr\")\n",
    "\n",
    "                for tr in ranks:\n",
    "                    tds = tr.find_all(\"td\")\n",
    "                    #for td in tds:\n",
    "                        #print(td.text)\n",
    "\n",
    "                    # some of the ecr defensive groups have teams in the rankings this will skip them\n",
    "                    name = tds[2].text.split(\"(\")[0].strip().replace(\".\", \"\")\n",
    "                    if name in list(team_map.keys()):\n",
    "                        continue\n",
    "\n",
    "                    if j in ['SUPERFLEX', 'FLEX']:\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[4].text\n",
    "                        low = tds[5].text\n",
    "                    else:\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[4].text\n",
    "                        low = tds[5].text\n",
    "\n",
    "\n",
    "                    player_ranks.append(['fantasyPros', today, season, week, j, 'ecr', rank, name, player, team, np.nan,  high, low])\n",
    "\n",
    "                temp = pd.DataFrame(player_ranks,  columns=ranking_columns)\n",
    "                df_ecr_ranks = pd.concat([df_ecr_ranks, temp])\n",
    "\n",
    "        if i == 1:\n",
    "            pos = ['QB', 'K', 'DST', 'IDP', 'DL' ,'LB', 'DB']\n",
    "            for j in pos:\n",
    "                player_ranks = []\n",
    "                driver.get(urls[i].format(j.lower()))\n",
    "\n",
    "                # Accepting cookies if there is a popup\n",
    "                try:\n",
    "                    WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[contains(text(), 'Accept')]\"))\n",
    "                    cookies = driver.find_element(\"xpath\", \"//button[contains(text(), 'Accept')]\")\n",
    "                    cookies.click()\n",
    "\n",
    "                except: pass\n",
    "\n",
    "                # select drop down that defaults to Overview and selecting Ranks\n",
    "                                                            # actual buttom xpath //*[@id=\"onetrust-accept-btn-handler\"]\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\"))\n",
    "                \n",
    "                drop = driver.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\")\n",
    "\n",
    "                drop.click()\n",
    "\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                # grab all html\n",
    "                html = driver.page_source\n",
    "                soup = bs(html, 'lxml')  #parse the html\n",
    "\n",
    "                table = soup.find(\"table\", id='ranking-table').find(\"tbody\")\n",
    "                ranks = table.find_all(\"tr\")\n",
    "\n",
    "                for tr in ranks:\n",
    "                    tds = tr.find_all(\"td\")\n",
    "\n",
    "                    # some of the ecr defensive groups have teams in the rankings this will skip them\n",
    "                    name = tds[2].text.split(\"(\")[0].strip().replace(\".\", \"\")\n",
    "                    if name in list(team_map.keys()):\n",
    "                        continue\n",
    "\n",
    "                    if j == 'IDP':\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[4].text\n",
    "                        low = tds[5].text\n",
    "\n",
    "                    else:\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[3].text\n",
    "                        low = tds[4].text\n",
    "\n",
    "                    player_ranks.append(['fantasyPros', today, season, week, j, 'ecr', rank, name, player, team, np.nan,  high, low])\n",
    "\n",
    "                temp = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "                df_ecr_ranks = pd.concat([df_ecr_ranks, temp])\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "    \n",
    "driver.close()\n",
    "df_ecr_ranks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1047980-f15f-4094-9a3c-bd53c42552ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecr_ranks.to_csv(\"../data/ranking/weekly/fpEcr_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da74d6-09b8-4a61-91f5-550c68441509",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fantasy Pros 5 Expert Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d9ebc2-35d7-4afb-8384-f24357aa6b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4816, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_fp_rankings = r\"https://www.fantasypros.com/nfl/fantasy-football-rankings/{pos}.php\"\n",
    "                  \n",
    "fp_url_positions = {\"QB\":\"weekly-qb\", \n",
    "                    \"RB\":\"weekly-half-point-ppr-rb\",\n",
    "                    \"WR\":\"weekly-half-point-ppr-wr\", \n",
    "                    \"TE\":\"weekly-half-point-ppr-te\", \n",
    "                    \"K\":\"weekly-k\", \n",
    "                    \"DST\":\"weekly-dst\",\n",
    "                    \"FLEX\":\"weekly-half-point-ppr-flex\", \n",
    "                    \"SUPERFLEX\":\"weekly-half-point-ppr-superflex\"}\n",
    "\n",
    "all_ranks = []\n",
    "\n",
    "for k,v in fp_url_positions.items(): \n",
    "    \n",
    "    time.sleep(3)\n",
    "    url_fp_formatted = url_fp_rankings.format(pos=v)\n",
    "    r = requests.get(url_fp_formatted)\n",
    "    soup = bs(r.text)\n",
    "    \n",
    "    # getting expert name and rank date\n",
    "    experts = []\n",
    "    for a in soup.find_all(\"th\", class_=\"expert__th\"):\n",
    "        temp = []\n",
    "        temp.append(a['data-sort-label'])   # expert name\n",
    "        temp.append(str.strip(a.find(\"div\", class_=\"expert__publish-date\").text))  #ranking publish date\n",
    "        experts.append(temp)\n",
    "\n",
    "    # getting player info and ranks\n",
    "    if k == \"DST\":\n",
    "        for p in soup.find_all(\"tr\", class_=\"player-row mpb-player__tr\"):\n",
    "            playerId = p[\"data-pid\"]  # fp playerid\n",
    "            shortName = p.find(\"span\", class_=\"mobile-only\").text.split()[0].replace(\".\", \"\") # fp short name\n",
    "            #fullName = p.find(\"span\", class_=\"everything-but-mobile js-sort-field\").text # fp full name\n",
    "            TEAM = p.find(\"span\", class_=\"player__team\").text  # player nfl team\n",
    "            POS = p.find(\"span\", class_=\"player__position\").text  # player position\n",
    "\n",
    "            html_ranks = p.find_all(\"td\", attrs={'class': None})\n",
    "            for r in range(len(html_ranks)):\n",
    "                temp_fp_ranking = [shortName, playerId, TEAM, POS] \n",
    "\n",
    "                temp_fp_ranking.insert(0, html_ranks[r].text) # ranking\n",
    "                temp_fp_ranking.insert(0, experts[r][0]) # expert\n",
    "                temp_fp_ranking.insert(0, k) # group ranking set\n",
    "                temp_fp_ranking.insert(0, week) #season week\n",
    "                temp_fp_ranking.insert(0, season) #season year\n",
    "                temp_fp_ranking.insert(0, experts[r][1]) # date\n",
    "                temp_fp_ranking.insert(0, \"fantasyPros\") # outlet\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                all_ranks.append(temp_fp_ranking)\n",
    "    \n",
    "    else:\n",
    "        for p in soup.find_all(\"tr\", class_=\"player-row mpb-player__tr\"):\n",
    "            playerId = p[\"data-pid\"]  # fp playerid\n",
    "            shortName = p.find(\"span\", class_=\"mobile-only\").text.replace(\".\", \"\") # fp short name\n",
    "            #fullName = p.find(\"span\", class_=\"everything-but-mobile js-sort-field\").text # fp full name\n",
    "            TEAM = p.find(\"span\", class_=\"player__team\").text  # player nfl team\n",
    "            POS = p.find(\"span\", class_=\"player__position\").text  # player position\n",
    "\n",
    "            html_ranks = p.find_all(\"td\", attrs={'class': None})\n",
    "            for r in range(len(html_ranks)):\n",
    "                temp_fp_ranking = [shortName, playerId, TEAM, POS] \n",
    "\n",
    "                temp_fp_ranking.insert(0, html_ranks[r].text) # ranking\n",
    "                temp_fp_ranking.insert(0, experts[r][0]) # expert\n",
    "                temp_fp_ranking.insert(0, k) # group ranking set\n",
    "                temp_fp_ranking.insert(0, week) #season week\n",
    "                temp_fp_ranking.insert(0, season) #season year\n",
    "                temp_fp_ranking.insert(0, experts[r][1]) # date\n",
    "                temp_fp_ranking.insert(0, \"fantasyPros\") # outlet\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                all_ranks.append(temp_fp_ranking)\n",
    "                \n",
    "df_fp_ranking = pd.DataFrame(all_ranks, columns=ranking_columns).replace(\"-\", np.nan).replace(\"N/A\", np.nan)\n",
    "df_fp_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e583f8f-d9c1-4d80-ac05-75eb64a0af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_ranking.to_csv(\"../data/ranking/weekly/fp_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d912a6-b546-4b95-8b7c-0cce17a9d432",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- ESPN - HPPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1d7c0-c74f-4776-be69-ee20695b8476",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ESPN Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ab5ff7-522e-4bc9-b70b-76b152527760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# open the initial projection page\n",
    "url_espn_proj = \"https://fantasy.espn.com/football/players/projections?leagueFormatId=3\"\n",
    "                \n",
    "driver.get(url_espn_proj) \n",
    "# sleep to let the html load\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "try:\n",
    "    # changing to the desired projection view\n",
    "    button = driver.find_element(By.XPATH, \"//button[@class='Button Button--filter player--filters__projections-button']\")\n",
    "    button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # grabs the entire pages html\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "    \n",
    "    # grabbing the number of pages there are in the projections\n",
    "    pagenation_list = soup.find(\"div\", class_=\"Pagination__wrap overflow-x-auto\")\n",
    "    pages = pagenation_list.find_all(\"li\")\n",
    "    last_page = pages[-1].text\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "    \n",
    "\n",
    "espn_player_proj_player = []\n",
    "page_count1 = 0\n",
    "page_count2 = 0\n",
    "\n",
    "for page in range(1, 13):  #int(last_page)+1):\n",
    "    try:\n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        # grabbing the projection tables\n",
    "        tables = soup.find_all(\"table\")\n",
    "        \n",
    "        # the player info table\n",
    "        for tr in tables[0].find_all(\"tr\"):\n",
    "            for td in tr:\n",
    "                if td.find(\"a\", class_=\"AnchorLink link clr-link pointer\"):\n",
    "                    \n",
    "                    #grabs the ESPN player id from the image url\n",
    "                    playerId = td.find(\"img\")['src'].split(\"/\")[-1].split(\".\")[0]\n",
    "                    #dst has player ID as the team abbreviation. This catches it\n",
    "                    try:\n",
    "                        int(playerId)\n",
    "                    except:\n",
    "                        playerId = \"\"\n",
    "                        \n",
    "                    name = td.find(\"a\", class_=\"AnchorLink link clr-link pointer\").text.replace(\".\", \"\")\n",
    "                    position = td.find(\"span\", class_=\"playerinfo__playerpos ttu\").text.replace(\"/\",\"\").split(\",\")[0]\n",
    "                    team = td.find(\"span\", class_=\"playerinfo__playerteam\").text.upper()\n",
    "\n",
    "                    espn_player_proj_player.append([\"espn\", today, season, week, playerId, name, np.nan, position, team, np.nan])\n",
    "\n",
    "\n",
    "        # the stat projection table\n",
    "        for tr in tables[1].find_all(\"tr\",class_=\"Table__TR Table__TR--lg Table__odd\"):\n",
    "            comp_att = tr.find(\"div\", {\"title\":\"Each Pass Completed & Each Pass Attempted\"}).text.split(\"/\")\n",
    "            pass_comps = comp_att[0]\n",
    "            pass_atts = comp_att[1]\n",
    "            pass_yds = tr.find(\"div\", {\"title\":\"Passing Yards\"}).text\n",
    "            pass_tds = tr.find(\"div\", {\"title\":\"TD Pass\"}).text\n",
    "            ints = tr.find(\"div\", {\"title\":\"Interceptions Thrown\"}).text\n",
    "            rush_atts = tr.find(\"div\", {\"title\":\"Rushing Attempts\"}).text\n",
    "            rush_yds = tr.find(\"div\", {\"title\":\"Rushing Yards\"}).text\n",
    "            rush_tds = tr.find(\"div\", {\"title\":\"TD Rush\"}).text\n",
    "            rec = tr.find(\"div\", {\"title\":\"Each reception\"}).text\n",
    "            rec_yds = tr.find(\"div\", {\"title\":\"Receiving Yards\"}).text\n",
    "            rec_tds = tr.find(\"div\", {\"title\":\"TD Reception\"}).text\n",
    "            rec_trgts = tr.find(\"div\", {\"title\":\"Receiving Target\"}).text\n",
    "            \n",
    "            espn_player_proj_player[page_count1].extend([pass_atts, pass_comps,pass_yds, 0, pass_tds,\n",
    "                                                         ints, 0, rush_atts,rush_yds,0, rush_tds,rec_trgts,rec,rec_yds,0,0,rec_tds,\n",
    "                                                        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "            page_count1 += 1\n",
    "       \n",
    "        # the fantasy points table\n",
    "                                        \n",
    "        for tr in tables[2].find_all(\"tr\",class_=\"Table__TR Table__TR--lg Table__odd\"):\n",
    "            for div in tr.find_all(\"div\"):\n",
    "                # some of the free agents/retired players don't have div[\"title\"] need to catch them with try\n",
    "                try:\n",
    "                    if 'point' in div['title']:\n",
    "                        total_ff_pts = div.find(\"span\").text\n",
    "                        avg_ff_pts = 0\n",
    "                except:\n",
    "                    total_ff_pts = 0\n",
    "                    avg_ff_pts = 0\n",
    "\n",
    "            espn_player_proj_player[page_count2].extend([total_ff_pts, avg_ff_pts])\n",
    "            page_count2 += 1\n",
    "        \n",
    "        #checks for last page\n",
    "        \n",
    "        if page < int(last_page):\n",
    "            # jumping to the next page\n",
    "            nextButton = driver.find_element(By.XPATH, \"//button[@class='Button Button--default Button--icon-noLabel Pagination__Button Pagination__Button--next']\")\n",
    "            nextButton.click()\n",
    "            time.sleep(10)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        driver.close()\n",
    "\n",
    "try:\n",
    "    driver.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# creating df from gathered data to merge into final df that matches the cbs structure\n",
    "temp_proj = pd.DataFrame(espn_player_proj_player, columns = projection_columns)\n",
    "                        \n",
    "\n",
    "df_espn_proj = pd.DataFrame(columns = projection_columns)\n",
    "\n",
    "#final espn projections data\n",
    "df_espn_proj = pd.concat([df_espn_proj, temp_proj]).replace(\"--\", 0)\n",
    "df_espn_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0f49b6-b1fc-4d77-98f0-83f5bb7abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_espn_proj.to_csv(\"../data/projection/weekly/espn_proj_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2d618-1994-4c74-a640-78b30f3e1710",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ESPN Rankings  - need to update urls for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bcd408-9f70-40af-9739-e73fadc676b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11316\\780950751.py:148: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_espn_ranking = pd.concat([df_espn_ranking, temp_df], axis = 0, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1850, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "espn_ranking_urls = {\n",
    "#\"QB\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankQBPPR/nfl-fantasy-football-rankings-2024-qb-quarterback\",\n",
    "\"QB\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24QB-41029053/nfl-fantasy-football-rankings-2024-qb-quarterback\",\n",
    "#\"RB\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankRBPPR/nfl-fantasy-football-rankings-2024-rb-running-back\",\n",
    "\"RB\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24RB-41030431/nfl-fantasy-football-rankings-2024-rb-running-back\",\n",
    "#\"WR\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankWRPPR/nfl-fantasy-football-rankings-2024-wr-wide-receiver\",\n",
    "\"WR\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24WR-41030448/nfl-fantasy-football-rankings-2024-wr-wide-receiver\",\n",
    "#\"TE\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankTEPPR/nfl-fantasy-football-rankings-2024-te-tight-end\",\n",
    "\"TE\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24TE-41030543/nfl-fantasy-football-rankings-2024-te-tight-end\",\n",
    "#\"K\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankKPPR/nfl-fantasy-football-rankings-2024-kicker\",\n",
    "\"K\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24K-41030564/nfl-fantasy-football-rankings-2024-k-kicker\",\n",
    "#\"DST\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankDSTPPR/nfl-fantasy-football-rankings-2024-dst-defense\",\n",
    "\"DST\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24DST-41030584/nfl-fantasy-football-rankings-2024-dst-defense\",\n",
    "#\"IDP\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankIDP/nfl-fantasy-football-rankings-2024-idp-defense-dl-lb-db\",\n",
    "\"IDP\":\"https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24IDP-41031094/nfl-fantasy-football-rankings-2024-idp-defense-dl-lb-db\"\n",
    "}\n",
    "\n",
    "# final dataframe structure to hosue all the rankings\n",
    "df_espn_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "#try to closed driver if there are any errors\n",
    "try:\n",
    "    # looping through the urls to aggregate the rankings\n",
    "    for group, url in espn_ranking_urls.items():\n",
    "        # opening the webpage and allowing the scripts to load for the HTML to be accessed\n",
    "\n",
    "        url_espn_formatted = url.format(season=season)\n",
    "        driver.get(url_espn_formatted)\n",
    "        time.sleep(10)\n",
    "\n",
    "        # grabs the entire pages html\n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        # this will hold a list of list. One list will be a players rank for a single expert\n",
    "        player_ranks = []\n",
    "\n",
    "        # IDP page has 3 separate tables for positions instead of a single position on the page and a single table handled in the else below\n",
    "        if group == \"IDP\":\n",
    "            ranking_tables = soup.find_all(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "            count = 0 # hard coded the positions based on the which table the site holds them in\n",
    "\n",
    "            # 3 tables for the 3 IDPs  DL, LB, DB\n",
    "            for ranking_table in ranking_tables:\n",
    "\n",
    "                # retrieves the expert names and the order they are listed\n",
    "                expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "                expert_names = []\n",
    "                for tr in range(2, len(expert_names_html)-1):\n",
    "                    expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "                player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "                for tr in player_ranks_html:\n",
    "\n",
    "                    tds = tr.find_all(\"td\")\n",
    "\n",
    "                    playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "                    name = tds[0].find(\"a\").text.replace(\".\", \"\")\n",
    "\n",
    "                    if count == 0:\n",
    "                        POS = \"DL\"\n",
    "                    elif count == 1:\n",
    "                        POS = \"LB\"\n",
    "                    elif count == 2:\n",
    "                        POS = \"DB\"\n",
    "\n",
    "                    # try block to handle injury designations that the site puts in the same text as the team name\n",
    "                    try:\n",
    "                        #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "                        injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "                        if len(injury) > 1:  # Accounts for suspended tag \"SSPD\"\n",
    "                            team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-4]\n",
    "                        else:\n",
    "                            team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-1]\n",
    "\n",
    "                    except:\n",
    "                        team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "                    for i in range(len(expert_names)):\n",
    "\n",
    "                        # expert name from the list generated from thead\n",
    "                        expert = expert_names[i]\n",
    "                        # position of the expert ranking column in tbody\n",
    "                        idx = i + 2\n",
    "\n",
    "                        # retrieves the expert rank from tbody rows\n",
    "                        exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "                        player_ranks.append([\"espn\", today, season, week, POS, expert, exRank, name, playerId, team,  POS, np.nan, np.nan])\n",
    "\n",
    "                count += 1\n",
    "\n",
    "\n",
    "        # for position specific rankings\n",
    "        else:\n",
    "\n",
    "            ranking_table = soup.find(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "            #driver.close()\n",
    "\n",
    "            # retrieves the expert names and the order they are listed\n",
    "            expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "            expert_names = []\n",
    "            for tr in range(2, len(expert_names_html)-1):\n",
    "                expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "            player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "            for tr in player_ranks_html:\n",
    "\n",
    "                tds = tr.find_all(\"td\")\n",
    "\n",
    "                playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "                if group == \"DST\":\n",
    "                    name = tds[0].find(\"a\").text.split()[0].replace(\".\", \"\")\n",
    "                else:\n",
    "                    name = tds[0].find(\"a\").text.replace(\".\", \"\")\n",
    "\n",
    "                POS = group\n",
    "\n",
    "                #team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "                # try block to handle injury designations that the site puts in the same text as the team name\n",
    "                try:\n",
    "                    #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "                    injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "                    if len(injury) > 1:  # Accounts for suspended tag \"SSPD\"\n",
    "                        team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-4]\n",
    "                    else:\n",
    "                        team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-1]\n",
    "\n",
    "                except:\n",
    "                    team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "                for i in range(len(expert_names)-1):\n",
    "\n",
    "                    # expert name from the list generated from thead\n",
    "                    expert = expert_names[i]\n",
    "                    # position of the expert ranking column in tbody\n",
    "                    idx = i + 2\n",
    "\n",
    "                    # retrieves the expert rank from tbody rows\n",
    "                    exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "                    player_ranks.append([\"espn\", today, season, week, group, expert, exRank, name, playerId, team,  POS, np.nan, np.nan])\n",
    "\n",
    "\n",
    "        temp_df = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "        df_espn_ranking = pd.concat([df_espn_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()    \n",
    "    \n",
    "driver.close() \n",
    "df_espn_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48b922ae-0c4b-44b3-a14a-9c2a6ad4d6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_espn_ranking.to_csv(\"../data/ranking/weekly/espn_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ba735-05c5-468d-b4a1-e6aef0481c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ------------ NFL --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d90e5-0fae-485b-9960-34d458d51f47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NFL projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f062d3-340a-4dc9-bc85-ae588209cb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1106, 60)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# position=  0:QB,RB,WR,TE  7:Kicker, 8:D\n",
    "nfl_proj_url = [\n",
    "\"https://fantasy.nfl.com/research/projections?offset={offset}&position=0&statCategory=projectedStats&statSeason={season}&statType=weekProjectedStats&statWeek={week}\",\n",
    "\"https://fantasy.nfl.com/research/projections?offset={offset}&position=7&statCategory=projectedStats&statSeason={season}&statType=weekProjectedStats&statWeek={week}\",\n",
    "\"https://fantasy.nfl.com/research/projections?offset={offset}&position=8&statCategory=projectedStats&statSeason={season}&statType=weekProjectedStats&statWeek={week}\",\n",
    "]\n",
    "\n",
    "\n",
    "df_nfl_proj = pd.DataFrame(columns=projection_columns)\n",
    "player_data = []\n",
    "# count will be updated to the player count after the first page load \n",
    "# this is being used to avoid loading more pages than needed\n",
    "count = 3000\n",
    "\n",
    "#looping through the 3 URLs, the site has QB,RB,WR,TE combined in a single list and then K and D on their own pages\n",
    "for i in range(3):\n",
    "    if i == 0:  # this will handle the offensive players\n",
    "        while count > 25:\n",
    "            \n",
    "            # this grabs the first page, else will handle all others\n",
    "            if count == 3000:\n",
    "                time.sleep(1)\n",
    "                r = requests.get(nfl_proj_url[0].format(offset=1, season=season, week=week))\n",
    "                soup = bs(r.text)\n",
    "\n",
    "                # grabs the number of players with projections on the site. pagenated at 25 a page\n",
    "                player_count = int(soup.find(\"span\", class_=\"paginationTitle\").text.split(\"of\")[-1].strip())\n",
    "                count = player_count\n",
    "\n",
    "                table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "\n",
    "                body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                for tr in body_trs:\n",
    "                    data = tr.find_all(\"td\")\n",
    "\n",
    "                    firstColA = data[0].find('a')\n",
    "                    playerId = firstColA['href'].split(\"=\")[2]\n",
    "                    fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                    posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                    pos = posAndTeam[0].strip()\n",
    "                    try:\n",
    "                        team = posAndTeam[1].strip()\n",
    "                    except:\n",
    "                        team = \"FA\"\n",
    "\n",
    "                    \n",
    "                    PassingYards = data[2].text\n",
    "                    TouchdownsPasses = data[3].text\n",
    "                    InterceptionsThrown = data[4].text\n",
    "                    RushingYards = data[5].text\n",
    "                    RushingTouchdowns = data[6].text\n",
    "                    Receptions = data[7].text\n",
    "                    ReceivingYards = data[8].text\n",
    "                    ReceivingTouchdowns = data[9].text\n",
    "                    retTd = data[10].text\n",
    "                    fumTd = data[11].text\n",
    "                    twoPt= data[12].text\n",
    "                    FumblesLost = data[13].text\n",
    "                    FantasyPoints = data[14].text\n",
    "\n",
    "                    temp = [\"nfl\", today, season, week, playerId,fullName,np.nan,pos,team,0,0,0,PassingYards,0,TouchdownsPasses, InterceptionsThrown,\n",
    "                            0,0,RushingYards,0,RushingTouchdowns,0,Receptions,ReceivingYards,0,0,ReceivingTouchdowns,\n",
    "                            FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,twoPt,FantasyPoints,0]\n",
    "                    player_data.append(temp)\n",
    "\n",
    "            else:\n",
    "                for j in range(26, player_count, 25):\n",
    "                    time.sleep(1)\n",
    "                    r = requests.get(nfl_proj_url[0].format(offset=j, season=season, week=week))\n",
    "                    soup = bs(r.text)\n",
    "                    table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "                    body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                    for tr in body_trs:\n",
    "                        data = tr.find_all(\"td\")\n",
    "\n",
    "                        firstColA = data[0].find('a')\n",
    "                        playerId = firstColA['href'].split(\"=\")[2]\n",
    "                        fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                        posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                        pos = posAndTeam[0].strip()\n",
    "                        try:\n",
    "                            team = posAndTeam[1].strip()\n",
    "                        except:\n",
    "                            team = \"FA\"\n",
    "\n",
    "                        PassingYards = data[2].text\n",
    "                        TouchdownsPasses = data[3].text\n",
    "                        InterceptionsThrown = data[4].text\n",
    "                        RushingYards = data[5].text\n",
    "                        RushingTouchdowns = data[6].text\n",
    "                        Receptions = data[7].text\n",
    "                        ReceivingYards = data[8].text\n",
    "                        ReceivingTouchdowns = data[9].text\n",
    "                        retTd = data[10].text\n",
    "                        fumTd = data[11].text\n",
    "                        twoPt= data[12].text\n",
    "                        FumblesLost = data[13].text\n",
    "                        FantasyPoints = data[14].text\n",
    "\n",
    "                        temp = [\"nfl\", today, season, week, playerId,fullName,np.nan,pos,team,0,0,0,PassingYards,0,TouchdownsPasses, InterceptionsThrown,\n",
    "                                0,0,RushingYards,0,RushingTouchdowns,0,Receptions,ReceivingYards,0,0,ReceivingTouchdowns,\n",
    "                                FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,twoPt,FantasyPoints,0]\n",
    "                        player_data.append(temp)\n",
    "\n",
    "                    count -= 25\n",
    "                    \n",
    "    else: # this will handle K and D\n",
    "        for j in range(2):  \n",
    "            \n",
    "            time.sleep(1)\n",
    "            r = requests.get(nfl_proj_url[i].format(offset=j*25+1, season=season, week=week))  # k and d only have 2 pages, j *25 + 1 handles the url offset that pagenates\n",
    "            soup = bs(r.text)\n",
    "\n",
    "            table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "\n",
    "            body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "            for tr in body_trs:\n",
    "                data = tr.find_all(\"td\")\n",
    "                temp = []\n",
    "                \n",
    "                firstColA = data[0].find('a')\n",
    "                playerId = firstColA['href'].split(\"=\")[2]\n",
    "                fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                \n",
    "                \n",
    "                if i == 1:  # K url\n",
    "                    \n",
    "                    pos = posAndTeam[0].strip()\n",
    "                    try:\n",
    "                        team = posAndTeam[1].strip()\n",
    "                    except:\n",
    "                        team = \"FA\"\n",
    "                        \n",
    "                    xpMade = data[2].text\n",
    "                    made0_19 = float(data[3].text.replace(\"-\", \"0\"))\n",
    "                    made20_29 = float(data[4].text.replace(\"-\", \"0\"))\n",
    "                    made30_39 = float(data[5].text.replace(\"-\", \"0\"))\n",
    "                    made40_49 = float(data[6].text.replace(\"-\", \"0\"))\n",
    "                    made50 = float(data[7].text.replace(\"-\", \"0\"))\n",
    "                    fgMade = made0_19 + made20_29 + made30_39 + made40_49 + made50\n",
    "                    FantasyPoints = data[8].text\n",
    "\n",
    "                    temp = [\"nfl\", today, season, week, playerId,fullName,np.nan,pos,team,0,0,0,0,0,0, 0,\n",
    "                                    0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "                                    fgMade,0,0,made0_19,0,made20_29,0,made30_39,0,made40_49,0,made50,0,xpMade,0,0,\n",
    "                                     0,0,0,0,0,0,0,0,0,0,0,0,0,0,FantasyPoints,0]\n",
    "                    player_data.append(temp)    \n",
    "                        \n",
    "                else: # D url\n",
    "                    pos = 'DST'\n",
    "                    team = fullName\n",
    "                    sacks = data[2].text\n",
    "                    interceptions = data[3].text\n",
    "                    fum = data[4].text\n",
    "                    safety = data[5].text\n",
    "                    defTd = data[6].text\n",
    "                    twoPt = data[7].text\n",
    "                    retTd = data[8].text\n",
    "                    ptsAllowed= data[9].text\n",
    "                    fantasyPts= data[10].text\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    temp = [\"nfl\", today, season, week, playerId,np.nan,np.nan,pos,team,0,0,0,0,0,0,0,\n",
    "                            0,0,0,0,0,0,0,0,0,0,0,\n",
    "                            FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,interceptions,safety,sacks,0,fum,0,\n",
    "                            defTd, retTd,ptsAllowed,0,0,0,0,0,twoPt,fantasyPts,0]\n",
    "                    \n",
    "                    player_data.append(temp)\n",
    "        \n",
    "df_nfl_proj = pd.DataFrame(player_data, columns=projection_columns).replace(\"-\",0)\n",
    "df_nfl_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0785e8c9-b294-44a8-82f6-2df4392579a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_proj.to_csv(\"../data/projection/weekly/nfl_proj_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72bb1a-23e1-42d0-b858-a87f7d5280be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NFL rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "305c60d3-1ed1-4744-9f54-9ed6ab23e221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_rank_url = {\n",
    "    \"QB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=QB&sort=1&statType=weekStats&week={week}\",     \n",
    "    \"RB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=RB&sort=1&statType=weekStats&week={week}\",\n",
    "    \"WR\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=WR&sort=1&statType=weekStats&week={week}\",\n",
    "    \"TE\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=TE&sort=1&statType=weekStats&week={week}\",\n",
    "    \"K\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=K&sort=1&statType=weekStats&week={week}\",\n",
    "    \"DST\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=DEF&sort=1&statType=weekStats&week={week}\"\n",
    "}\n",
    "\n",
    "df_nfl_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "for k,v in nfl_rank_url.items():\n",
    "    \n",
    "    time.sleep(1)\n",
    "    r = requests.get(nfl_rank_url[k].format(week=week))\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # grabs the number of players with projections on the site. pagenated at 25 a page\n",
    "    rank_table = soup.find(\"table\", class_=\"tableType-player noGroups\").find(\"tbody\")\n",
    "\n",
    "    player_ranks = []\n",
    "    for tr in rank_table.find_all(\"tr\"):\n",
    "        player_data = []\n",
    "        td = tr.find_all(\"td\")\n",
    "\n",
    "        pos_raw = td[0].text\n",
    "        if pos_raw == '--':\n",
    "            pos_raw = pos_raw.replace('--',0)\n",
    "        pos_rank = int(pos_raw)\n",
    "        playerId = int(td[1].find(\"a\")['href'].split(\"=\")[-1])\n",
    "        full_name = td[1].find(\"a\").text.replace(\".\", \"\")\n",
    "        \n",
    "        pos = td[1].find(\"em\").text.split(\"-\")[0].strip()\n",
    "        \n",
    "        if k == \"DST\":\n",
    "            team = \"\"\n",
    "            pos = \"DST\"\n",
    "        \n",
    "        else:\n",
    "            # no team name for FAs\n",
    "            try:\n",
    "                team = td[1].find(\"em\").text.split(\"-\")[1].strip()\n",
    "            except:\n",
    "                team = \"FA\"\n",
    "        \n",
    "        ovr_rank = td[-1].text\n",
    "        if ovr_rank == '--':\n",
    "            ovr_rank = ovr_rank.replace('--','0')   \n",
    "        ovr_rank = int(ovr_rank)\n",
    "\n",
    "        player_data = [\"nfl\", today, season, week, k, \"nfl\", pos_rank, full_name, playerId, team, pos, np.nan, np.nan]\n",
    "        player_ranks.append(player_data)\n",
    "\n",
    "    temp_df = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "    df_nfl_ranking = pd.concat([df_nfl_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "    \n",
    "df_nfl_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab273ae-b631-44ab-86f4-a7a690845559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_ranking.to_csv(\"../data/ranking/weekly/nfl_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98ffd1-867c-4ae9-ae0e-48802134d9be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Weekly Bettings Lines from Betting Pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c67308-813d-4325-bc5b-7db9a0d23ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'season = 2022\\nweek = 17\\nstrWeek = \"17\"\\ntoday = date(2023, 1, 1)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''season = 2022\n",
    "week = 17\n",
    "strWeek = \"17\"\n",
    "today = date(2023, 1, 1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1985d63a-a15d-4f86-b040-f8ada6c4a38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "urls = [\n",
    "    r'https://www.bettingpros.com/nfl/odds/spread',#/?season={season}&week={week}',\n",
    "    r'https://www.bettingpros.com/nfl/odds/moneyline',#/?season={season}&week={week}\",\n",
    "    r'https://www.bettingpros.com/nfl/odds/total'#/?season={season}&week={week}\"\n",
    "]\n",
    "  \n",
    "# empty, to be filled with data from websites\n",
    "df_lines = pd.DataFrame(columns=[\"date\", \"season\", \"week\", \"overUnder\", \"overUnderCost\" \n",
    "                                                \"awayTeam\", \"awaySpread\", \"awayCost\", \"awayMoneyline\",  \n",
    "                                                \"homeTeam\", \"homeSpread\", \"homeCost\", \"homeMoneyLine\"]) \n",
    "# try to closed driver on error\n",
    "try:\n",
    "    for i in range(len(urls)):    \n",
    "        driver.get(urls[i].format(season=season, week=week)) \n",
    "        # sleep to let the html load\n",
    "        time.sleep(7)\n",
    "\n",
    "        #scrolling through web page to open all games\n",
    "        y_loc_jump = 500 # initial y loc for scroll\n",
    "        for z in range(5):\n",
    "            driver.execute_script(\"window.scrollTo(0, {});\".format(str(y_loc_jump)))\n",
    "            y_loc_jump += 500 # adjust the scroll target down the page\n",
    "            time.sleep(2)\n",
    "        # jump back to top of page after loading \n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(str(0)))\n",
    "        time.sleep(1)\n",
    "        \n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        data = soup.find_all(\"div\", class_=\"flex odds-offer\")\n",
    "        \n",
    "        matchups = []\n",
    "######### spreads\n",
    "        if i == 0:\n",
    "\n",
    "            for div in data:\n",
    "\n",
    "                data_team = div.find_all(\"a\", class_=\"link team-overview__team-name\")\n",
    "                data_spread = div.find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "                # team 1 data - away\n",
    "                away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                away_spread_line = data_spread[1].find_all(\"span\", class_=\"odds-cell__line\")[0].text.replace(\"+\", \"\")\n",
    "\n",
    "                if (away_spread_line == \"NL\") or (away_spread_line == \"--\"):\n",
    "                    away_spread_line = np.nan\n",
    "\n",
    "                away_spread_cost = data_spread[1].find_all(\"span\", class_=\"odds-cell__cost\")[0].text.strip().replace(\"+\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                if (away_spread_cost == \"NL\") or (away_spread_cost == \"--\"):\n",
    "                    away_spread_cost = np.nan\n",
    "\n",
    "                #team 2 data - home\n",
    "                home_team = data_team[1]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                home_spread_line = data_spread[1].find_all(\"span\", class_=\"odds-cell__line\")[1].text.replace(\"+\", \"\")\n",
    "\n",
    "                if (home_spread_line == \"NL\") or (home_spread_line == \"--\"):\n",
    "                    home_spread_line = np.nan\n",
    "\n",
    "                home_spread_cost = data_spread[1].find_all(\"span\", class_=\"odds-cell__cost\")[1].text.strip().replace(\"+\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                if (home_spread_cost == \"NL\") or (home_spread_cost == \"--\"):\n",
    "                    home_spread_cost = np.nan\n",
    "\n",
    "                matchup = [today, season, week,\n",
    "                           np.nan, np.nan,  # placeholder for overUnders\n",
    "                           away_team, float(away_spread_line), away_spread_cost, np.nan,\n",
    "                           home_team, float(home_spread_line), home_spread_cost, np.nan\n",
    "                          ]\n",
    "                \n",
    "                matchups.append(matchup)\n",
    "\n",
    "            df_lines = pd.DataFrame(matchups, columns=[\"date\", \"season\", \"week\", \"overUnder\", \"overUnderCost\",\n",
    "                                                    \"awayTeam\", \"awaySpread\", \"awayCost\", \"awayMoneyline\",  \n",
    "                                                    \"homeTeam\", \"homeSpread\", \"homeCost\", \"homeMoneyLine\"])\n",
    "\n",
    "#########moneylines\n",
    "        elif i == 1:\n",
    "            for div in data:\n",
    "\n",
    "                data_team = div.find_all(\"a\", class_=\"team-overview__team-name\")\n",
    "                data_moneylines = div.find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "                # team 1 data - away\n",
    "                away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                away_moneyline = data_moneylines[1].find_all(\"span\", class_=\"odds-cell__line\")[0].text.strip().replace(\"+\", \"\")\n",
    "                if (away_moneyline == \"NL\") or (away_moneyline == \"--\"):\n",
    "                    away_moneyline = np.nan\n",
    "                elif away_moneyline == \"EVEN\":\n",
    "                    away_moneyline = 0\n",
    "                else:\n",
    "                    away_moneyline = float(away_moneyline)\n",
    "\n",
    "                #team 2 data - home\n",
    "                home_moneyline = data_moneylines[1].find_all(\"span\", class_=\"odds-cell__line\")[1].text.strip().replace(\"+\", \"\")\n",
    "                if (home_moneyline == \"NL\") or (home_moneyline == \"--\"):\n",
    "                    home_moneyline = np.nan\n",
    "                elif home_moneyline == \"EVEN\":\n",
    "                    home_moneyline = 0\n",
    "                else:\n",
    "                    home_moneyline = float(home_moneyline)\n",
    "                #print(\"ML: \", away_moneyline, home_moneyline)\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'awayMoneyline'] = away_moneyline\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'homeMoneyLine'] = home_moneyline\n",
    "\n",
    "######## totals\n",
    "        elif i == 2:\n",
    "\n",
    "            for div in data:\n",
    "\n",
    "                data_team = div.find_all(\"a\", class_=\"team-overview__team-name\")\n",
    "                data_spread = div.find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "                # team 1 data - away\n",
    "                away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                overUnder_line = data_spread[0].find_all(\"span\", class_=\"odds-cell__line\")[0].text.strip()\n",
    "                \n",
    "                if (overUnder_line == \"NL\") or (overUnder_line == \"--\"):\n",
    "                    overUnder_line = np.nan\n",
    "                elif (overUnder_line == 'OFF'):\n",
    "                    overUnder_line = data_spread[1].find_all(\"span\", class_=\"odds-cell__line\")[0].text.strip()\n",
    "                    overUnder_line = float(overUnder_line.replace(\"+\", \"\").split(\" \")[1])\n",
    "                else:\n",
    "                    overUnder_line = float(overUnder_line.replace(\"+\", \"\").split(\" \")[1])\n",
    "\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'overUnder'] = overUnder_line\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'overUnderCost'] = -110\n",
    "                #print(\"OU: \", overUnder_line)\n",
    "    df_lines = df_lines.replace(np.nan, None).replace(\"EVEN\", 100)\n",
    "    df_lines.to_csv(\"../data/betting/lines{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), \n",
    "                    index=False, )\n",
    "\n",
    "except Exception as ex:\n",
    "    print(i)\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "\n",
    "driver.close()  \n",
    "\n",
    "df_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88f4ec-d774-404b-a63b-1bea55f6e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7ea1c-8542-4e5c-ab55-9c370f23ff65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# END OF WEEK SCORES AND STATS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bacdd0fb-b264-4d26-a2c9-ea57499b8960",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "last run: 1/1/25 week 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efaefffa-c461-40fd-b988-efef6e429b12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 18 success\n"
     ]
    }
   ],
   "source": [
    "# import credentials\n",
    "dbUser = creds['mysqlSurface']['users'][1]\n",
    "dbPw = creds['mysqlSurface']['creds']['jb']\n",
    "dbHost = creds['mysqlSurface']['dbNFL']['host']\n",
    "dbName = creds['mysqlSurface']['dbNFL']['database']\n",
    "dbConnectionString = creds['pymysql'][league]\n",
    "\n",
    "# creating lookup dictionaries that will be used across multiple database inserts\n",
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    # getting outlet db ids to convert the scraped names/ids\n",
    "    outletLookup = pd.read_sql(\"SELECT outletId, outletName  FROM outlet;\", conn)\n",
    "    outletLookup = pd.Series(outletLookup.outletId.values, index=outletLookup.outletName).to_dict()\n",
    "    \n",
    "    # getting team db ids to convert datasource names to the ids\n",
    "    #teamLookup = pd.read_sql(\"SELECT teamId, name  FROM team;\", conn)\n",
    "    teamLookup = pd.read_sql(\"SELECT teamId, name  FROM team;\", conn)\n",
    "    teams = pd.read_sql(\"SELECT *  FROM team;\", conn)\n",
    "    teamLookup = pd.Series(teamLookup.teamId.values, index=teamLookup.name).to_dict()\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)\n",
    "\n",
    "url_scores = r'https://www.cbssports.com/nfl/scoreboard/all/{season}/regular/{week}/'\n",
    "prev_week = week - 1\n",
    "\n",
    "# retreiving HTML and converting it to soup\n",
    "r = requests.get(url_scores.format(season=season, week=str(prev_week)))\n",
    "soup = bs(r.text)\n",
    "\n",
    "# accessing tables with the data\n",
    "tables = soup.find_all(\"div\", class_=\"live-update\")\n",
    "\n",
    "\n",
    "records = pd.DataFrame(columns = [\"season\", \"week\", \"teamId\", \"wins\", \"losses\", \"ties\"])\n",
    "games = pd.DataFrame(columns=[\"season\", \"week\", \"homeTeamId\", \"homeQ1Pts\", \"homeQ2Pts\", \"homeQ3Pts\", \"homeQ4Pts\", \"homeTotalPts\",\n",
    "        \"awayTeamId\", \"awayQ1Pts\", \"awayQ2Pts\", \"awayQ3Pts\", \"awayQ4Pts\", \"awayTotalPts\",\n",
    "        \"homeWinner\", \"awayWinner\", \"tie\", \"winningTeamId\", \"gameId\"])\n",
    "\n",
    "for t in tables:\n",
    "    game = t.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "    # away team \n",
    "    away = game[0]\n",
    "    away_team = away.find(\"a\")['href'].split(\"/\")[3]\n",
    "    away_record = away.find(\"span\", class_=\"record\").text.split(\"-\")\n",
    "    away_win = away_record[0]\n",
    "    away_loss = away_record[1]\n",
    "    if len(away_record) == 3:\n",
    "        away_tie = away_record[2]\n",
    "    else: \n",
    "        away_tie = 0\n",
    "    away_scores = away.find_all(\"td\")\n",
    "    away_q1Pts = int(away_scores[1].text)\n",
    "    away_q2Pts = int(away_scores[2].text)\n",
    "    away_q3Pts = int(away_scores[3].text)\n",
    "    away_q4Pts = int(away_scores[4].text)\n",
    "    away_totalPts = int(away_scores[5].text)\n",
    "\n",
    "    # home team \n",
    "    home = game[1]\n",
    "    home_team = home.find(\"a\")['href'].split(\"/\")[3]\n",
    "    home_record = home.find(\"span\", class_=\"record\").text.split(\"-\")\n",
    "    home_win = home_record[0]\n",
    "    home_loss = home_record[1]\n",
    "    if len(home_record) == 3:\n",
    "        home_tie = home_record[2]\n",
    "    else: \n",
    "        home_tie = 0\n",
    "    home_scores = home.find_all(\"td\")\n",
    "    home_q1Pts = int(home_scores[1].text)\n",
    "    home_q2Pts = int(home_scores[2].text)\n",
    "    home_q3Pts = int(home_scores[3].text)\n",
    "    home_q4Pts = int(home_scores[4].text)\n",
    "    home_totalPts = int(home_scores[5].text)\n",
    "\n",
    "    # designating winner or tie\n",
    "    home_winner = 0\n",
    "    away_winner = 0\n",
    "    tie = 0\n",
    "    winning_team = np.nan\n",
    "\n",
    "    if home_totalPts > away_totalPts:\n",
    "        home_winner = 1\n",
    "        winning_team = home_team\n",
    "    elif home_totalPts < away_totalPts:\n",
    "        away_winner = 1\n",
    "        winning_team = away_team\n",
    "    else:\n",
    "        tie = 1\n",
    "    \n",
    "    # gathering data into list for df creation and creating temp dfs to concat with main df\n",
    "    home_records = [season, prev_week, home_team, home_win, home_loss, home_tie]\n",
    "    away_records = [season, prev_week, away_team, away_win, away_loss, away_tie]\n",
    "    game_data = [season, week-1, home_team, home_q1Pts, home_q2Pts, home_q3Pts, home_q4Pts, home_totalPts,\n",
    "                 away_team, away_q1Pts, away_q2Pts, away_q3Pts, away_q4Pts, away_totalPts,\n",
    "                home_winner, away_winner, tie, winning_team]\n",
    "\n",
    "     \n",
    "    temp_games = pd.DataFrame([game_data], columns=[\"season\", \"week\", \"homeTeamId\", \"homeQ1Pts\", \"homeQ2Pts\", \"homeQ3Pts\", \"homeQ4Pts\", \"homeTotalPts\",\n",
    "        \"awayTeamId\", \"awayQ1Pts\", \"awayQ2Pts\", \"awayQ3Pts\", \"awayQ4Pts\", \"awayTotalPts\",\n",
    "        \"homeWinner\", \"awayWinner\", \"tie\", \"winningTeamId\"])\n",
    "    \n",
    "    cols = ['season', 'week', 'homeTeamId', 'awayTeamId']\n",
    "    temp_games['gameId'] = temp_games[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    temp_games = temp_games.replace(\"WAS\", \"WSH\").replace(\"JAC\", \"JAX\")\n",
    "    \n",
    "    games = pd.concat([games, temp_games])\n",
    "    \n",
    "    temp_records_h = pd.DataFrame([home_records], columns = [\"season\", \"week\", \"teamId\", \"wins\", \"losses\", \"ties\"])\n",
    "    temp_records_a = pd.DataFrame([away_records], columns = [\"season\", \"week\", \"teamId\", \"wins\", \"losses\", \"ties\"])\n",
    "    temp_records_h = pd.concat([temp_records_h, temp_records_a])\n",
    "    temp_records_h = temp_records_h.replace(\"WAS\", \"WSH\").replace(\"JAC\", \"JAX\")\n",
    "    \n",
    "    records = pd.concat([records, temp_records_h])\n",
    "    #records.rename(columns={'team':'teamId'})\n",
    "\n",
    "\n",
    "# LOAD TO DB\n",
    "try:\n",
    "    #conn = mysql.connector.connect(user=dbUser, password=dbPw,host=dbHost,database=dbName)\n",
    "    #cursor = conn.cursor()\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "    \n",
    "        games['homeTeamId'] = games['homeTeamId'].map(teamLookup)\n",
    "        games['awayTeamId'] = games['awayTeamId'].map(teamLookup)\n",
    "        games['winningTeamId'] = games['winningTeamId'].map(teamLookup)\n",
    "        records['teamId'] = records['teamId'].map(teamLookup)\n",
    "\n",
    "        records.to_sql('weeklyrecord', conn, if_exists='append', index=False)\n",
    "        games.to_sql('weeklyscore', conn, if_exists='append', index=False)\n",
    "        \n",
    "    print(season, week, \"success\")\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e126a9c-f967-482c-8a69-a884ab6abd79",
   "metadata": {},
   "source": [
    "# <<< SCRATCH >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451717-1406-4e74-a167-d8d173dbb68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "engine = create_engine(dbConnectionString)\n",
    "with engine.connect() as conn:\n",
    "    lookup = pd.read_sql(\"SELECT playerId, nflId FROM player WHERE name = 'Travis Kelce';\", conn)\n",
    "    #lookup = pd.Series(lookup.playerId.values, index=lookup.nflId).to_dict()\n",
    "nf = pd.read_csv(r\"C:\\Users\\jrbrz\\Desktop\\projects\\projects\\ffDraft\\data\\ranking\\weekly\\nfl_rank_2022-06_2022-10-13.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb4499-588f-4d93-9a32-a4f1e8138ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510f30a-2590-41a6-bc07-8d89e1486762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_cbs_proj, df_cbs_ranking, df_cbs_adp, \n",
    "       df_fp_ranking, df_fp_adp, \n",
    "       df_espn_proj, df_espn_ranking, df_espn_adp,\n",
    "       df_nfl_proj, df_nfl_ranking,\n",
    "       df_overUnders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ea648-7781-4bbf-a907-1ea8a333c1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "driver.get(\"https://www.espn.com/fantasy/football/story/_/id/33898295/fantasy-football-idp-rankings-2022-top-50-defensive-linemen-linebackers-defensive-backs\")\n",
    "time.sleep(10)\n",
    "\n",
    "# grabs the entire pages html\n",
    "html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "soup = bs(html)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2197e91-dc66-4a1b-b49c-7fa13f338933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba15ae-7851-4b9d-9481-ad5eab39871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure to create json file \n",
    "c = {}\n",
    "c = json.dumps(c)\n",
    "with open('config.txt', 'w') as f:\n",
    "    f.write(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756ce1c-1f44-4181-9a79-5975c7deb7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fantasyfootball-maF7LdMm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
