{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2bddad12-2646-4263-aada-1655c0c5d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, re, os, json\n",
    "import pymysql, pyodbc\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import date\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#importing credentials\n",
    "with open('../../../Notes-General/config.txt', 'r') as f:\n",
    "    creds = f.read()\n",
    "creds = json.loads(creds)\n",
    "league = 'nfl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf3688b-7dfc-4ef5-b989-ce31a711c5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What week is it >>>>>> ?  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "today = date.today()\n",
    "season = 2023\n",
    "week = int(input(\"What week is it >>>>>> ? \"))\n",
    "if week < 10:\n",
    "    strWeek = \"0\" + str(week)\n",
    "else:\n",
    "    strWeek = week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd2a5d2-4375-41f1-a832-8777b6abbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column structures for stat projections\n",
    "projection_columns = [\"outlet\",\"date\", \"season\", \"week\", \"playerId\", \"name\", \"shortName\", \"pos\", \"team\", 'GamesPlayed',\n",
    " 'PassAttempts','PassCompletions','PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    " 'InterceptionsThrown', 'PasserRating',\n",
    " 'RushingAttempts','RushingYards', 'AverageYardsPerRush', 'RushingTouchdowns',\n",
    " 'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception','ReceivingTouchdowns',\n",
    " 'FumblesLost',\n",
    " 'FieldGoalsMade','FieldGoalAttempts','LongestFieldGoal','FieldGoals119Yards','FieldGoals119YardAttempts',\n",
    " 'FieldGoals2029Yards','FieldGoals2029YardAttempts','FieldGoals3039Yards','FieldGoals3039YardAttempts',\n",
    " 'FieldGoals4049Yards','FieldGoals4049YardAttempts','FieldGoals50Yards','FieldGoals50YardsAttempts',\n",
    " 'ExtraPointsMade','ExtraPointsAttempted',\n",
    " 'Interceptions','Safeties','Sacks','Tackles','DefensiveFumblesRecovered','ForcedFumbles','DefensiveTouchdowns', \n",
    " 'ReturnTouchdowns','PointsAllowed','PointsAllowedPerGame','NetPassingYardsAllowed','RushingYardsAllowed', \n",
    " 'TotalYardsAllowed', 'YardsAgainstPerGame', 'twoPt','FantasyPoints','FantasyPointsPerGame']\n",
    "\n",
    "ranking_columns = [\"outlet\", \"date\", \"season\", \"week\", \"group\", \"expert\", \"rank\",\"name\",\"playerId\",\"team\",\"pos\", \"high\", \"low\"]\n",
    "\n",
    "team_map = {\n",
    "    \"Jacksonville Jaguars\":\"JAX\",\"Los Angeles Rams\":\"LA\",\"Philadelphia Eagles\":\"PHI\",\"Minnesota Vikings\":\"MIN\",\n",
    "    \"Houston Texans\":\"HOU\",\"Los Angeles Chargers\":\"LAC\",\"Baltimore Ravens\":\"BAL\",\"New England Patriots\":\"NE\",\n",
    "    \"Carolina Panthers\":\"CAR\",\"Denver Broncos\":\"DEN\",\"Arizona Cardinals\":\"ARI\",\"New Orleans Saints\":\"NO\",\n",
    "    \"Detroit Lions\":\"DET\",\"Pittsburgh Steelers\":\"PIT\",\"Chicago Bears\":\"CHI\",\"Seattle Seahawks\":\"SEA\",\n",
    "    \"Buffalo Bills\":\"BUF\",\"Tennessee Titans\":\"TEN\",\"Atlanta Falcons\":\"ATL\",\"Cincinnati Bengals\":\"CIN\",\n",
    "    \"Kansas City Chiefs\":\"KC\",\"Washington Redskins\":\"WAS\",\"Dallas Cowboys\":\"DAL\",\"Tampa Bay Buccaneers\":\"TB\",\n",
    "    \"Green Bay Packers\":\"GB\",\"New York Giants\":\"NYG\",\"San Francisco 49ers\":\"SF\",\"Cleveland Browns\":\"CLE\",\n",
    "    \"Oakland Raiders\":\"OAK\",\"Indianapolis Colts\":\"IND\",\"Miami Dolphins\":\"MIA\",\"New York Jets\":\"NYJ\"}\n",
    "team_mascot_map = {\n",
    "    \"Jaguars\":\"JAX\",\"Rams\":\"LA\",\"Eagles\":\"PHI\",\"Vikings\":\"MIN\",\n",
    "    \"Texans\":\"HOU\",\"Chargers\":\"LAC\",\"Ravens\":\"BAL\",\"Patriots\":\"NE\",\n",
    "    \"Panthers\":\"CAR\",\"Broncos\":\"DEN\",\"Cardinals\":\"ARI\",\"Saints\":\"NO\",\n",
    "    \"Lions\":\"DET\",\"Steelers\":\"PIT\",\"Bears\":\"CHI\",\"Seahawks\":\"SEA\",\n",
    "    \"Bills\":\"BUF\",\"Titans\":\"TEN\",\"Falcons\":\"ATL\",\"Bengals\":\"CIN\",\n",
    "    \"Chiefs\":\"KC\",\"Redskins\":\"WAS\",\"Cowboys\":\"DAL\",\"Buccaneers\":\"TB\",\n",
    "    \"Packers\":\"GB\",\"Giants\":\"NYG\",\"49ers\":\"SF\",\"Browns\":\"CLE\",\n",
    "    \"Raiders\":\"OAK\",\"Colts\":\"IND\",\"Dolphins\":\"MIA\",\"Jets\":\"NYJ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccecebe4-4180-47dc-8cfe-71525d2ad042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 1 01\n"
     ]
    }
   ],
   "source": [
    "print(season, week, strWeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c85bb-f992-462e-a2d2-8c385eac036e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- CBS - PPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5c759-c181-4c54-81e1-44b5d37e24e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### cbs projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07100377-0987-4797-a14e-98ff2b71d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_projections = \"https://www.cbssports.com/fantasy/football/stats/{pos}/{season}/{week}/projections/ppr/\"\n",
    "positions = [\"QB\", \"RB\", \"WR\", \"TE\", \"K\", \"DST\"]\n",
    "tableClass = \"TableBase-table\"  \n",
    "tableHeader = \"TableBase-headTr\"\n",
    "headerClass = \"Tablebase-tooltipInner\"\n",
    "tableRow = \"TableBase-bodyTr\"\n",
    "tableD = \"TableBase-bodyTd\"\n",
    "\n",
    "df_cbs_proj = pd.DataFrame(columns=projection_columns)\n",
    "\n",
    "# loop through each position to retrieve HTML and convert to df\n",
    "for p in range(len(positions)):\n",
    "    \n",
    "    time.sleep(3)\n",
    "    #updating URL for each position\n",
    "    url_formatted = url_projections.format(pos=positions[p], season=season, week=week)\n",
    "\n",
    "    # retreiving HTML and converting it to soup\n",
    "    r = requests.get(url_formatted)\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # accessing table with the data\n",
    "    table = soup.find(\"table\", class_= tableClass)\n",
    "\n",
    "    \n",
    "    # accounting for the difference in DEF headers\n",
    "    if positions[p] == \"DST\":\n",
    "        cols = [\"pos\", \"team\",\"name\"]\n",
    "    else:\n",
    "        cols = [\"playerId\", \"name\", \"shortName\", \"pos\", \"team\"]\n",
    "    \n",
    "    ### grabbing column names from the thead for the position. These will be used to create the temp. pos dataframe\n",
    "    #retrieving column names from the HTML\n",
    "    for i in table.find_all(\"div\", class_=headerClass):\n",
    "        cols.append(''.join(filter(str.isalnum, i.text)))\n",
    "        \n",
    "    # accessing the data in the body\n",
    "    body = table.find(\"tbody\")\n",
    "    # looping through rows\n",
    "    data = []\n",
    "    for tr in body.find_all(\"tr\", class_=tableRow):\n",
    "        # accounting for DST and populating pos as DST since it is not provided\n",
    "        if positions[p] == \"DST\":\n",
    "            player_data = [\"DST\"]\n",
    "        else:\n",
    "            player_data = []\n",
    "        \n",
    "        for td in tr.find_all(\"td\", class_=tableD):\n",
    "            \n",
    "            if positions[p] == \"DST\":\n",
    "            # pulling team name \n",
    "                span = td.find_all(\"span\",class_=\"CellLogoNameLockup\")\n",
    "                if span:   \n",
    "                    for s in span:\n",
    "                        player_data.append(s.find(\"a\")[\"href\"].split(\"/\")[3])\n",
    "                        player_data.append(str.strip(td.text))\n",
    "                \n",
    "                # non-span <Td>, \n",
    "                else:\n",
    "                    player_data.append(str.strip(td.text))\n",
    "                    \n",
    "            # processing table body for all pos except DST\n",
    "            else:\n",
    "                #the player name, id, pos, and team are all in spans. \n",
    "                #the spans are not present in the stat <td>'s\n",
    "                span_short = td.find_all(\"span\",class_=\"CellPlayerName--short\")\n",
    "                span_long = td.find_all(\"span\",class_=\"CellPlayerName--long\")\n",
    "\n",
    "                # if the <td> has a span, the player info will be extracted\n",
    "                if span_long:\n",
    "\n",
    "                    for s in span_long:\n",
    "                        # player Id from the href url\n",
    "                        player_data.append(s.find(\"a\")[\"href\"].split(\"/\")[4])\n",
    "                        # player full name\n",
    "                        player_data.append(str.strip(s.find(\"a\").text).replace(\".\", \"\"))\n",
    "\n",
    "                    for s in span_short:\n",
    "                        # player short name\n",
    "                        player_data.append(s.find(\"a\").text.replace(\".\", \"\"))\n",
    "                        #player position\n",
    "                        player_data.append(str.strip(s.find(\"span\", class_=\"CellPlayerName-position\").text))\n",
    "                        #player nfl team\n",
    "                        player_data.append(str.strip(s.find(\"span\", class_=\"CellPlayerName-team\").text).replace(\"JAC\", \"JAX\").replace(\"WAS\", \"WSH\"))\n",
    "            \n",
    "                # non-span <Td>\n",
    "                else:\n",
    "                    player_data.append(str.strip(td.text))\n",
    "        \n",
    "        # creates the list of players, each player is a list with stats\n",
    "        data.append(player_data)\n",
    "    \n",
    "    # converts list of list to data frame with the applicable columns pulled earlier\n",
    "    pos_df = pd.DataFrame(data, columns=cols)\n",
    "    \n",
    "    # concats all of the position data to the master df\n",
    "    df_cbs_proj = pd.concat([df_cbs_proj, pos_df], axis=0, ignore_index=True)\n",
    "\n",
    "df_cbs_proj.loc[:,'outlet'] = \"cbs\"\n",
    "df_cbs_proj.loc[:,'date'] = today\n",
    "df_cbs_proj.loc[:,'season'] = season\n",
    "df_cbs_proj.loc[:,'week'] = week\n",
    "df_cbs_proj.loc[:,'LongestFieldGoal'] = np.nan\n",
    "\n",
    "df_cbs_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8000a90e-b09f-4a07-b0d6-14ef6bfdd525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cbs_proj.to_csv(\"../data/projection/weekly/cbs_proj_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a430b-f61b-4e8b-8ba8-759a2ca73a19",
   "metadata": {
    "tags": []
   },
   "source": [
    "### cbs rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a5817-09b5-4d22-a8f6-8a0ee0a7702b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_rankings = \"https://www.cbssports.com/fantasy/football/rankings/ppr/{pos}\"#/weekly/\"\n",
    "\n",
    "positions = [\"QB\", \"RB\", \"WR\", \"TE\", \"K\", \"DST\", \"FLEX\"]\n",
    "\n",
    "# key class names that will be targeted\n",
    "parentDivClass = \"rankings-table multi-authors hide-player-stats\"  # contains all expert rankings (3 tables)\n",
    "individualRankingDivClass = \"experts-column triple\"  # 3 of these for their 3 experts  \n",
    "authorNameAClass = \"author-name\"\n",
    "playersDivClass = \"player-wrapper\"  # the divs of interest are in here but it also includes data that is not needed \n",
    "\n",
    "df_cbs_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "for pos in positions:\n",
    "    time.sleep(3)    \n",
    "    # retreiving HTML and converting it to soup\n",
    "    url_formatted = url_rankings.format(pos=pos)\n",
    "    r = requests.get(url_formatted)\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # finding the tables with rankings\n",
    "    rankingTables = soup.find_all(\"div\", class_=individualRankingDivClass)\n",
    "    \n",
    "    # looping through the 3 expert ranks that are in their own tables\n",
    "    player_ranking_data = []\n",
    "    if pos == \"FLEX\":\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, season, week, pos, expert]\n",
    "                try:\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"player-name\").text).replace(\".\", \"\"))  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[0])) # contains the player nfl team \n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[1])) # contains the player nfl position\n",
    "                    temp.append(np.nan)\n",
    "                    temp.append(np.nan)\n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    elif pos == \"DST\":\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, season, week, pos, expert]\n",
    "                try:\n",
    "                    team = str.strip(p.find(\"span\", class_=\"player-name\").text)\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(team)  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(team) # contains the player nfl team \n",
    "                    temp.append(pos) # contains the player nfl position\n",
    "                    temp.append(np.nan)\n",
    "                    temp.append(np.nan)\n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    else:\n",
    "        for rt in rankingTables:\n",
    "            #extracting expert name\n",
    "            expert = rt.find(\"a\", class_=authorNameAClass).span.text\n",
    "\n",
    "            #looping through the divs that contain all the player level ranking data\n",
    "            ranks = rt.find(\"div\", class_=playersDivClass)\n",
    "            for p in ranks:\n",
    "                temp = [\"cbs\", today, season, week, pos, expert]\n",
    "                try:\n",
    "                    temp.append(str.strip(p.find(\"div\", class_=\"rank\").text))  #expert rank, number  .text\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"player-name\").text).replace(\".\", \"\"))  #cbs shortName  .text\n",
    "                    temp.append(str.strip(p.find(\"a\")[\"href\"].split(\"/\")[4])) # cbs playerId is in the url\n",
    "                    temp.append(str.strip(p.find(\"span\", class_=\"team position\").text.split()[0])) # contains the player nfl team \n",
    "                    temp.append(pos) # contains the player nfl team\n",
    "                    temp.append(np.nan)\n",
    "                    temp.append(np.nan)\n",
    "                    player_ranking_data.append(temp)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    # creating temp dataframe that includes all 3 expert rankings for a grouping to add to the master df \n",
    "    temp_df = pd.DataFrame(player_ranking_data, columns=ranking_columns)        \n",
    "    df_cbs_ranking = pd.concat([df_cbs_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "    \n",
    "df_cbs_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c079ff-817a-437b-a2fe-8d008eb391d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_cbs_ranking.to_csv(\"../data/ranking/weekly/cbs_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4be72-4592-4191-8e08-0cf285af7865",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- Fantasy Pros - HPPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288c4e0-fc76-42a4-9965-5d8228911798",
   "metadata": {},
   "source": [
    "They use CBS and ESPN for season stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adcebc-ed8b-424e-be41-bb0730ece2da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fantasy Pros Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f16806-64a0-459c-8faf-9996b8fb0a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fantasy Pros ECRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916dea94-2c3e-448a-84b7-0234bed10961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1371, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "waitTime = 10\n",
    "\n",
    "df_ecr_ranks = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "urls = [\n",
    "    r\"https://www.fantasypros.com/nfl/rankings/half-point-ppr-{}\",\n",
    "    r\"https://www.fantasypros.com/nfl/rankings/{}\"\n",
    "] \n",
    "# try to close driver if there are any errors\n",
    "try:\n",
    "    for i in range(len(urls)):\n",
    "        if i == 0:\n",
    "            pos = ['SUPERFLEX', 'FLEX', 'TE', 'WR', 'RB']\n",
    "            for j in pos:\n",
    "                player_ranks = []\n",
    "                driver.get(urls[i].format(j.lower()))\n",
    "\n",
    "                # Accepting cookies if there is a popup\n",
    "                try:\n",
    "                    WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[contains(text(), 'Accept Cookies')]\"))\n",
    "                    cookies = driver.find_element(\"xpath\", '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "                    cookies.click()\n",
    "\n",
    "                except: pass\n",
    "\n",
    "                #driver.execute_script('videos = document.querySelectorAll(\"video\"); for(video of videos) {video.pause()}')\n",
    "\n",
    "                # select drop down that defaults to Overview and selecting Ranks\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                # grab all html\n",
    "                html = driver.page_source\n",
    "                soup = bs(html, 'lxml')  #parse the html\n",
    "\n",
    "                table = soup.find(\"table\", id='ranking-table').find(\"tbody\")\n",
    "                ranks = table.find_all(\"tr\")\n",
    "\n",
    "                for tr in ranks:\n",
    "                    tds = tr.find_all(\"td\")\n",
    "                    #for td in tds:\n",
    "                        #print(td.text)\n",
    "\n",
    "                    # some of the ecr defensive groups have teams in the rankings this will skip them\n",
    "                    name = tds[2].text.split(\"(\")[0].strip().replace(\".\", \"\")\n",
    "                    if name in list(team_map.keys()):\n",
    "                        continue\n",
    "\n",
    "                    if j in ['SUPERFLEX', 'FLEX']:\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[4].text\n",
    "                        low = tds[5].text\n",
    "                    else:\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[4].text\n",
    "                        low = tds[5].text\n",
    "\n",
    "\n",
    "                    player_ranks.append(['fantasyPros', today, season, week, j, 'ecr', rank, name, player, team, np.nan,  high, low])\n",
    "\n",
    "                temp = pd.DataFrame(player_ranks,  columns=ranking_columns)\n",
    "                df_ecr_ranks = pd.concat([df_ecr_ranks, temp])\n",
    "\n",
    "        if i == 1:\n",
    "            pos = ['QB', 'K', 'DST', 'IDP', 'DL' ,'LB', 'DB']\n",
    "            for j in pos:\n",
    "                player_ranks = []\n",
    "                driver.get(urls[i].format(j.lower()))\n",
    "\n",
    "                # Accepting cookies if there is a popup\n",
    "                try:\n",
    "                    WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[contains(text(), 'Accept Cookies')]\"))\n",
    "                    cookies = driver.find_element(\"xpath\", \"//button[contains(text(), 'Accept Cookies')]\")\n",
    "                    cookies.click()\n",
    "\n",
    "                except: pass\n",
    "\n",
    "                # select drop down that defaults to Overview and selecting Ranks\n",
    "                                                            # actual buttom xpath //*[@id=\"onetrust-accept-btn-handler\"]\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[span[contains(text(), 'Overview')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                WebDriverWait(driver, timeout=waitTime).until(lambda d: d.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\"))     \n",
    "                drop = driver.find_element(\"xpath\", \"//button[div[contains(text(), 'Ranks')]]\")\n",
    "                drop.click()\n",
    "\n",
    "                # grab all html\n",
    "                html = driver.page_source\n",
    "                soup = bs(html, 'lxml')  #parse the html\n",
    "\n",
    "                table = soup.find(\"table\", id='ranking-table').find(\"tbody\")\n",
    "                ranks = table.find_all(\"tr\")\n",
    "\n",
    "                for tr in ranks:\n",
    "                    tds = tr.find_all(\"td\")\n",
    "\n",
    "                    # some of the ecr defensive groups have teams in the rankings this will skip them\n",
    "                    name = tds[2].text.split(\"(\")[0].strip().replace(\".\", \"\")\n",
    "                    if name in list(team_map.keys()):\n",
    "                        continue\n",
    "\n",
    "                    if j == 'IDP':\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[4].text\n",
    "                        low = tds[5].text\n",
    "\n",
    "                    else:\n",
    "                        rank = tds[0].text\n",
    "                        team = tds[2].text.split(\"(\")[1].strip().replace(\")\", \"\")\n",
    "                        player = tds[2].find(\"div\", class_='player-cell player-cell__td')['data-player']\n",
    "                        high = tds[3].text\n",
    "                        low = tds[4].text\n",
    "\n",
    "                    player_ranks.append(['fantasyPros', today, season, week, j, 'ecr', rank, name, player, team, np.nan,  high, low])\n",
    "\n",
    "                temp = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "                df_ecr_ranks = pd.concat([df_ecr_ranks, temp])\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "    \n",
    "driver.close()\n",
    "df_ecr_ranks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1047980-f15f-4094-9a3c-bd53c42552ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecr_ranks.to_csv(\"../data/ranking/weekly/fpEcr_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da74d6-09b8-4a61-91f5-550c68441509",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fantasy Pros 5 Expert Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d9ebc2-35d7-4afb-8384-f24357aa6b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6760, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_fp_rankings = r\"https://www.fantasypros.com/nfl/fantasy-football-rankings/{pos}.php\"\n",
    "                  \n",
    "fp_url_positions = {\"QB\":\"weekly-qb\", \n",
    "                    \"RB\":\"weekly-half-point-ppr-rb\",\n",
    "                    \"WR\":\"weekly-half-point-ppr-wr\", \n",
    "                    \"TE\":\"weekly-half-point-ppr-te\", \n",
    "                    \"K\":\"weekly-k\", \n",
    "                    \"DST\":\"weekly-dst\",\n",
    "                    \"FLEX\":\"weekly-half-point-ppr-flex\", \n",
    "                    \"SUPERFLEX\":\"weekly-half-point-ppr-superflex\"}\n",
    "\n",
    "all_ranks = []\n",
    "\n",
    "for k,v in fp_url_positions.items(): \n",
    "    \n",
    "    time.sleep(3)\n",
    "    url_fp_formatted = url_fp_rankings.format(pos=v)\n",
    "    r = requests.get(url_fp_formatted)\n",
    "    soup = bs(r.text)\n",
    "    \n",
    "    # getting expert name and rank date\n",
    "    experts = []\n",
    "    for a in soup.find_all(\"th\", class_=\"expert__th\"):\n",
    "        temp = []\n",
    "        temp.append(a['data-sort-label'])   # expert name\n",
    "        temp.append(str.strip(a.find(\"div\", class_=\"expert__publish-date\").text))  #ranking publish date\n",
    "        experts.append(temp)\n",
    "\n",
    "    # getting player info and ranks\n",
    "    if k == \"DST\":\n",
    "        for p in soup.find_all(\"tr\", class_=\"player-row mpb-player__tr\"):\n",
    "            playerId = p[\"data-pid\"]  # fp playerid\n",
    "            shortName = p.find(\"span\", class_=\"mobile-only\").text.split()[0].replace(\".\", \"\") # fp short name\n",
    "            #fullName = p.find(\"span\", class_=\"everything-but-mobile js-sort-field\").text # fp full name\n",
    "            TEAM = p.find(\"span\", class_=\"player__team\").text  # player nfl team\n",
    "            POS = p.find(\"span\", class_=\"player__position\").text  # player position\n",
    "\n",
    "            html_ranks = p.find_all(\"td\", attrs={'class': None})\n",
    "            for r in range(len(html_ranks)):\n",
    "                temp_fp_ranking = [shortName, playerId, TEAM, POS] \n",
    "\n",
    "                temp_fp_ranking.insert(0, html_ranks[r].text) # ranking\n",
    "                temp_fp_ranking.insert(0, experts[r][0]) # expert\n",
    "                temp_fp_ranking.insert(0, k) # group ranking set\n",
    "                temp_fp_ranking.insert(0, week) #season week\n",
    "                temp_fp_ranking.insert(0, season) #season year\n",
    "                temp_fp_ranking.insert(0, experts[r][1]) # date\n",
    "                temp_fp_ranking.insert(0, \"fantasyPros\") # outlet\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                all_ranks.append(temp_fp_ranking)\n",
    "    \n",
    "    else:\n",
    "        for p in soup.find_all(\"tr\", class_=\"player-row mpb-player__tr\"):\n",
    "            playerId = p[\"data-pid\"]  # fp playerid\n",
    "            shortName = p.find(\"span\", class_=\"mobile-only\").text.replace(\".\", \"\") # fp short name\n",
    "            #fullName = p.find(\"span\", class_=\"everything-but-mobile js-sort-field\").text # fp full name\n",
    "            TEAM = p.find(\"span\", class_=\"player__team\").text  # player nfl team\n",
    "            POS = p.find(\"span\", class_=\"player__position\").text  # player position\n",
    "\n",
    "            html_ranks = p.find_all(\"td\", attrs={'class': None})\n",
    "            for r in range(len(html_ranks)):\n",
    "                temp_fp_ranking = [shortName, playerId, TEAM, POS] \n",
    "\n",
    "                temp_fp_ranking.insert(0, html_ranks[r].text) # ranking\n",
    "                temp_fp_ranking.insert(0, experts[r][0]) # expert\n",
    "                temp_fp_ranking.insert(0, k) # group ranking set\n",
    "                temp_fp_ranking.insert(0, week) #season week\n",
    "                temp_fp_ranking.insert(0, season) #season year\n",
    "                temp_fp_ranking.insert(0, experts[r][1]) # date\n",
    "                temp_fp_ranking.insert(0, \"fantasyPros\") # outlet\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                temp_fp_ranking.append(np.nan)\n",
    "                all_ranks.append(temp_fp_ranking)\n",
    "                \n",
    "df_fp_ranking = pd.DataFrame(all_ranks, columns=ranking_columns).replace(\"-\", np.nan).replace(\"N/A\", np.nan)\n",
    "df_fp_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e583f8f-d9c1-4d80-ac05-75eb64a0af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_ranking.to_csv(\"../data/ranking/weekly/fp_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d912a6-b546-4b95-8b7c-0cce17a9d432",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ----------- ESPN - HPPR -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1d7c0-c74f-4776-be69-ee20695b8476",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ESPN Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ab5ff7-522e-4bc9-b70b-76b152527760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "# open the initial projection page\n",
    "url_espn_proj = \"https://fantasy.espn.com/football/players/projections?leagueFormatId=3\"\n",
    "                \n",
    "driver.get(url_espn_proj) \n",
    "# sleep to let the html load\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "try:\n",
    "    # changing to the desired projection view\n",
    "    button = driver.find_element(By.XPATH, \"//button[@class='Button Button--filter player--filters__projections-button']\")\n",
    "    button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # grabs the entire pages html\n",
    "    html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = bs(html)\n",
    "    \n",
    "    # grabbing the number of pages there are in the projections\n",
    "    pagenation_list = soup.find(\"div\", class_=\"Pagination__wrap overflow-x-auto\")\n",
    "    pages = pagenation_list.find_all(\"li\")\n",
    "    last_page = pages[-1].text\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "    \n",
    "\n",
    "espn_player_proj_player = []\n",
    "page_count1 = 0\n",
    "page_count2 = 0\n",
    "\n",
    "for page in range(1, 18):  #int(last_page)+1):\n",
    "    try:\n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        # grabbing the projection tables\n",
    "        tables = soup.find_all(\"table\")\n",
    "        \n",
    "        # the player info table\n",
    "        for tr in tables[0].find_all(\"tr\"):\n",
    "            for td in tr:\n",
    "                if td.find(\"a\", class_=\"AnchorLink link clr-link pointer\"):\n",
    "                    \n",
    "                    #grabs the ESPN player id from the image url\n",
    "                    playerId = td.find(\"img\")['src'].split(\"/\")[-1].split(\".\")[0]\n",
    "                    #dst has player ID as the team abbreviation. This catches it\n",
    "                    try:\n",
    "                        int(playerId)\n",
    "                    except:\n",
    "                        playerId = \"\"\n",
    "                        \n",
    "                    name = td.find(\"a\", class_=\"AnchorLink link clr-link pointer\").text.replace(\".\", \"\")\n",
    "                    position = td.find(\"span\", class_=\"playerinfo__playerpos ttu\").text.replace(\"/\",\"\").split(\",\")[0]\n",
    "                    team = td.find(\"span\", class_=\"playerinfo__playerteam\").text.upper()\n",
    "\n",
    "                    espn_player_proj_player.append([\"espn\", today, season, week, playerId, name, np.nan, position, team, np.nan])\n",
    "\n",
    "\n",
    "        # the stat projection table\n",
    "        for tr in tables[1].find_all(\"tr\",class_=\"Table__TR Table__TR--lg Table__odd\"):\n",
    "            comp_att = tr.find(\"div\", {\"title\":\"Each Pass Completed & Each Pass Attempted\"}).text.split(\"/\")\n",
    "            pass_comps = comp_att[0]\n",
    "            pass_atts = comp_att[1]\n",
    "            pass_yds = tr.find(\"div\", {\"title\":\"Passing Yards\"}).text\n",
    "            pass_tds = tr.find(\"div\", {\"title\":\"TD Pass\"}).text\n",
    "            ints = tr.find(\"div\", {\"title\":\"Interceptions Thrown\"}).text\n",
    "            rush_atts = tr.find(\"div\", {\"title\":\"Rushing Attempts\"}).text\n",
    "            rush_yds = tr.find(\"div\", {\"title\":\"Rushing Yards\"}).text\n",
    "            rush_tds = tr.find(\"div\", {\"title\":\"TD Rush\"}).text\n",
    "            rec = tr.find(\"div\", {\"title\":\"Each reception\"}).text\n",
    "            rec_yds = tr.find(\"div\", {\"title\":\"Receiving Yards\"}).text\n",
    "            rec_tds = tr.find(\"div\", {\"title\":\"TD Reception\"}).text\n",
    "            rec_trgts = tr.find(\"div\", {\"title\":\"Receiving Target\"}).text\n",
    "            \n",
    "            espn_player_proj_player[page_count1].extend([pass_atts, pass_comps,pass_yds, 0, pass_tds,\n",
    "                                                         ints, 0, rush_atts,rush_yds,0, rush_tds,rec_trgts,rec,rec_yds,0,0,rec_tds,\n",
    "                                                        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "            page_count1 += 1\n",
    "       \n",
    "        # the fantasy points table\n",
    "                                        \n",
    "        for tr in tables[2].find_all(\"tr\",class_=\"Table__TR Table__TR--lg Table__odd\"):\n",
    "            for div in tr.find_all(\"div\"):\n",
    "                # some of the free agents/retired players don't have div[\"title\"] need to catch them with try\n",
    "                try:\n",
    "                    if 'point' in div['title']:\n",
    "                        total_ff_pts = div.find(\"span\").text\n",
    "                        avg_ff_pts = 0\n",
    "                except:\n",
    "                    total_ff_pts = 0\n",
    "                    avg_ff_pts = 0\n",
    "\n",
    "            espn_player_proj_player[page_count2].extend([total_ff_pts, avg_ff_pts])\n",
    "            page_count2 += 1\n",
    "        \n",
    "        #checks for last page\n",
    "        \n",
    "        if page < int(last_page):\n",
    "            # jumping to the next page\n",
    "            nextButton = driver.find_element(By.XPATH, \"//button[@class='Button Button--default Button--icon-noLabel Pagination__Button Pagination__Button--next']\")\n",
    "            nextButton.click()\n",
    "            time.sleep(10)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        driver.close()\n",
    "\n",
    "try:\n",
    "    driver.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# creating df from gathered data to merge into final df that matches the cbs structure\n",
    "temp_proj = pd.DataFrame(espn_player_proj_player, columns = projection_columns)\n",
    "                        \n",
    "\n",
    "df_espn_proj = pd.DataFrame(columns = projection_columns)\n",
    "\n",
    "#final espn projections data\n",
    "df_espn_proj = pd.concat([df_espn_proj, temp_proj]).replace(\"--\", 0)\n",
    "df_espn_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0f49b6-b1fc-4d77-98f0-83f5bb7abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_espn_proj.to_csv(\"../data/projection/weekly/espn_proj_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2d618-1994-4c74-a640-78b30f3e1710",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ESPN Rankings  - need to update urls for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bcd408-9f70-40af-9739-e73fadc676b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\1680112104.py:141: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_espn_ranking = pd.concat([df_espn_ranking, temp_df], axis = 0, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1850, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "espn_ranking_urls = {\n",
    "\"QB\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankQBPPR/nfl-fantasy-football-rankings-2023-qb-quarterback\",\n",
    "\"RB\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankRBPPR/nfl-fantasy-football-rankings-2023-rb-running-back\",\n",
    "\"WR\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankWRPPR/nfl-fantasy-football-rankings-2023-wr-wide-receiver\",\n",
    "\"TE\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankTEPPR/nfl-fantasy-football-rankings-2023-te-tight-end\",\n",
    "\"K\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankKPPR/nfl-fantasy-football-rankings-2023-kicker\",\n",
    "\"DST\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankDSTPPR/nfl-fantasy-football-rankings-2023-dst-defense\",\n",
    "\"IDP\":\"https://www.espn.com/fantasy/football/story/_/page/23ffweekrankIDP/nfl-fantasy-football-rankings-2023-idp-defense-dl-lb-db\",\n",
    "}\n",
    "\n",
    "# final dataframe structure to hosue all the rankings\n",
    "df_espn_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "#try to closed driver if there are any errors\n",
    "try:\n",
    "    # looping through the urls to aggregate the rankings\n",
    "    for group, url in espn_ranking_urls.items():\n",
    "        # opening the webpage and allowing the scripts to load for the HTML to be accessed\n",
    "\n",
    "        url_espn_formatted = url.format(season=season)\n",
    "        driver.get(url_espn_formatted)\n",
    "        time.sleep(10)\n",
    "\n",
    "        # grabs the entire pages html\n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        # this will hold a list of list. One list will be a players rank for a single expert\n",
    "        player_ranks = []\n",
    "\n",
    "        # IDP page has 3 separate tables for positions instead of a single position on the page and a single table handled in the else below\n",
    "        if group == \"IDP\":\n",
    "            ranking_tables = soup.find_all(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "            count = 0 # hard coded the positions based on the which table the site holds them in\n",
    "\n",
    "            # 3 tables for the 3 IDPs  DL, LB, DB\n",
    "            for ranking_table in ranking_tables:\n",
    "\n",
    "                # retrieves the expert names and the order they are listed\n",
    "                expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "                expert_names = []\n",
    "                for tr in range(2, len(expert_names_html)-1):\n",
    "                    expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "                player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "                for tr in player_ranks_html:\n",
    "\n",
    "                    tds = tr.find_all(\"td\")\n",
    "\n",
    "                    playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "                    name = tds[0].find(\"a\").text.replace(\".\", \"\")\n",
    "\n",
    "                    if count == 0:\n",
    "                        POS = \"DL\"\n",
    "                    elif count == 1:\n",
    "                        POS = \"LB\"\n",
    "                    elif count == 2:\n",
    "                        POS = \"DB\"\n",
    "\n",
    "                    # try block to handle injury designations that the site puts in the same text as the team name\n",
    "                    try:\n",
    "                        #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "                        injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "                        if len(injury) > 1:  # Accounts for suspended tag \"SSPD\"\n",
    "                            team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-4]\n",
    "                        else:\n",
    "                            team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-1]\n",
    "\n",
    "                    except:\n",
    "                        team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "                    for i in range(len(expert_names)):\n",
    "\n",
    "                        # expert name from the list generated from thead\n",
    "                        expert = expert_names[i]\n",
    "                        # position of the expert ranking column in tbody\n",
    "                        idx = i + 2\n",
    "\n",
    "                        # retrieves the expert rank from tbody rows\n",
    "                        exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "                        player_ranks.append([\"espn\", today, season, week, POS, expert, exRank, name, playerId, team,  POS, np.nan, np.nan])\n",
    "\n",
    "                count += 1\n",
    "\n",
    "\n",
    "        # for position specific rankings\n",
    "        else:\n",
    "\n",
    "            ranking_table = soup.find(\"table\", class_=\"inline-table rankings-table fullWidth sortable\")\n",
    "            #driver.close()\n",
    "\n",
    "            # retrieves the expert names and the order they are listed\n",
    "            expert_names_html = ranking_table.find(\"thead\").find_all(\"th\")\n",
    "            expert_names = []\n",
    "            for tr in range(2, len(expert_names_html)-1):\n",
    "                expert_names.append(expert_names_html[tr].text)\n",
    "\n",
    "            player_ranks_html = ranking_table.find(\"tbody\").find_all(\"tr\", class_=\"\")\n",
    "            for tr in player_ranks_html:\n",
    "\n",
    "                tds = tr.find_all(\"td\")\n",
    "\n",
    "                playerId = tds[0].find(\"a\")[\"data-player-id\"]\n",
    "                if group == \"DST\":\n",
    "                    name = tds[0].find(\"a\").text.split()[0].replace(\".\", \"\")\n",
    "                else:\n",
    "                    name = tds[0].find(\"a\").text.replace(\".\", \"\")\n",
    "\n",
    "                POS = group\n",
    "\n",
    "                #team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "                # try block to handle injury designations that the site puts in the same text as the team name\n",
    "                try:\n",
    "                    #if there is a injury designation, it retrieves it and then removes it from the team name\n",
    "                    injury = tds[0].find_all(\"div\", class_=\"rank\")[0].find(\"span\").text\n",
    "                    if len(injury) > 1:  # Accounts for suspended tag \"SSPD\"\n",
    "                        team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-4]\n",
    "                    else:\n",
    "                        team = tds[0].find_all(\"div\", class_=\"rank\")[0].text.split(\",\")[1].strip().upper()[:-1]\n",
    "\n",
    "                except:\n",
    "                    team = tds[0].find(\"div\", class_=\"rank\").text.split(\",\")[1].strip().upper()\n",
    "\n",
    "                for i in range(len(expert_names)-1):\n",
    "\n",
    "                    # expert name from the list generated from thead\n",
    "                    expert = expert_names[i]\n",
    "                    # position of the expert ranking column in tbody\n",
    "                    idx = i + 2\n",
    "\n",
    "                    # retrieves the expert rank from tbody rows\n",
    "                    exRank = pd.to_numeric(tds[idx].text, errors='coerce')\n",
    "\n",
    "                    player_ranks.append([\"espn\", today, season, week, group, expert, exRank, name, playerId, team,  POS, np.nan, np.nan])\n",
    "\n",
    "\n",
    "        temp_df = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "        df_espn_ranking = pd.concat([df_espn_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()    \n",
    "    \n",
    "driver.close() \n",
    "df_espn_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48b922ae-0c4b-44b3-a14a-9c2a6ad4d6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_espn_ranking.to_csv(\"../data/ranking/weekly/espn_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ba735-05c5-468d-b4a1-e6aef0481c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ------------ NFL --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d90e5-0fae-485b-9960-34d458d51f47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NFL projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f062d3-340a-4dc9-bc85-ae588209cb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# position=  0:QB,RB,WR,TE  7:Kicker, 8:D\n",
    "nfl_proj_url = [\n",
    "\"https://fantasy.nfl.com/research/projections?offset={offset}&position=0&statCategory=projectedStats&statSeason={season}&statType=weekProjectedStats&statWeek={week}\",\n",
    "\"https://fantasy.nfl.com/research/projections?offset={offset}&position=7&statCategory=projectedStats&statSeason={season}&statType=weekProjectedStats&statWeek={week}\",\n",
    "\"https://fantasy.nfl.com/research/projections?offset={offset}&position=8&statCategory=projectedStats&statSeason={season}&statType=weekProjectedStats&statWeek={week}\",\n",
    "]\n",
    "\n",
    "\n",
    "df_nfl_proj = pd.DataFrame(columns=projection_columns)\n",
    "player_data = []\n",
    "# count will be updated to the player count after the first page load \n",
    "# this is being used to avoid loading more pages than needed\n",
    "count = 3000\n",
    "\n",
    "#looping through the 3 URLs, the site has QB,RB,WR,TE combined in a single list and then K and D on their own pages\n",
    "for i in range(3):\n",
    "    if i == 0:  # this will handle the offensive players\n",
    "        while count > 25:\n",
    "            \n",
    "            # this grabs the first page, else will handle all others\n",
    "            if count == 3000:\n",
    "                time.sleep(1)\n",
    "                r = requests.get(nfl_proj_url[0].format(offset=1, season=season, week=week))\n",
    "                soup = bs(r.text)\n",
    "\n",
    "                # grabs the number of players with projections on the site. pagenated at 25 a page\n",
    "                player_count = int(soup.find(\"span\", class_=\"paginationTitle\").text.split(\"of\")[-1].strip())\n",
    "                count = player_count\n",
    "\n",
    "                table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "\n",
    "                body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                for tr in body_trs:\n",
    "                    data = tr.find_all(\"td\")\n",
    "\n",
    "                    firstColA = data[0].find('a')\n",
    "                    playerId = firstColA['href'].split(\"=\")[2]\n",
    "                    fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                    posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                    pos = posAndTeam[0].strip()\n",
    "                    try:\n",
    "                        team = posAndTeam[1].strip()\n",
    "                    except:\n",
    "                        team = \"FA\"\n",
    "\n",
    "                    \n",
    "                    PassingYards = data[2].text\n",
    "                    TouchdownsPasses = data[3].text\n",
    "                    InterceptionsThrown = data[4].text\n",
    "                    RushingYards = data[5].text\n",
    "                    RushingTouchdowns = data[6].text\n",
    "                    Receptions = data[7].text\n",
    "                    ReceivingYards = data[8].text\n",
    "                    ReceivingTouchdowns = data[9].text\n",
    "                    retTd = data[10].text\n",
    "                    fumTd = data[11].text\n",
    "                    twoPt= data[12].text\n",
    "                    FumblesLost = data[13].text\n",
    "                    FantasyPoints = data[14].text\n",
    "\n",
    "                    temp = [\"nfl\", today, season, week, playerId,fullName,np.nan,pos,team,0,0,0,PassingYards,0,TouchdownsPasses, InterceptionsThrown,\n",
    "                            0,0,RushingYards,0,RushingTouchdowns,0,Receptions,ReceivingYards,0,0,ReceivingTouchdowns,\n",
    "                            FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,twoPt,FantasyPoints,0]\n",
    "                    player_data.append(temp)\n",
    "\n",
    "            else:\n",
    "                for j in range(26, player_count, 25):\n",
    "                    time.sleep(1)\n",
    "                    r = requests.get(nfl_proj_url[0].format(offset=j, season=season, week=week))\n",
    "                    soup = bs(r.text)\n",
    "                    table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "                    body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                    for tr in body_trs:\n",
    "                        data = tr.find_all(\"td\")\n",
    "\n",
    "                        firstColA = data[0].find('a')\n",
    "                        playerId = firstColA['href'].split(\"=\")[2]\n",
    "                        fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                        posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                        pos = posAndTeam[0].strip()\n",
    "                        try:\n",
    "                            team = posAndTeam[1].strip()\n",
    "                        except:\n",
    "                            team = \"FA\"\n",
    "\n",
    "                        PassingYards = data[2].text\n",
    "                        TouchdownsPasses = data[3].text\n",
    "                        InterceptionsThrown = data[4].text\n",
    "                        RushingYards = data[5].text\n",
    "                        RushingTouchdowns = data[6].text\n",
    "                        Receptions = data[7].text\n",
    "                        ReceivingYards = data[8].text\n",
    "                        ReceivingTouchdowns = data[9].text\n",
    "                        retTd = data[10].text\n",
    "                        fumTd = data[11].text\n",
    "                        twoPt= data[12].text\n",
    "                        FumblesLost = data[13].text\n",
    "                        FantasyPoints = data[14].text\n",
    "\n",
    "                        temp = [\"nfl\", today, season, week, playerId,fullName,np.nan,pos,team,0,0,0,PassingYards,0,TouchdownsPasses, InterceptionsThrown,\n",
    "                                0,0,RushingYards,0,RushingTouchdowns,0,Receptions,ReceivingYards,0,0,ReceivingTouchdowns,\n",
    "                                FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,twoPt,FantasyPoints,0]\n",
    "                        player_data.append(temp)\n",
    "\n",
    "                    count -= 25\n",
    "                    \n",
    "    else: # this will handle K and D\n",
    "        for j in range(2):  \n",
    "            \n",
    "            time.sleep(1)\n",
    "            r = requests.get(nfl_proj_url[i].format(offset=j*25+1, season=season, week=week))  # k and d only have 2 pages, j *25 + 1 handles the url offset that pagenates\n",
    "            soup = bs(r.text)\n",
    "\n",
    "            table = soup.find_all(\"table\", class_=\"tableType-player hasGroups\")\n",
    "\n",
    "            body_trs = table[0].find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "            for tr in body_trs:\n",
    "                data = tr.find_all(\"td\")\n",
    "                temp = []\n",
    "                \n",
    "                firstColA = data[0].find('a')\n",
    "                playerId = firstColA['href'].split(\"=\")[2]\n",
    "                fullName = firstColA.text.strip().replace(\".\", \"\")\n",
    "\n",
    "                posAndTeam = data[0].find('em').text.split(\"-\")\n",
    "                \n",
    "                \n",
    "                if i == 1:  # K url\n",
    "                    \n",
    "                    pos = posAndTeam[0].strip()\n",
    "                    try:\n",
    "                        team = posAndTeam[1].strip()\n",
    "                    except:\n",
    "                        team = \"FA\"\n",
    "                        \n",
    "                    xpMade = data[2].text\n",
    "                    made0_19 = float(data[3].text.replace(\"-\", \"0\"))\n",
    "                    made20_29 = float(data[4].text.replace(\"-\", \"0\"))\n",
    "                    made30_39 = float(data[5].text.replace(\"-\", \"0\"))\n",
    "                    made40_49 = float(data[6].text.replace(\"-\", \"0\"))\n",
    "                    made50 = float(data[7].text.replace(\"-\", \"0\"))\n",
    "                    fgMade = made0_19 + made20_29 + made30_39 + made40_49 + made50\n",
    "                    FantasyPoints = data[8].text\n",
    "\n",
    "                    temp = [\"nfl\", today, season, week, playerId,fullName,np.nan,pos,team,0,0,0,0,0,0, 0,\n",
    "                                    0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "                                    fgMade,0,0,made0_19,0,made20_29,0,made30_39,0,made40_49,0,made50,0,xpMade,0,0,\n",
    "                                     0,0,0,0,0,0,0,0,0,0,0,0,0,0,FantasyPoints,0]\n",
    "                    player_data.append(temp)    \n",
    "                        \n",
    "                else: # D url\n",
    "                    pos = 'DST'\n",
    "                    team = fullName\n",
    "                    sacks = data[2].text\n",
    "                    interceptions = data[3].text\n",
    "                    fum = data[4].text\n",
    "                    safety = data[5].text\n",
    "                    defTd = data[6].text\n",
    "                    twoPt = data[7].text\n",
    "                    retTd = data[8].text\n",
    "                    ptsAllowed= data[9].text\n",
    "                    fantasyPts= data[10].text\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    temp = [\"nfl\", today, season, week, playerId,np.nan,np.nan,pos,team,0,0,0,0,0,0,0,\n",
    "                            0,0,0,0,0,0,0,0,0,0,0,\n",
    "                            FumblesLost,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,interceptions,safety,sacks,0,fum,0,\n",
    "                            defTd, retTd,ptsAllowed,0,0,0,0,0,twoPt,fantasyPts,0]\n",
    "                    \n",
    "                    player_data.append(temp)\n",
    "        \n",
    "df_nfl_proj = pd.DataFrame(player_data, columns=projection_columns).replace(\"-\",0)\n",
    "df_nfl_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0785e8c9-b294-44a8-82f6-2df4392579a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_proj.to_csv(\"../data/projection/weekly/nfl_proj_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72bb1a-23e1-42d0-b858-a87f7d5280be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NFL rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305c60d3-1ed1-4744-9f54-9ed6ab23e221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_rank_url = {\n",
    "    \"QB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=QB&sort=1&statType=weekStats&week={week}\",     \n",
    "    \"RB\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=RB&sort=1&statType=weekStats&week={week}\",\n",
    "    \"WR\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=WR&sort=1&statType=weekStats&week={week}\",\n",
    "    \"TE\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=TE&sort=1&statType=weekStats&week={week}\",\n",
    "    \"K\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=K&sort=1&statType=weekStats&week={week}\",\n",
    "    \"DST\":\"https://fantasy.nfl.com/research/rankings?leagueId=0&position=DEF&sort=1&statType=weekStats&week={week}\"\n",
    "}\n",
    "\n",
    "df_nfl_ranking = pd.DataFrame(columns=ranking_columns)\n",
    "\n",
    "for k,v in nfl_rank_url.items():\n",
    "    \n",
    "    time.sleep(1)\n",
    "    r = requests.get(nfl_rank_url[k].format(week=week))\n",
    "    soup = bs(r.text)\n",
    "\n",
    "    # grabs the number of players with projections on the site. pagenated at 25 a page\n",
    "    rank_table = soup.find(\"table\", class_=\"tableType-player noGroups\").find(\"tbody\")\n",
    "\n",
    "    player_ranks = []\n",
    "    for tr in rank_table.find_all(\"tr\"):\n",
    "        player_data = []\n",
    "        td = tr.find_all(\"td\")\n",
    "        \n",
    "        pos_rank = int(td[0].text)\n",
    "        playerId = int(td[1].find(\"a\")['href'].split(\"=\")[-1])\n",
    "        full_name = td[1].find(\"a\").text.replace(\".\", \"\")\n",
    "        \n",
    "        pos = td[1].find(\"em\").text.split(\"-\")[0].strip()\n",
    "        \n",
    "        if k == \"DST\":\n",
    "            team = \"\"\n",
    "            pos = \"DST\"\n",
    "        \n",
    "        else:\n",
    "            # no team name for FAs\n",
    "            try:\n",
    "                team = td[1].find(\"em\").text.split(\"-\")[1].strip()\n",
    "            except:\n",
    "                team = \"FA\"\n",
    "            \n",
    "        ovr_rank = int(td[-1].text)\n",
    "\n",
    "        player_data = [\"nfl\", today, season, week, k, \"nfl\", pos_rank, full_name, playerId, team, pos, np.nan, np.nan]\n",
    "        player_ranks.append(player_data)\n",
    "\n",
    "    temp_df = pd.DataFrame(player_ranks, columns=ranking_columns)\n",
    "    df_nfl_ranking = pd.concat([df_nfl_ranking, temp_df], axis = 0, ignore_index=True)\n",
    "    \n",
    "df_nfl_ranking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dab273ae-b631-44ab-86f4-a7a690845559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_ranking.to_csv(\"../data/ranking/weekly/nfl_rank_{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98ffd1-867c-4ae9-ae0e-48802134d9be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Weekly Bettings Lines from Betting Pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "620ca20e-866d-4631-9fb9-5e6a207da033",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "\n",
    "driver.get(urls[0].format(season=season, week=week)) \n",
    "# sleep to let the html load\n",
    "time.sleep(10)\n",
    "\n",
    "html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "soup = bs(html)\n",
    "\n",
    "data = soup.find_all(\"div\", class_=\"flex odds-offer\")\n",
    "\n",
    "matchups = []\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e274a60-40d1-401e-acb9-54c0d54d500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3854e203-519c-488c-bbc5-9237a09f0c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"typography odds-cell__cost\" data-v-e0bd5419=\"\" data-v-f8c6c06d=\"\" style=\"--64dcb17e: #16191D; --0b8a2e86: 1.1rem; --584c03bd: 600; --faa089fc: left; --97ef6e2e: none; --da5e331a: normal;\">(-110)</span>,\n",
       " <span class=\"typography odds-cell__cost\" data-v-e0bd5419=\"\" data-v-f8c6c06d=\"\" style=\"--64dcb17e: #16191D; --0b8a2e86: 1.1rem; --584c03bd: 600; --faa089fc: left; --97ef6e2e: none; --da5e331a: normal;\">(-110)</span>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_team = data[0].find_all(\"a\", class_=\"team-overview__team-name\")\n",
    "data_spread = data[0].find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "# team 1 data - away\n",
    "away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title().strip()\n",
    "away_spread_line = data_spread[2].find_all(\"span\", class_=\"odds-cell__line\")#[1].text.replace(\"+\", \"\")\n",
    "\n",
    "data_spread[0].find_all(\"span\", class_=\"odds-cell__cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6b82b-cbe5-4e5b-b580-46d17f328535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee3c58b1-2253-4f0b-a29a-5dd763de1b02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 13)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "urls = [\n",
    "    r'https://www.bettingpros.com/nfl/odds/spread/?season={season}&week={week}',\n",
    "    r\"https://www.bettingpros.com/nfl/odds/moneyline/?season={season}&week={week}\",\n",
    "    r\"https://www.bettingpros.com/nfl/odds/total/?season={season}&week={week}\"\n",
    "]\n",
    "  \n",
    "# empty, to be filled with data from websites\n",
    "df_lines = pd.DataFrame(columns=[\"date\", \"season\", \"week\", \"overUnder\", \"overUnderCost\" \n",
    "                                                \"awayTeam\", \"awaySpread\", \"awayCost\", \"awayMoneyline\",  \n",
    "                                                \"homeTeam\", \"homeSpread\", \"homeCost\", \"homeMoneyLine\"]) \n",
    "# try to closed driver on error\n",
    "try:\n",
    "    for i in range(len(urls)):\n",
    "        driver.get(urls[i].format(season=season, week=week)) \n",
    "        # sleep to let the html load\n",
    "        time.sleep(10)\n",
    "\n",
    "        html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = bs(html)\n",
    "\n",
    "        data = soup.find_all(\"div\", class_=\"flex odds-offer\")\n",
    "\n",
    "        matchups = []\n",
    "        # spreads\n",
    "        if i == 0:\n",
    "\n",
    "            for div in data:\n",
    "\n",
    "                data_team = div.find_all(\"a\", class_=\"team-overview__team-name\")\n",
    "                data_spread = div.find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "                # team 1 data - away\n",
    "                away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                away_spread_line = data_spread[0].find_all(\"span\", class_=\"odds-cell__line\")[0].text.replace(\"+\", \"\")\n",
    "\n",
    "                if (away_spread_line == \"NL\") or (away_spread_line == \"--\"):\n",
    "                    away_spread_line = np.nan\n",
    "\n",
    "                away_spread_cost = data_spread[0].find_all(\"span\", class_=\"odds-cell__cost\")[0].text.strip().replace(\"+\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                if (away_spread_cost == \"NL\") or (away_spread_cost == \"--\"):\n",
    "                    away_spread_cost = np.nan\n",
    "\n",
    "                #team 2 data - home\n",
    "                home_team = data_team[1]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                home_spread_line = data_spread[0].find_all(\"span\", class_=\"odds-cell__line\")[1].text.replace(\"+\", \"\")\n",
    "\n",
    "                if (home_spread_line == \"NL\") or (home_spread_line == \"--\"):\n",
    "                    home_spread_line = np.nan\n",
    "\n",
    "                home_spread_cost = data_spread[0].find_all(\"span\", class_=\"odds-cell__cost\")[1].text.strip().replace(\"+\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                if (home_spread_cost == \"NL\") or (home_spread_cost == \"--\"):\n",
    "                    home_spread_cost = np.nan\n",
    "\n",
    "                matchup = [today, season, week,\n",
    "                           np.nan, np.nan,  # placeholder for overUnders\n",
    "                           away_team, float(away_spread_line), away_spread_cost, np.nan,\n",
    "                           home_team, float(home_spread_line), home_spread_cost, np.nan\n",
    "                          ]\n",
    "\n",
    "                matchups.append(matchup)\n",
    "\n",
    "            df_lines = pd.DataFrame(matchups, columns=[\"date\", \"season\", \"week\", \"overUnder\", \"overUnderCost\",\n",
    "                                                    \"awayTeam\", \"awaySpread\", \"awayCost\", \"awayMoneyline\",  \n",
    "                                                    \"homeTeam\", \"homeSpread\", \"homeCost\", \"homeMoneyLine\"])\n",
    "\n",
    "        #moneylines\n",
    "        elif i == 1:\n",
    "            for div in data:\n",
    "\n",
    "                data_team = div.find_all(\"a\", class_=\"team-overview__team-name\")\n",
    "                data_moneylines = div.find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "                # team 1 data - away\n",
    "                away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "                away_moneyline = data_moneylines[1].find_all(\"span\", class_=\"odds-cell__line\")[0].text.strip().replace(\"+\", \"\")\n",
    "                if (away_moneyline == \"NL\") or (away_moneyline == \"--\"):\n",
    "                    away_moneyline = np.nan\n",
    "                elif away_moneyline == \"EVEN\":\n",
    "                    away_moneyline = 0\n",
    "                else:\n",
    "                    away_moneyline = float(away_moneyline)\n",
    "\n",
    "                #team 2 data - home\n",
    "                home_moneyline = data_moneylines[1].find_all(\"span\", class_=\"odds-cell__line\")[1].text.strip().replace(\"+\", \"\")\n",
    "                if (home_moneyline == \"NL\") or (home_moneyline == \"--\"):\n",
    "                    home_moneyline = np.nan\n",
    "                elif home_moneyline == \"EVEN\":\n",
    "                    home_moneyline = 0\n",
    "                else:\n",
    "                    home_moneyline = float(home_moneyline)\n",
    "\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'awayMoneyline'] = away_moneyline\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'homeMoneyLine'] = home_moneyline\n",
    "\n",
    "        # totals\n",
    "        elif i == 2:\n",
    "\n",
    "            for div in data:\n",
    "\n",
    "                data_team = div.find_all(\"a\", class_=\"team-overview__team-name\")\n",
    "                data_spread = div.find_all(\"div\", class_=\"flex odds-offer__item\")\n",
    "\n",
    "                # team 1 data - away\n",
    "                away_team = data_team[0]['href'].split(\"/\")[3].replace(\"-\", \" \").title()\n",
    "\n",
    "                overUnder_line = data_spread[0].find_all(\"span\", class_=\"odds-cell__line\")[0].text.strip()\n",
    "                if (overUnder_line == \"NL\") or (overUnder_line == \"--\"):\n",
    "                    overUnder_line = np.nan\n",
    "                else:\n",
    "                    overUnder_line = float(overUnder_line.replace(\"+\", \"\").split(\" \")[1])\n",
    "\n",
    "\n",
    "\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'overUnder'] = overUnder_line\n",
    "                df_lines.loc[df_lines[df_lines['awayTeam'] == away_team].index[0],'overUnderCost'] = -110\n",
    "\n",
    "    df_lines = df_lines.replace(np.nan, None).replace(\"EVEN\", 100)\n",
    "    df_lines.to_csv(\"../data/betting/weekly/lines{season}-{week}_{date}.csv\".format(season=season, week=strWeek, date=today), \n",
    "                    index=False, )\n",
    "\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    driver.close()\n",
    "\n",
    "driver.close()  \n",
    "\n",
    "df_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a36ec-c26f-45e5-8581-6fe77361c70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7ea1c-8542-4e5c-ab55-9c370f23ff65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# END OF WEEK SCORES AND STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ed20032-a671-4e7f-b588-06f9aa31ca85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_scores = r'https://www.cbssports.com/nfl/scoreboard/all/{season}/regular/{week}/'\n",
    "prev_week = week - 1\n",
    "# retreiving HTML and converting it to soup\n",
    "r = requests.get(url_scores.format(season=season, week=str(prev_week)))\n",
    "soup = bs(r.text)\n",
    "\n",
    "# accessing tables with the data\n",
    "tables = soup.find_all(\"div\", class_=\"live-update\")\n",
    "\n",
    "\n",
    "records = pd.DataFrame(columns = [\"season\", \"week\", \"team\", \"wins\", \"losses\", \"ties\"])\n",
    "games = pd.DataFrame(columns=[\"season\", \"week\", \"homeTeam\", \"homeQ1Pts\", \"homeQ2Pts\", \"homeQ3Pts\", \"homeQ4Pts\", \"homeTotalPts\",\n",
    "        \"awayTeam\", \"awayQ1Pts\", \"awayQ2Pts\", \"awayQ3Pts\", \"awayQ4Pts\", \"awayTotalPts\",\n",
    "        \"homeWinner\", \"awayWinner\", \"tie\", \"winningTeam\", \"gameId\"])\n",
    "\n",
    "for t in tables:\n",
    "    game = t.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "    # away team \n",
    "    away = game[0]\n",
    "    away_team = away.find(\"a\")['href'].split(\"/\")[3]\n",
    "    away_record = away.find(\"span\", class_=\"record\").text.split(\"-\")\n",
    "    away_win = away_record[0]\n",
    "    away_loss = away_record[1]\n",
    "    if len(away_record) == 3:\n",
    "        away_tie = away_record[2]\n",
    "    else: \n",
    "        away_tie = 0\n",
    "    away_scores = away.find_all(\"td\")\n",
    "    away_q1Pts = int(away_scores[1].text)\n",
    "    away_q2Pts = int(away_scores[2].text)\n",
    "    away_q3Pts = int(away_scores[3].text)\n",
    "    away_q4Pts = int(away_scores[4].text)\n",
    "    away_totalPts = int(away_scores[5].text)\n",
    "\n",
    "    # home team \n",
    "    home = game[1]\n",
    "    home_team = home.find(\"a\")['href'].split(\"/\")[3]\n",
    "    home_record = home.find(\"span\", class_=\"record\").text.split(\"-\")\n",
    "    home_win = home_record[0]\n",
    "    home_loss = home_record[1]\n",
    "    if len(home_record) == 3:\n",
    "        home_tie = home_record[2]\n",
    "    else: \n",
    "        home_tie = 0\n",
    "    home_scores = home.find_all(\"td\")\n",
    "    home_q1Pts = int(home_scores[1].text)\n",
    "    home_q2Pts = int(home_scores[2].text)\n",
    "    home_q3Pts = int(home_scores[3].text)\n",
    "    home_q4Pts = int(home_scores[4].text)\n",
    "    home_totalPts = int(home_scores[5].text)\n",
    "\n",
    "    # designating winner or tie\n",
    "    home_winner = 0\n",
    "    away_winner = 0\n",
    "    tie = 0\n",
    "    winning_team = np.nan\n",
    "\n",
    "    if home_totalPts > away_totalPts:\n",
    "        home_winner = 1\n",
    "        winning_team = home_team\n",
    "    elif home_totalPts < away_totalPts:\n",
    "        away_winner = 1\n",
    "        winning_team = away_team\n",
    "    else:\n",
    "        tie = 1\n",
    "    \n",
    "    # gathering data into list for df creation and creating temp dfs to concat with main df\n",
    "    home_records = [season, prev_week, home_team, home_win, home_loss, home_tie]\n",
    "    away_records = [season, prev_week, away_team, away_win, away_loss, away_tie]\n",
    "    game_data = [season, week-1, home_team, home_q1Pts, home_q2Pts, home_q3Pts, home_q4Pts, home_totalPts,\n",
    "                 away_team, away_q1Pts, away_q2Pts, away_q3Pts, away_q4Pts, away_totalPts,\n",
    "                home_winner, away_winner, tie, winning_team]\n",
    "\n",
    "     \n",
    "    temp_games = pd.DataFrame([game_data], columns=[\"season\", \"week\", \"homeTeam\", \"homeQ1Pts\", \"homeQ2Pts\", \"homeQ3Pts\", \"homeQ4Pts\", \"homeTotalPts\",\n",
    "        \"awayTeam\", \"awayQ1Pts\", \"awayQ2Pts\", \"awayQ3Pts\", \"awayQ4Pts\", \"awayTotalPts\",\n",
    "        \"homeWinner\", \"awayWinner\", \"tie\", \"winningTeam\"])\n",
    "    \n",
    "    cols = ['season', 'week', 'homeTeam', 'awayTeam']\n",
    "    temp_games['gameId'] = temp_games[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    temp_games = temp_games.replace(\"WAS\", \"WSH\").replace(\"JAC\", \"JAX\")\n",
    "    \n",
    "    games = pd.concat([games, temp_games])\n",
    "    \n",
    "    temp_records_h = pd.DataFrame([home_records], columns = [\"season\", \"week\", \"team\", \"wins\", \"losses\", \"ties\"])\n",
    "    temp_records_a = pd.DataFrame([away_records], columns = [\"season\", \"week\", \"team\", \"wins\", \"losses\", \"ties\"])\n",
    "    temp_records_h = pd.concat([temp_records_h, temp_records_a])\n",
    "    temp_records_h = temp_records_h.replace(\"WAS\", \"WSH\").replace(\"JAC\", \"JAX\")\n",
    "    \n",
    "    records = pd.concat([records, temp_records_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6c381-bf77-4713-9889-5ab38369df06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# INSERT DATA INTO DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b0ef2-86bc-43ee-a7a8-a4726bbd276d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CONNECT TO DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59b0cd8c-97d0-4c3d-a867-40f49f96f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import credentials\n",
    "dbUser = creds['mysqlSurface']['users'][1]\n",
    "dbPw = creds['mysqlSurface']['creds']['jb']\n",
    "dbHost = creds['mysqlSurface']['dbNFL']['host']\n",
    "dbName = creds['mysqlSurface']['dbNFL']['database']\n",
    "dbConnectionString = creds['pymysql'][league]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b165fb23-5ccb-431a-b60a-15b6f1453d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating lookup dictionaries that will be used across multiple database inserts\n",
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    # getting outlet db ids to convert the scraped names/ids\n",
    "    outletLookup = pd.read_sql(\"SELECT outletId, outletName  FROM outlet;\", conn)\n",
    "    outletLookup = pd.Series(outletLookup.outletId.values, index=outletLookup.outletName).to_dict()\n",
    "    \n",
    "    # getting team db ids to convert datasource names to the ids\n",
    "    teamLookup = pd.read_sql(\"SELECT teamId, name  FROM team;\", conn)\n",
    "    teamLookup = pd.Series(teamLookup.teamId.values, index=teamLookup.name).to_dict()\n",
    "    \n",
    "    # getting expert db ids to convert the scraped names/ids\n",
    "    expertLookup = pd.read_sql(\"SELECT analystId, analystName  FROM analyst;\", conn)\n",
    "    expertLookup = pd.Series(expertLookup.analystId.values, index=expertLookup.analystName).to_dict()\n",
    "\n",
    "    \n",
    "    # create name table back up\n",
    "    names = pd.read_sql(con=conn, sql=\"SELECT * FROM player\")\n",
    "    names.to_csv(\"../data/names_backup.csv\", index=False)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a38b4-ff4e-44ae-9cd1-024f2b0ce61b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LOAD RANKINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e3a3033-a7e6-4c3b-bc47-1782be9411df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:20: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_missing_players_rank = pd.concat([df_missing_players_rank,\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_load_rank = pd.concat([df_load_rank, temp])\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:20: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:20: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_13028\\4255197158.py:20: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #main df to hold all data\n",
    "    df_load_rank = pd.DataFrame(columns = [\n",
    "        'outlet', 'date', 'season', 'week', 'group', 'expert', 'rank', 'high', 'low','playerId'\n",
    "    ])\n",
    "\n",
    "    # df to hold players that are not in the database for a source yet\n",
    "    df_missing_players_rank = pd.DataFrame(columns=['date', 'outlet', 'group', 'playerId', 'sourceId', 'name'])\n",
    "\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "\n",
    "        # combining all outlets rankings to a single dataframe and converting names to the database Ids\n",
    "        directory = r'../data/ranking/weekly'\n",
    "        #looping through every rank file to aggregate into single df\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory,filename)\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(f):\n",
    "                temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
    "                                   ###################################################################\n",
    "                                   names=['outlet','date', 'season', 'week', 'group', 'expert', 'rank', \n",
    "                                          'name', 'sourceId', 'team', 'pos', 'high', 'low', 'playerId'],\n",
    "                          skiprows=1)\n",
    "                            ##########################################################################\n",
    "\n",
    "                # updtaing outlet specific playerIds to database IDs\n",
    "                if 'cbs_' in f:\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, cbsId FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup['playerId'].values, index=lookup.cbsId).to_dict()\n",
    "\n",
    "                elif 'espn_' in f:\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, espnId FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.espnId).to_dict()\n",
    "\n",
    "                elif ('fp_' in f) or ('fpEcr_' in f):\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, fpId FROM player WHERE fpId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.fpId).to_dict()\n",
    "\n",
    "                elif 'nfl_' in f:\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, nflId FROM player WHERE nflId IS NOT NULL;\", conn)\n",
    "                    lookup['nflId'] = pd.to_numeric(lookup['nflId']) \n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.nflId).to_dict()\n",
    "\n",
    "                # using the lookup to make the change from outletId to dbId\n",
    "                temp['playerId'] = temp['sourceId'].map(lookup)\n",
    "\n",
    "                ####################################\n",
    "                # creating a df to hold that date for players who are not in the player table for the source\n",
    "                if temp[pd.isnull(temp['playerId'])].shape[0] > 0:\n",
    "                    df_missing_players_rank = pd.concat([df_missing_players_rank, \n",
    "                                                    temp.loc[pd.isnull(temp['playerId']), ['date', 'outlet', 'group', 'playerId', 'sourceId', 'name']]])\n",
    "                ####################################\n",
    "\n",
    "                # updating outlet name to db outlet id \n",
    "                temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
    "\n",
    "                #updating expert name to db expert id\n",
    "                temp['expert'] = temp['expert'].replace(expertLookup)\n",
    "\n",
    "                temp = temp[['outlet', 'date', 'season', 'week', 'group', 'expert', 'rank', 'high', 'low', 'playerId']]\n",
    "                # adding outlet dataframe to the upload dataframe\n",
    "                df_load_rank = pd.concat([df_load_rank, temp])\n",
    "\n",
    "                \n",
    "    df_load_rank = df_load_rank.replace(np.nan, None)\n",
    "    # removing unranked players and rankings that have been loaded already\n",
    "    df_load_rank = df_load_rank.loc[pd.notnull(df_load_rank['rank'])]\n",
    "    df_load_rank = df_load_rank.loc[df_load_rank['date'] >= pd.to_datetime(today)]\n",
    "\n",
    "\n",
    "\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6276d5d5-7b37-4053-ae65-d787b5089f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1316, 6)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if df_missing_players_rank.shape[0] > 0:\n",
    "    df_missing_players_rank.to_csv('../data/missingPlayersRank.csv')\n",
    "df_missing_players_rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67996567-0b47-435a-b763-8ca9b87298d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # insert using SQLalchemy\n",
    "        data = df_load_rank.replace(np.nan, None).values.tolist()\n",
    "        rankMeta = sal.MetaData(engine)\n",
    "        rankTable = sal.Table('ranking', rankMeta, autoload=True)\n",
    "        conn.execute(sal.insert(rankTable).values(data))\n",
    "    \n",
    "    ''' old mysql.connector connection\n",
    "        query_insert = \"\"\"\n",
    "            INSERT INTO ranking \n",
    "                (outletId, date, season, week, rankGroup, analystId, ranking, high, low, playerId)\n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "        cursor.executemany(query_insert, df_load_rank.values.tolist())\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        '''\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2db20d99-d669-4970-8938-5de28fdbab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UPDATE player SET cbsId = %s,cbsName = %s WHERE playerId = %s'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 'update'\n",
    "source = 'cbs'\n",
    "\n",
    "db_action = {\n",
    "                'update':{\n",
    "                        \"espn\": \"\"\"UPDATE player SET espnId = %s,espnName = %s WHERE playerId = %s\"\"\",\n",
    "                         \"cbs\": \"\"\"UPDATE player SET cbsId = %s,cbsName = %s WHERE playerId = %s\"\"\",\n",
    "                         \"fp\": \"\"\"UPDATE player SET fpId = %s,fpName = %s WHERE playerId = %s\"\"\",\n",
    "                         \"nfl\": \"\"\"UPDATE player SET nflId = %s,nflName = %s WHERE playerId = %s\"\"\"\n",
    "                },\n",
    "             'add':{\n",
    "                         \"espn\": \"\"\"INSERT INTO player (posId,teamId,espnId,espnName,name) VALUES (%s,%s,%s,%s,%s)\"\"\",\n",
    "                         \"cbs\": \"\"\"INSERT INTO player (posId,teamId,cbsId,cbsName,name) VALUES (%s,%s,%s,%s,%s)\"\"\",\n",
    "                         \"fp\": \"\"\"INSERT INTO player (posId,teamId,fpId,fpName,name) VALUES (%s,%s,%s,%s,%s)\"\"\",\n",
    "                         \"nfl\": \"\"\"INSERT INTO player (posId,teamId,nflId,nflName,name) VALUES (%s,%s,%s,%s,%s)\"\"\"\n",
    "             },\n",
    "            'delete':{\n",
    "                \n",
    "            }\n",
    "}\n",
    "\n",
    "db_action[action][source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dae5f5fb-41bb-49e0-bdfd-54fe22e694f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "## THIS CAN BE USED TO UPDATE THE DB FOR MISSING PLAYERS\n",
    "#####\n",
    "df_missing = pd.read_excel(r'../data/adds.xlsx', sheet_name=action)\n",
    "\n",
    "try:\n",
    "    #engine = create_engine(dbConnectionString)\n",
    "    #conn = engine.connect()\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.executemany(db_action[action][source], df_missing.values.tolist())\n",
    "        \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"success\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eac37e-f87b-4497-bb48-34e669eb8b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LOAD PROJECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a7c9c14-4b30-4a0b-8f63-285b28a62063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:  \n",
    "    projection_columns_dbLoad = [\"playerId\", \"date\", \"season\", \"week\", \"outlet\",'GamesPlayed',\n",
    "     'PassAttempts','PassCompletions','PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    "     'InterceptionsThrown', 'PasserRating','RushingAttempts','RushingYards', 'AverageYardsPerRush', 'RushingTouchdowns',\n",
    "     'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception','ReceivingTouchdowns',\n",
    "     'FumblesLost','FieldGoalsMade','FieldGoalAttempts','LongestFieldGoal','FieldGoals119Yards','FieldGoals119YardAttempts',\n",
    "     'FieldGoals2029Yards','FieldGoals2029YardAttempts','FieldGoals3039Yards','FieldGoals3039YardAttempts',\n",
    "     'FieldGoals4049Yards','FieldGoals4049YardAttempts','FieldGoals50Yards','FieldGoals50YardsAttempts',\n",
    "     'ExtraPointsMade','ExtraPointsAttempted','Interceptions','Safeties','Sacks','Tackles','DefensiveFumblesRecovered',\n",
    "     'ForcedFumbles','DefensiveTouchdowns', 'ReturnTouchdowns','PointsAllowed','PointsAllowedPerGame','NetPassingYardsAllowed',\n",
    "     'RushingYardsAllowed','TotalYardsAllowed', 'YardsAgainstPerGame', 'twoPt', 'FantasyPoints','FantasyPointsPerGame']\n",
    "\n",
    "    #main df to hold all data\n",
    "    df_load_proj = pd.DataFrame(columns = projection_columns_dbLoad)\n",
    "    \n",
    "    # df to hold players that are not in the database for a source yet\n",
    "    df_missing_players_proj = pd.DataFrame(columns=['date', 'outlet', 'playerId', 'sourceId', 'name'])\n",
    "\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "    \n",
    "        directory = r'../data/projection/weekly'\n",
    "        #looping through every ADP file to aggregate into single df\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory,filename)\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(f):\n",
    "                temp = pd.read_csv(f, parse_dates=['date'], \n",
    "                                   names=['outlet', 'date', 'season', 'week', 'sourceId', 'name', 'shortName', 'pos', 'team', 'GamesPlayed', \n",
    "                                          'PassAttempts', 'PassCompletions', 'PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    "                                          'InterceptionsThrown', 'PasserRating', 'RushingAttempts', 'RushingYards', 'AverageYardsPerRush', \n",
    "                                          'RushingTouchdowns', 'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception', \n",
    "                                          'ReceivingTouchdowns', 'FumblesLost', 'FieldGoalsMade', 'FieldGoalAttempts', 'LongestFieldGoal', \n",
    "                                          'FieldGoals119Yards', 'FieldGoals119YardAttempts', 'FieldGoals2029Yards', 'FieldGoals2029YardAttempts', \n",
    "                                          'FieldGoals3039Yards', 'FieldGoals3039YardAttempts', 'FieldGoals4049Yards', 'FieldGoals4049YardAttempts', \n",
    "                                          'FieldGoals50Yards', 'FieldGoals50YardsAttempts', 'ExtraPointsMade', 'ExtraPointsAttempted', 'Interceptions', \n",
    "                                          'Safeties', 'Sacks', 'Tackles', 'DefensiveFumblesRecovered', 'ForcedFumbles', 'DefensiveTouchdowns', \n",
    "                                          'ReturnTouchdowns', 'PointsAllowed', 'PointsAllowedPerGame', 'NetPassingYardsAllowed', \n",
    "                                          'RushingYardsAllowed', 'TotalYardsAllowed', 'YardsAgainstPerGame', 'twoPt', 'FantasyPoints', \n",
    "                                          'FantasyPointsPerGame'],\n",
    "                          skiprows=1)\n",
    "\n",
    "                # creating dicts to convert outlet name/id to db id\n",
    "                if 'cbs' in f:\n",
    "\n",
    "                    # converting full team name to db table 'TEAM'.name\n",
    "                    lookup = pd.read_sql(\"SELECT cbsId, name FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.cbsId.values, index=lookup.name).to_dict()\n",
    "\n",
    "                    # updating the cbs source data full team name to the abbreviated db name\n",
    "                    temp['team'] = temp['team'].replace(\"JAC\", \"JAX\").replace(\"WAS\",\"WSH\")\n",
    "                    temp.loc[temp['sourceId'].isna(), 'sourceId'] = temp.loc[temp['sourceId'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                    # cbs source data does not have playerId for the defenses\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, cbsId FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup['playerId'].values, index=lookup.cbsId).to_dict()\n",
    "\n",
    "                elif 'espn' in f:\n",
    "                    # espn source data does not have playerId for the defenses\n",
    "                     # converting full team name to db table 'TEAM'.name\n",
    "                    lookup = pd.read_sql(\"SELECT espnId, name FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.espnId.values, index=lookup.name).to_dict()\n",
    "\n",
    "                    # updating the nfl source data full team name to the abbreviated db name\n",
    "                    temp.loc[temp['sourceId'].isna(), 'sourceId'] = temp.loc[temp['sourceId'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                    lookup = pd.read_sql(\"SELECT espnId, playerId FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.espnId).to_dict()\n",
    "\n",
    "\n",
    "                elif 'nfl' in f:\n",
    "                    # converting full team name to db table 'TEAM'.name\n",
    "                    lookup = pd.read_sql(\"SELECT nflName, name FROM team WHERE nflName IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.name.values, index=lookup.nflName).to_dict()\n",
    "\n",
    "                    # updating the nfl source data full team name to the abbreviated db name\n",
    "                    temp.loc[temp['name'].isna(), 'team'] = temp.loc[temp['name'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, nflId FROM player WHERE nflId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.nflId).to_dict()\n",
    "\n",
    "\n",
    "                # using the lookup to make the change from outletId to dbId\n",
    "                temp['playerId'] = temp['sourceId'].map(lookup)\n",
    "\n",
    "                # creating a df to hold that date for players who are not in the player table for the source\n",
    "                if temp[pd.isnull(temp['playerId'])].shape[0] > 0:\n",
    "                    df_missing_players_proj = pd.concat([df_missing_players_proj, \n",
    "                                                    temp.loc[pd.isnull(temp['playerId']), ['date', 'outlet', 'playerId', 'sourceId', 'name']]])\n",
    "\n",
    "                # updating outlet name to db outlet id \n",
    "                temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
    "\n",
    "\n",
    "                temp = temp[projection_columns_dbLoad].replace(\"-\", None).replace(\"—\", None)\n",
    "\n",
    "                df_load_proj = pd.concat([df_load_proj, temp])\n",
    "\n",
    "    df_load_proj = df_load_proj.replace(np.nan, None)\n",
    "\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88a02685-4bed-4735-adee-6e73d42d9e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if df_missing_players_proj.shape[0] > 0:\n",
    "    df_missing_players_proj.to_csv('../data/missingPlayersProj.csv')\n",
    "df_missing_players_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f971363-a467-4f1d-9658-5de69df08321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # insert using SQLalchemy\n",
    "        data = df_load_proj.replace(np.nan, None).values.tolist()\n",
    "        projMeta = sal.MetaData(engine)\n",
    "        projTable = sal.Table('projection', projMeta, autoload=True)\n",
    "        conn.execute(sal.insert(projTable).values(data))\n",
    "\n",
    "        '''  old connection using mysql.connector\n",
    "        query_insert = \"\"\"\n",
    "            INSERT INTO projection \n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "                 %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "                 %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(query_insert, df_load_proj.values.tolist())\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        '''\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8dab3-149d-4676-9d5c-1cdefaef2472",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### INSERT BETTINGPROS LINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f83fee50-de9e-4f53-922d-7de921eb0bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    #cursor = conn.cursor()\n",
    "    with engine.connect() as conn:\n",
    "    \n",
    "        lookup = pd.read_sql(\"SELECT bpName, teamId FROM team;\", conn)\n",
    "        lookup = pd.Series(lookup.teamId.values, index=lookup.bpName).to_dict()\n",
    "            \n",
    "        df_lines['awayTeam'] = df_lines['awayTeam'].map(lookup)\n",
    "        df_lines['homeTeam'] = df_lines['homeTeam'].map(lookup)\n",
    "\n",
    "        # insert using SQLalchemy\n",
    "        data = df_lines.replace(np.nan, None).values.tolist()\n",
    "        bettingMeta = sal.MetaData(engine)\n",
    "        bettingTable = sal.Table('betting', bettingMeta, autoload=True)\n",
    "        conn.execute(sal.insert(bettingTable).values(data))\n",
    "        \n",
    "        #### use with the connection from mysql.connector\n",
    "        '''        \n",
    "        query_insert = \"\"\"\n",
    "            INSERT INTO betting \n",
    "                (date, season, week, overUnder, overUnderCost, awayTeamId,\n",
    "               awaySpread, awayCost, awayMoneyline, homeTeamId, homeSpread,\n",
    "               homeCost, homeMoneyLine)\n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(query_insert, df_lines.replace(np.nan, None).values.tolist())\n",
    "        conn.commit()\n",
    "        '''\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623151c-589b-43ff-939b-e8f14816dfe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### WEEKLY TEAM RECORDS AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05a0f645-8411-4c98-a0c8-d488124d32a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #conn = mysql.connector.connect(user=dbUser, password=dbPw,host=dbHost,database=dbName)\n",
    "    #cursor = conn.cursor()\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "    \n",
    "        games['homeTeam'] = games['homeTeam'].map(teamLookup)\n",
    "        games['awayTeam'] = games['awayTeam'].map(teamLookup)\n",
    "        games['winningTeam'] = games['winningTeam'].map(teamLookup)\n",
    "        records['team'] = records['team'].map(teamLookup)\n",
    "\n",
    "        # insert using SQLalchemy\n",
    "        data = games.replace(np.nan, None).values.tolist()\n",
    "        gamesMeta = sal.MetaData(engine)\n",
    "        gamesTable = sal.Table('weeklyScore', gamesMeta, autoload=True)\n",
    "        conn.execute(sal.insert(gamesTable).values(data))\n",
    "\n",
    "        # insert using SQLalchemy\n",
    "        data = records.replace(np.nan, None).values.tolist()\n",
    "        recordsMeta = sal.MetaData(engine)\n",
    "        recordsTable = sal.Table('weeklyRecord', recordsMeta, autoload=True)\n",
    "        conn.execute(sal.insert(recordsTable).values(data))\n",
    "\n",
    "    '''    THIS WAS USED with the mysql.connector connection\n",
    "        query_insert1 = \"\"\"\n",
    "            INSERT INTO weeklyRecord\n",
    "                (season, week, teamId, wins, losses, ties)\n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        query_insert2 = \"\"\"\n",
    "            INSERT INTO weeklyScore \n",
    "                (season,week,homeTeamId,homeQ1Pts,homeQ2Pts,homeQ3Pts,homeQ4Pts,\n",
    "                homeTotalPts,awayTeamId,awayQ1Pts,awayQ2Pts,awayQ3Pts,awayQ4Pts,\n",
    "                awayTotalPts,homeWinner,awayWinner,tie,winningTeamId,gameId)\n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s,%s, %s, %s, %s, %s, %s,\n",
    "                %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.executemany(query_insert1, records.replace(np.nan, None).values.tolist())\n",
    "        cursor.executemany(query_insert2, games.replace(np.nan, None).values.tolist())\n",
    "        conn.commit()    \n",
    "        conn.close()\n",
    "        '''\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc336b9-3cd8-4240-9eac-e413de791397",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LOADING OLDER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a8481-da78-4a68-ab33-276ccc1db411",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### INSERTING PLAYER AND TEAM NAMES/ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "fdcf7673-109e-47ec-a11e-b88a73e1d5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query_insert = \"\"\"\n",
    "        INSERT INTO team \n",
    "            (name, espnId, espnName, nflId, nflName, fpId, fpName, cbsId, cbsName, bpId, bpName)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.executemany(query_insert, teams.values.tolist())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6f7a1b9e-49df-439f-be3b-175b91cf8bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mflId</th>\n",
       "      <th>sportradarId</th>\n",
       "      <th>gsisId</th>\n",
       "      <th>pffId</th>\n",
       "      <th>sleeperId</th>\n",
       "      <th>yahooId</th>\n",
       "      <th>fleaflickerId</th>\n",
       "      <th>rotowireId</th>\n",
       "      <th>rotoworldId</th>\n",
       "      <th>ktcId</th>\n",
       "      <th>pfrId</th>\n",
       "      <th>cfbrefId</th>\n",
       "      <th>statsId</th>\n",
       "      <th>statsglobalId</th>\n",
       "      <th>fantasydataId</th>\n",
       "      <th>swishId</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15833</td>\n",
       "      <td>9c32f8cb-807a-4864-bc0b-b846d279606d</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8290</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15926</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>33959</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1164968</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mflId                          sportradarId gsisId pffId  sleeperId  \\\n",
       "0  15833  9c32f8cb-807a-4864-bc0b-b846d279606d   None  None       8290   \n",
       "\n",
       "  yahooId fleaflickerId  rotowireId rotoworldId ktcId pfrId cfbrefId  statsId  \\\n",
       "0    None          None       15926        None  None  None     None    33959   \n",
       "\n",
       "  statsglobalId fantasydataId  swishId    id  \n",
       "0          None          None  1164968  1520  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#players = pd.read_excel(\"Data/names_backup.csv\", sheet_name='names').replace(np.nan, None)#, index_col='playerId').replace(np.nan, None)\n",
    "#updates, adds, delete, Sheet2\n",
    "players = pd.read_excel(\"Data/adds.xlsx\", sheet_name='Sheet1')\n",
    "players = players.replace(np.nan, None).replace(0,None)\n",
    "players.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b91ff1c3-ba55-4e5d-90dd-e244834f57e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query_insert = \"\"\"\n",
    "        INSERT INTO player \n",
    "            (posId, teamId, name, espnId, espnName, bpId, bpName, nflId, nflName, fpId, fpName, cbsId, cbsName)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "#    query_insert = \"\"\"\n",
    "#        INSERT INTO player \n",
    "#            (name, posId, teamId, mflId, sportradarId, fpId,\n",
    "#            gsisId, pffId, sleeperId, nflId, espnId, \n",
    "#            yahooId,fleaflickerId, cbsId, rotowireId, rotoworldId, ktcId, \n",
    "#            pfrId, cfbrefId, statsId, statsglobalId, fantasydataId, swishId)\n",
    "#        VALUES \n",
    "#            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "#            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "#    \"\"\"\n",
    "    \n",
    "    query_update = \"\"\"\n",
    "        REPLACE INTO player \n",
    "            (playerId, posId, teamId, name, espnId, espnName, bpId, bpName, nflId, nflName, fpId, fpName, cbsId, cbsName)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    query_update1 = \"\"\"\n",
    "        UPDATE player \n",
    "        SET \n",
    "            mflId = %s,\n",
    "            sportradarId = %s, \n",
    "            gsisId = %s, \n",
    "            pffId = %s, \n",
    "            sleeperId = %s, \n",
    "            yahooId = %s, \n",
    "            fleaflickerId = %s, \n",
    "            rotowireId = %s, \n",
    "            rotoworldId = %s, \n",
    "            ktcId = %s, \n",
    "            pfrId = %s, \n",
    "            cfbrefId = %s, \n",
    "            statsId = %s, \n",
    "            statsglobalId = %s, \n",
    "            fantasydataId = %s, \n",
    "            swishId = %s\n",
    "        WHERE playerId = %s\n",
    "    \"\"\"\n",
    "    \n",
    "    query_insert1 = \"\"\"\n",
    "        INSERT INTO player \n",
    "            (posId, teamId, cbsId, cbsName, name)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    query_delete = '''\n",
    "        DELETE FROM player\n",
    "        WHERE playerId = %s\n",
    "    '''\n",
    "    \n",
    "    query_update = '''\n",
    "        UPDATE projection SET playerID = %s WHERE playerId = %s\n",
    "    '''\n",
    "    \n",
    "    cursor.executemany(query_update1, players.values.tolist())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b9167-2d11-4bd9-8b03-f4040a55ca83",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LOAD ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "29feb9b2-4a32-4e18-96ec-b23ba952b21e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "c:\\users\\jrbrz\\.virtualenvs\\data-adbtfuvq\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    # columns for db table adp\n",
    "    adp_columns = ['outlet', 'date', 'playerId', 'adp','high','low']\n",
    "\n",
    "    #main df to hold all data\n",
    "    df_load_adp = pd.DataFrame(columns = adp_columns)\n",
    "\n",
    "    directory = r\"data\\adp\\2022\"\n",
    "    #looping through every ADP file to aggregate into single df\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory,filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            temp = pd.read_excel(f, parse_dates=['date'])\n",
    "\n",
    "            # creating dicts to convert outlet name/id to db id\n",
    "            if 'cbs' in f:\n",
    "\n",
    "                # creating cbs ID to db ID map\n",
    "                lookup = pd.read_sql(\"SELECT playerId, cbsId FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup['playerId'].values, index=lookup.cbsId).to_dict()\n",
    "\n",
    "            elif 'espn' in f:\n",
    "                # espn source data does not have playerId for the defenses\n",
    "                # converting full team name to db table 'TEAM'.name\n",
    "                lookup = pd.read_sql(\"SELECT espnId, name FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup.espnId.values, index=lookup.name).to_dict()\n",
    "\n",
    "                # updating the player table team name to player table Id\n",
    "                temp['team'] = temp['team'].str.upper()\n",
    "                temp.loc[temp['playerId'].isna(), 'playerId'] = temp.loc[temp['playerId'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                lookup = pd.read_sql(\"SELECT espnId, playerId FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup.playerId.values, index=lookup.espnId).to_dict()\n",
    "\n",
    "\n",
    "            elif 'fp' in f:\n",
    "\n",
    "                # updating the fantasy pro source data id to db id\n",
    "                temp = temp.loc[pd.notnull(temp['adp'])]\n",
    "\n",
    "                lookup = pd.read_sql(\"SELECT playerId, fpId FROM player WHERE fpId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup.playerId.values, index=lookup.fpId).to_dict()\n",
    "\n",
    "\n",
    "\n",
    "            # using the lookup to make the change from outletId to dbId\n",
    "            temp['playerId'] = temp['playerId'].map(lookup)\n",
    "\n",
    "            # updating outlet name to db outlet id \n",
    "            temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
    "\n",
    "\n",
    "            temp = temp[adp_columns]\n",
    "            # adding outlet dataframe to the upload dataframe\n",
    "            df_load_adp = pd.concat([df_load_adp, temp])\n",
    "\n",
    "    df_load_adp = df_load_adp.replace(np.nan, None)\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "0b79c8aa-f07b-49a7-834b-985968866e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #engine = create_engine(dbConnectionString)\n",
    "    #conn = engine.connect()\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query_insert = \"\"\"\n",
    "        INSERT INTO adp \n",
    "\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.executemany(query_insert, df_load_adp.values.tolist())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e126a9c-f967-482c-8a69-a884ab6abd79",
   "metadata": {},
   "source": [
    "# <<< SCRATCH >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58451717-1406-4e74-a167-d8d173dbb68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "engine = create_engine(dbConnectionString)\n",
    "with engine.connect() as conn:\n",
    "    lookup = pd.read_sql(\"SELECT playerId, nflId FROM player WHERE name = 'Travis Kelce';\", conn)\n",
    "    #lookup = pd.Series(lookup.playerId.values, index=lookup.nflId).to_dict()\n",
    "nf = pd.read_csv(r\"C:\\Users\\jrbrz\\Desktop\\projects\\projects\\ffDraft\\data\\ranking\\weekly\\nfl_rank_2022-06_2022-10-13.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "4ddb4499-588f-4d93-9a32-a4f1e8138ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5510f30a-2590-41a6-bc07-8d89e1486762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_cbs_proj, df_cbs_ranking, df_cbs_adp, \n",
    "       df_fp_ranking, df_fp_adp, \n",
    "       df_espn_proj, df_espn_ranking, df_espn_adp,\n",
    "       df_nfl_proj, df_nfl_ranking,\n",
    "       df_overUnders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ea648-7781-4bbf-a907-1ea8a333c1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = Service(r\"..\\browsers\\geckodriver.exe\")\n",
    "driver = webdriver.Firefox(service=service)\n",
    "\n",
    "driver.get(\"https://www.espn.com/fantasy/football/story/_/id/33898295/fantasy-football-idp-rankings-2022-top-50-defensive-linemen-linebackers-defensive-backs\")\n",
    "time.sleep(10)\n",
    "\n",
    "# grabs the entire pages html\n",
    "html = driver.execute_script(\"return document.body.innerHTML\")\n",
    "soup = bs(html)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b2197e91-dc66-4a1b-b49c-7fa13f338933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "35ba15ae-7851-4b9d-9481-ad5eab39871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure to create json file \n",
    "c = {}\n",
    "c = json.dumps(c)\n",
    "with open('config.txt', 'w') as f:\n",
    "    f.write(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756ce1c-1f44-4181-9a79-5975c7deb7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
