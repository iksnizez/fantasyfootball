{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3ea065-fdc4-46cd-bdcd-d14d5524ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, re, os, json\n",
    "import pymysql, pyodbc\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import date\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#importing credentials\n",
    "with open('../../../Notes-General/config.txt', 'r') as f:\n",
    "    creds = f.read()\n",
    "creds = json.loads(creds)\n",
    "league = 'nfl'\n",
    "\n",
    "# import credentials\n",
    "dbUser = creds['mysqlSurface']['users'][1]\n",
    "dbPw = creds['mysqlSurface']['creds']['jb']\n",
    "dbHost = creds['mysqlSurface']['dbNFL']['host']\n",
    "dbName = creds['mysqlSurface']['dbNFL']['database']\n",
    "dbConnectionString = creds['pymysql'][league]\n",
    "\n",
    "today = date.today()\n",
    "season = 2023\n",
    "week = int(input(\"What week is it >>>>>> ? \"))\n",
    "if week < 10:\n",
    "    strWeek = \"0\" + str(week)\n",
    "else:\n",
    "    strWeek = week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc8d1a8-4e5c-45e4-bd29-0e2c78e90b03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "teammap = {'ATL': 'ATL',\n",
    " 'BUF': 'BUF',\n",
    " 'CHI': 'CHI',\n",
    " 'CIN': 'CIN',\n",
    " 'CLE': 'CLE',\n",
    " 'DAL': 'DAL',\n",
    " 'DEN': 'DEN',\n",
    " 'DET': 'DET',\n",
    " 'GB': 'GB',\n",
    " 'TEN': 'TEN',\n",
    " 'IND': 'IND',\n",
    " 'KC': 'KC',\n",
    " 'LV': 'LV',\n",
    " 'LAR': 'LA',\n",
    " 'MIA': 'MIA',\n",
    " 'MIN': 'MIN',\n",
    " 'NE': 'NE',\n",
    " 'NO': 'NO',\n",
    " 'NYG': 'NYG',\n",
    " 'NYJ': 'NYJ',\n",
    " 'PHI': 'PHI',\n",
    " 'ARI': 'ARI',\n",
    " 'PIT': 'PIT',\n",
    " 'LAC': 'LAC',\n",
    " 'SF': 'SF',\n",
    " 'SEA': 'SEA',\n",
    " 'TB': 'TB',\n",
    " 'WSH': 'WAS',\n",
    " 'CAR': 'CAR',\n",
    " 'JAX': 'JAX',\n",
    " 'BAL': 'BAL',\n",
    " 'HOU': 'HOU',\n",
    " 'FA': 'FA',\n",
    " 'STL': 'SL',\n",
    " 'SD': 'SD',\n",
    " 'OAK': 'OAK'}\n",
    "teammapid = {'ATL': '3800',\n",
    " 'BUF': '0610',\n",
    " 'CHI': '0810',\n",
    " 'CIN': '0920',\n",
    " 'CLE': '1050',\n",
    " 'DAL': '1200',\n",
    " 'DEN': '1400',\n",
    " 'DET': '1540',\n",
    " 'GB': '1800',\n",
    " 'TEN': '2100',\n",
    " 'IND': '2200',\n",
    " 'KC': '2310',\n",
    " 'LV': '2520',\n",
    " 'LAR': '2510',\n",
    " 'MIA': '2700',\n",
    " 'MIN': '3000',\n",
    " 'NE': '3200',\n",
    " 'NO': '3300',\n",
    " 'NYG': '3410',\n",
    " 'NYJ': '3430',\n",
    " 'PHI': '3700',\n",
    " 'ARI': '3800',\n",
    " 'PIT': '3900',\n",
    " 'LAC': '4400',\n",
    " 'SF': '4500',\n",
    " 'SEA': '4600',\n",
    " 'TB': '4900',\n",
    " 'WSH': '5110',\n",
    " 'CAR': '0750',\n",
    " 'JAX': '2250',\n",
    " 'BAL': '0325',\n",
    " 'HOU': '2120',\n",
    " 'FA': '0',\n",
    " 'STL': '0',\n",
    " 'SD': '0',\n",
    " 'OAK': '0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299adc92-cb96-4ea3-9a18-df83a5015328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lookup dictionaries that will be used across multiple database inserts\n",
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    # getting outlet db ids to convert the scraped names/ids\n",
    "    outletLookup = pd.read_sql(\"SELECT outletId, outletName  FROM outlet;\", conn)\n",
    "    outletLookup = pd.Series(outletLookup.outletId.values, index=outletLookup.outletName).to_dict()\n",
    "    \n",
    "    # getting team db ids to convert datasource names to the ids\n",
    "    #teamLookup = pd.read_sql(\"SELECT teamId, name  FROM team;\", conn)\n",
    "    teamLookup = pd.read_sql(\"SELECT teamId, nflfastrName  FROM team;\", conn)\n",
    "    teams = pd.read_sql(\"SELECT *  FROM team;\", conn)\n",
    "    teamLookup = pd.Series(teamLookup.teamId.values, index=teamLookup.nflfastrName).to_dict()\n",
    "    \n",
    "    \n",
    "    # getting expert db ids to convert the scraped names/ids\n",
    "    expertLookup = pd.read_sql(\"SELECT analystId, analystName  FROM analyst;\", conn)\n",
    "    expertLookup = pd.Series(expertLookup.analystId.values, index=expertLookup.analystName).to_dict()\n",
    "\n",
    "    # getting pos db ids for espn\n",
    "    posLookup = pd.read_sql(\"SELECT posId, pos  FROM pos;\", conn)\n",
    "    posLookup = pd.Series(posLookup.posId.values, index=posLookup.pos).to_dict()\n",
    "\n",
    "    # create name table back up\n",
    "    names = pd.read_sql(con=conn, sql=\"SELECT * FROM player\")\n",
    "    names.to_csv(\"../data/names_backup.csv\", index=False)\n",
    "\n",
    "    dbPlayers = pd.read_sql(\"SELECT joinName, team, position FROM players WHERE season = 2023;\", conn)\n",
    "    dbPlayers = pd.read_sql(\"SELECT * FROM players;\", conn)\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648d628-f74d-4252-be5e-2674286412fd",
   "metadata": {},
   "source": [
    "# ADDING NEW PLAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790929b4-9890-4ebc-9cfa-cf3cad716077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e10afd-9589-4697-b2d9-05c6ff94ec6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eb97ff9-518f-45f7-aaed-26babf5b96d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LOAD RANKINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee0def-7878-4876-a8d1-b4e1bc3e5e67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\4057947312.py:20: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\4057947312.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_missing_players_rank = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are trying to merge on object and int64 columns for key 'joinName'. If you wish to proceed you should use pd.concat\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #main df to hold all data\n",
    "    df_load_rank = pd.DataFrame(columns = [\n",
    "        'outlet', 'date', 'season', 'week', 'group', 'expert', 'rank', 'high', 'low','playerId'\n",
    "    ])  \n",
    "\n",
    "    # df to hold players that are not in the database for a source yet\n",
    "    df_missing_players_rank = pd.DataFrame(columns=['date', 'outlet', 'group', 'playerId', 'sourceId', 'name'])\n",
    "\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "\n",
    "        # combining all outlets rankings to a single dataframe and converting names to the database Ids\n",
    "        directory = r'../data/ranking/weekly'\n",
    "        #looping through every rank file to aggregate into single df\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory,filename)\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(f):\n",
    "                temp = pd.read_csv(f, parse_dates=['date'], infer_datetime_format=True,\n",
    "                                   ###################################################################\n",
    "                                   names=['outlet','date', 'season', 'week', 'group', 'expert', 'rank', \n",
    "                                          'name', 'sourceId', 'team', 'pos', 'high', 'low', 'playerId'],\n",
    "                          skiprows=1)\n",
    "                            ##########################################################################\n",
    "\n",
    "                # updtaing outlet specific playerIds to database IDs\n",
    "                if 'cbs_' in f:\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, cbsId FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup['playerId'].values, index=lookup.cbsId).to_dict()\n",
    "\n",
    "                elif 'espn_' in f:\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, espnId FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.espnId).to_dict()\n",
    "\n",
    "                elif ('fp_' in f) or ('fpEcr_' in f):\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, fpId FROM player WHERE fpId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.fpId).to_dict()\n",
    "\n",
    "                elif 'nfl_' in f:\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, nflId FROM player WHERE nflId IS NOT NULL;\", conn)\n",
    "                    lookup['nflId'] = pd.to_numeric(lookup['nflId']) \n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.nflId).to_dict()\n",
    "\n",
    "                # using the lookup to make the change from outletId to dbId\n",
    "                temp['playerId'] = temp['sourceId'].map(lookup)\n",
    "\n",
    "                ####################################\n",
    "                # creating a df to hold that date for players who are not in the player table for the source\n",
    "                if temp[pd.isnull(temp['playerId'])].shape[0] > 0:\n",
    "                    df_missing_players_rank = pd.concat(\n",
    "                                                    [df_missing_players_rank, \n",
    "                                                    temp.loc[\n",
    "                                                             pd.isnull(temp['playerId']), \n",
    "                                                             ['date', 'outlet', 'group', \n",
    "                                                              'playerId', 'sourceId', 'name']\n",
    "                                                        ]\n",
    "                                                    ]\n",
    "                                            )\n",
    "                    df_missing_players_rank['joinName'] = df_missing_players_rank['name'].str.lower()\n",
    "                    df_missing_players_rank['joinName'] = df_missing_players_rank['joinName'].str.replace(\".\", \"\")\n",
    "                    df_missing_players_rank['joinName'] = df_missing_players_rank['joinName'].str.replace(\"''\", \"\")\n",
    "                    df_missing_players_rank['joinName'] = df_missing_players_rank['joinName'].str.replace(\"`\", \"\")\n",
    "                    df_missing_players_rank['joinName'] = df_missing_players_rank['joinName'].str.replace(\"\", \"\")\n",
    "                    \n",
    "                    # grab the player data from nflfastR\n",
    "                    dbPlayers = pd.read_sql(\"SELECT joinName, team, position FROM players ;\", conn)\n",
    "                    \n",
    "                    # processing data\n",
    "                    dbPlayers['position'] = dbPlayers['position'].replace(posLookup)\n",
    "                    dbPlayers['team'] = dbPlayers['team'].replace(teamLookup)\n",
    "\n",
    "                    \n",
    "                    # adding position and team id to missing data\n",
    "                    df_missing_players_rank = df_missing_players_rank.join(dbPlayers, on='joinName', how='left')\n",
    "                    \n",
    "                \n",
    "                ####################################\n",
    "\n",
    "                # updating outlet name to db outlet id \n",
    "                temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
    "\n",
    "                #updating expert name to db expert id\n",
    "                temp['expert'] = temp['expert'].replace(expertLookup)\n",
    "\n",
    "                temp = temp[['outlet', 'date', 'season', 'week', 'group', 'expert', 'rank', 'high', 'low', 'playerId']]\n",
    "                # adding outlet dataframe to the upload dataframe\n",
    "                df_load_rank = pd.concat([df_load_rank, temp])\n",
    "\n",
    "                \n",
    "    df_load_rank = df_load_rank.replace(np.nan, None)\n",
    "    # removing unranked players and rankings that have been loaded already\n",
    "    df_load_rank = df_load_rank.loc[pd.notnull(df_load_rank['rank'])]\n",
    "    df_load_rank = df_load_rank.loc[df_load_rank['date'] >= pd.to_datetime(today)]\n",
    "\n",
    "\n",
    "\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ade86b-a414-4ce4-8386-c95a5e73b418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if df_missing_players_rank.shape[0] > 0:\n",
    "    df_missing_players_rank.to_csv('../data/missingPlayersRank.csv')\n",
    "df_missing_players_rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df3140-9e9b-4ace-90a5-0050cd790838",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # insert using SQLalchemy\n",
    "        data = df_load_rank.replace(np.nan, None).values.tolist()\n",
    "        rankMeta = sal.MetaData(engine)\n",
    "        rankTable = sal.Table('ranking', rankMeta, autoload=True)\n",
    "        conn.execute(sal.insert(rankTable).values(data))\n",
    "    \n",
    "    ''' old mysql.connector connection\n",
    "        query_insert = \"\"\"\n",
    "            INSERT INTO ranking \n",
    "                (outletId, date, season, week, rankGroup, analystId, ranking, high, low, playerId)\n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "        cursor.executemany(query_insert, df_load_rank.values.tolist())\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        '''\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9156230-f6de-4f68-8099-016e8761a67a",
   "metadata": {},
   "source": [
    "#### INPUT TO CONTROL DB ACTIONS - USED TO MANUALLY ADD PLAYERS MISSING FROM DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a248c0c-7f09-4e08-9999-a8fee0e0d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'update'\n",
    "source = 'cbs'\n",
    "\n",
    "db_action = {\n",
    "            'update':{\n",
    "                        \"espn\": \"\"\"UPDATE player SET espnId = %s,espnName = %s WHERE playerId = %s\"\"\",\n",
    "                         \"cbs\": \"\"\"UPDATE player SET cbsId = %s,cbsName = %s WHERE playerId = %s\"\"\",\n",
    "                         \"fp\": \"\"\"UPDATE player SET fpId = %s,fpName = %s WHERE playerId = %s\"\"\",\n",
    "                         \"nfl\": \"\"\"UPDATE player SET nflId = %s,nflName = %s WHERE playerId = %s\"\"\"\n",
    "                },\n",
    "             'add':{\n",
    "                         \"espn\": \"\"\"INSERT INTO player (posId,teamId,espnId,espnName,name) VALUES (%s,%s,%s,%s,%s)\"\"\",\n",
    "                         \"cbs\": \"\"\"INSERT INTO player (posId,teamId,cbsId,cbsName,name) VALUES (%s,%s,%s,%s,%s)\"\"\",\n",
    "                         \"fp\": \"\"\"INSERT INTO player (posId,teamId,fpId,fpName,name) VALUES (%s,%s,%s,%s,%s)\"\"\",\n",
    "                         \"nfl\": \"\"\"INSERT INTO player (posId,teamId,nflId,nflName,name) VALUES (%s,%s,%s,%s,%s)\"\"\"\n",
    "             },\n",
    "            'delete':{\n",
    "                \n",
    "            }\n",
    "}\n",
    "\n",
    "db_action[action][source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e65d1b-ca97-4e35-b646-7b47b9cf321e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "## THIS CAN BE USED TO UPDATE THE DB FOR MISSING PLAYERS\n",
    "#####\n",
    "df_missing = pd.read_excel(r'../data/adds.xlsx', sheet_name=action)\n",
    "\n",
    "try:\n",
    "    #engine = create_engine(dbConnectionString)\n",
    "    #conn = engine.connect()\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.executemany(db_action[action][source], df_missing.values.tolist())\n",
    "        \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"success\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca540040-7041-4779-b718-a5bee7cba22e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LOAD PROJECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8c991-5479-4db6-bd94-3d3200706710",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:  \n",
    "    projection_columns_dbLoad = [\"playerId\", \"date\", \"season\", \"week\", \"outlet\",'GamesPlayed',\n",
    "     'PassAttempts','PassCompletions','PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    "     'InterceptionsThrown', 'PasserRating','RushingAttempts','RushingYards', 'AverageYardsPerRush', 'RushingTouchdowns',\n",
    "     'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception','ReceivingTouchdowns',\n",
    "     'FumblesLost','FieldGoalsMade','FieldGoalAttempts','LongestFieldGoal','FieldGoals119Yards','FieldGoals119YardAttempts',\n",
    "     'FieldGoals2029Yards','FieldGoals2029YardAttempts','FieldGoals3039Yards','FieldGoals3039YardAttempts',\n",
    "     'FieldGoals4049Yards','FieldGoals4049YardAttempts','FieldGoals50Yards','FieldGoals50YardsAttempts',\n",
    "     'ExtraPointsMade','ExtraPointsAttempted','Interceptions','Safeties','Sacks','Tackles','DefensiveFumblesRecovered',\n",
    "     'ForcedFumbles','DefensiveTouchdowns', 'ReturnTouchdowns','PointsAllowed','PointsAllowedPerGame','NetPassingYardsAllowed',\n",
    "     'RushingYardsAllowed','TotalYardsAllowed', 'YardsAgainstPerGame', 'twoPt', 'FantasyPoints','FantasyPointsPerGame']\n",
    "\n",
    "    projcols = ['outlet', 'date', 'season', 'week', 'sourceId', 'name', 'shortName', 'pos', 'team', 'GamesPlayed', \n",
    "                'PassAttempts', 'PassCompletions', 'PassingYards', 'PassingYardsPerGame', 'TouchdownsPasses', \n",
    "                'InterceptionsThrown', 'PasserRating', 'RushingAttempts', 'RushingYards', 'AverageYardsPerRush', \n",
    "                'RushingTouchdowns', 'Targets', 'Receptions', 'ReceivingYards', 'YardsPerGame', 'AverageYardsPerReception', \n",
    "                'ReceivingTouchdowns', 'FumblesLost', 'FieldGoalsMade', 'FieldGoalAttempts', 'LongestFieldGoal', \n",
    "                'FieldGoals119Yards', 'FieldGoals119YardAttempts', 'FieldGoals2029Yards', 'FieldGoals2029YardAttempts', \n",
    "                'FieldGoals3039Yards', 'FieldGoals3039YardAttempts', 'FieldGoals4049Yards', 'FieldGoals4049YardAttempts', \n",
    "                'FieldGoals50Yards', 'FieldGoals50YardsAttempts', 'ExtraPointsMade', 'ExtraPointsAttempted', 'Interceptions', \n",
    "                'Safeties', 'Sacks', 'Tackles', 'DefensiveFumblesRecovered', 'ForcedFumbles', 'DefensiveTouchdowns', \n",
    "                'ReturnTouchdowns', 'PointsAllowed', 'PointsAllowedPerGame', 'NetPassingYardsAllowed',  'RushingYardsAllowed',\n",
    "                'TotalYardsAllowed', 'YardsAgainstPerGame', 'twoPt', 'FantasyPoints', 'FantasyPointsPerGame']\n",
    "\n",
    "    #main df to hold all data\n",
    "    df_load_proj = pd.DataFrame(columns = projection_columns_dbLoad)\n",
    "    \n",
    "    # df to hold players that are not in the database for a source yet\n",
    "    df_missing_players_proj = pd.DataFrame(columns=['date', 'outlet', 'playerId', 'sourceId', 'name'])\n",
    "\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "    \n",
    "        directory = r'../data/projection/weekly'\n",
    "        #looping through every ADP file to aggregate into single df\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory,filename)\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(f):\n",
    "                temp = pd.read_csv(f, parse_dates=['date'], \n",
    "                                   names=projcols,\n",
    "                          skiprows=1)\n",
    "\n",
    "                # creating dicts to convert outlet name/id to db id\n",
    "                if 'cbs' in f:\n",
    "\n",
    "                    # converting full team name to db table 'TEAM'.name\n",
    "                    lookup = pd.read_sql(\"SELECT cbsId, name FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.cbsId.values, index=lookup.name).to_dict()\n",
    "\n",
    "                    # updating the cbs source data full team name to the abbreviated db name\n",
    "                    temp['team'] = temp['team'].replace(\"JAC\", \"JAX\").replace(\"WAS\",\"WSH\")\n",
    "                    temp.loc[temp['sourceId'].isna(), 'sourceId'] = temp.loc[temp['sourceId'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                    # cbs source data does not have playerId for the defenses\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, cbsId FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup['playerId'].values, index=lookup.cbsId).to_dict()\n",
    "\n",
    "                elif 'espn' in f:\n",
    "                    # espn source data does not have playerId for the defenses\n",
    "                     # converting full team name to db table 'TEAM'.name\n",
    "                    lookup = pd.read_sql(\"SELECT espnId, name FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.espnId.values, index=lookup.name).to_dict()\n",
    "\n",
    "                    # updating the nfl source data full team name to the abbreviated db name\n",
    "                    temp.loc[temp['sourceId'].isna(), 'sourceId'] = temp.loc[temp['sourceId'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                    lookup = pd.read_sql(\"SELECT espnId, playerId FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.espnId).to_dict()\n",
    "\n",
    "\n",
    "                elif 'nfl' in f:\n",
    "                    # converting full team name to db table 'TEAM'.name\n",
    "                    lookup = pd.read_sql(\"SELECT nflName, name FROM team WHERE nflName IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.name.values, index=lookup.nflName).to_dict()\n",
    "\n",
    "                    # updating the nfl source data full team name to the abbreviated db name\n",
    "                    temp.loc[temp['name'].isna(), 'team'] = temp.loc[temp['name'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                    lookup = pd.read_sql(\"SELECT playerId, nflId FROM player WHERE nflId IS NOT NULL;\", conn)\n",
    "                    lookup = pd.Series(lookup.playerId.values, index=lookup.nflId).to_dict()\n",
    "\n",
    "\n",
    "                # using the lookup to make the change from outletId to dbId\n",
    "                temp['playerId'] = temp['sourceId'].map(lookup)\n",
    "\n",
    "                # creating a df to hold that date for players who are not in the player table for the source\n",
    "                if temp[pd.isnull(temp['playerId'])].shape[0] > 0:\n",
    "                    df_missing_players_proj = pd.concat([df_missing_players_proj, \n",
    "                                                    temp.loc[pd.isnull(temp['playerId']), ['date', 'outlet', 'playerId', 'sourceId', 'name']]])\n",
    "\n",
    "                # updating outlet name to db outlet id \n",
    "                temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
    "\n",
    "\n",
    "                temp = temp[projection_columns_dbLoad].replace(\"-\", None).replace(\"—\", None)\n",
    "\n",
    "                df_load_proj = pd.concat([df_load_proj, temp])\n",
    "\n",
    "    df_load_proj = df_load_proj.replace(np.nan, None)\n",
    "\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe57d1-c326-4dc0-8757-ff0b61dc1d23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if df_missing_players_proj.shape[0] > 0:\n",
    "    df_missing_players_proj.to_csv('../data/missingPlayersProj.csv')\n",
    "df_missing_players_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9b68c-45e8-4603-9c3f-dae0e5e35254",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # insert using SQLalchemy\n",
    "        data = df_load_proj.replace(np.nan, None).values.tolist()\n",
    "        projMeta = sal.MetaData(engine)\n",
    "        projTable = sal.Table('projection', projMeta, autoload=True)\n",
    "        conn.execute(sal.insert(projTable).values(data))\n",
    "\n",
    "        '''  old connection using mysql.connector\n",
    "        query_insert = \"\"\"\n",
    "            INSERT INTO projection \n",
    "            VALUES \n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "                 %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "                 %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.executemany(query_insert, df_load_proj.values.tolist())\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        '''\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86b65c-d91a-4b1c-b695-79d9faa98e8f",
   "metadata": {},
   "source": [
    "# INSERT BETTING LINES - updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd3b4b2-6025-4597-875c-f733a198fdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\3401799653.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_lines = pd.concat([df_lines, temp], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "directory = r'../data/betting'\n",
    "bettingTableName = 'betting'\n",
    "\n",
    "bettingCols = ['date', 'season', 'week', 'overUnder', 'overUnderCost', 'awayTeamId',\n",
    "               'awaySpread', 'awayCost', 'awayMoneyline', 'homeTeamId', 'homeSpread',\n",
    "               'homeCost', 'homeMoneyLine']\n",
    "\n",
    "df_lines = pd.DataFrame(columns=bettingCols)\n",
    "\n",
    "#looping through every betting line file to aggregate into single df\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        temp = pd.read_csv(f, parse_dates=['date'],  names=bettingCols, skiprows=1)\n",
    "        df_lines = pd.concat([df_lines, temp], ignore_index=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        tran = conn.begin()\n",
    "\n",
    "        lookup = pd.read_sql(\"SELECT bpName, teamId FROM team;\", conn)\n",
    "        lookup = pd.Series(lookup.teamId.values, index=lookup.bpName).to_dict()\n",
    "            \n",
    "        df_lines['awayTeamId'] = df_lines['awayTeamId'].map(lookup)\n",
    "        df_lines['homeTeamId'] = df_lines['homeTeamId'].map(lookup)\n",
    "\n",
    "        df_lines.to_sql(bettingTableName, conn, if_exists='append', index=False)\n",
    "\n",
    "        tran.commit()\n",
    "        conn.close()\n",
    "\n",
    "    print(\"success\")\n",
    "except Exception as ex:\n",
    "    tran.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7fe60-1d16-4a30-b3a8-848bcf7b9f13",
   "metadata": {},
   "source": [
    "# OLDER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7a708-ff85-4971-aa4f-7742a56d0003",
   "metadata": {},
   "source": [
    "## INSERTING PLAYER NAMES/ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d84565-ae98-40e7-a124-3b627512ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#players = pd.read_excel(\"Data/names_backup.csv\", sheet_name='names').replace(np.nan, None)#, index_col='playerId').replace(np.nan, None)\n",
    "#updates, adds, delete, Sheet2\n",
    "players = pd.read_excel(\"Data/adds.xlsx\", sheet_name='Sheet1')\n",
    "players = players.replace(np.nan, None).replace(0,None)\n",
    "players.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e72a78-de5d-4897-87b7-aaebb4987d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query_insert = \"\"\"\n",
    "        INSERT INTO player \n",
    "            (posId, teamId, name, espnId, espnName, bpId, bpName, nflId, nflName, fpId, fpName, cbsId, cbsName)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"    \n",
    "    query_update = \"\"\"\n",
    "        REPLACE INTO player \n",
    "            (playerId, posId, teamId, name, espnId, espnName, bpId, bpName, nflId, nflName, fpId, fpName, cbsId, cbsName)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\" \n",
    "    query_update1 = \"\"\"\n",
    "        UPDATE player \n",
    "        SET \n",
    "            mflId = %s, sportradarId = %s, gsisId = %s, pffId = %s,  sleeperId = %s, yahooId = %s, \n",
    "            fleaflickerId = %s, rotowireId = %s, rotoworldId = %s,  ktcId = %s, pfrId = %s, \n",
    "            cfbrefId = %s, statsId = %s, statsglobalId = %s, fantasydataId = %s, swishId = %s\n",
    "        WHERE playerId = %s\n",
    "    \"\"\"\n",
    "    \n",
    "    query_insert1 = \"\"\"\n",
    "        INSERT INTO player \n",
    "            (posId, teamId, cbsId, cbsName, name)\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    query_delete = '''\n",
    "        DELETE FROM player\n",
    "        WHERE playerId = %s\n",
    "    '''\n",
    "    \n",
    "    query_update = '''\n",
    "        UPDATE projection SET playerID = %s WHERE playerId = %s\n",
    "    '''\n",
    "    \n",
    "    cursor.executemany(query_update1, players.values.tolist())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443c280-a91d-4eb2-adde-eb12ea04c7e6",
   "metadata": {},
   "source": [
    "## ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee932b55-72eb-42b3-8f76-b77ee492034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_load_adp = pd.concat([df_load_adp, temp])\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_load_adp = pd.concat([df_load_adp, temp])\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
      "C:\\Users\\jrbrz\\AppData\\Local\\Temp\\ipykernel_11700\\1599444925.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  temp['outlet'] = temp['outlet'].replace(outletLookup)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    # columns for db table adp\n",
    "    adp_columns = ['outlet', 'date', 'playerId', 'adp','high','low']\n",
    "\n",
    "    #main df to hold all data\n",
    "    df_load_adp = pd.DataFrame(columns = adp_columns)\n",
    "\n",
    "    directory = r\"..\\data\\adp\"\n",
    "    #looping through every ADP file to aggregate into single df\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory,filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            temp = pd.read_excel(f, parse_dates=['date'])\n",
    "\n",
    "            # creating dicts to convert outlet name/id to db id\n",
    "            if 'cbs' in f:\n",
    "\n",
    "                # creating cbs ID to db ID map\n",
    "                lookup = pd.read_sql(\"SELECT playerId, cbsId FROM player WHERE cbsId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup['playerId'].values, index=lookup.cbsId).to_dict()\n",
    "\n",
    "            elif 'espn' in f:\n",
    "                # espn source data does not have playerId for the defenses\n",
    "                # converting full team name to db table 'TEAM'.name\n",
    "                lookup = pd.read_sql(\"SELECT espnId, name FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup.espnId.values, index=lookup.name).to_dict()\n",
    "\n",
    "                # updating the player table team name to player table Id\n",
    "                temp['team'] = temp['team'].str.upper()\n",
    "                temp.loc[temp['playerId'].isna(), 'playerId'] = temp.loc[temp['playerId'].isna(), 'team'].map(lookup)\n",
    "\n",
    "                lookup = pd.read_sql(\"SELECT espnId, playerId FROM player WHERE espnId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup.playerId.values, index=lookup.espnId).to_dict()\n",
    "\n",
    "\n",
    "            elif 'fp' in f:\n",
    "\n",
    "                # updating the fantasy pro source data id to db id\n",
    "                temp = temp.loc[pd.notnull(temp['adp'])]\n",
    "\n",
    "                lookup = pd.read_sql(\"SELECT playerId, fpId FROM player WHERE fpId IS NOT NULL;\", conn)\n",
    "                lookup = pd.Series(lookup.playerId.values, index=lookup.fpId).to_dict()\n",
    "\n",
    "\n",
    "\n",
    "            # using the lookup to make the change from outletId to dbId\n",
    "            temp['playerId'] = temp['playerId'].map(lookup)\n",
    "\n",
    "            # updating outlet name to db outlet id \n",
    "            temp['outlet'] = temp['outlet'].replace(outletLookup)\n",
    "\n",
    "\n",
    "            temp = temp[adp_columns]\n",
    "            # adding outlet dataframe to the upload dataframe\n",
    "            df_load_adp = pd.concat([df_load_adp, temp])\n",
    "\n",
    "    df_load_adp = df_load_adp.replace(np.nan, None)\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b71b04-c152-4d56-9cbe-9f77b9e9392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Connection' object has no attribute 'cursor'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(dbConnectionString)\n",
    "    conn = engine.connect()\n",
    "    conn = mysql.connector.connect(user=dbUser, password=dbPw,\n",
    "                              host=dbHost,database=dbName)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    query_insert = \"\"\"\n",
    "        INSERT INTO adp \n",
    "\n",
    "        VALUES \n",
    "            (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.executemany(query_insert, df_load_adp.values.tolist())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as ex:\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2969b-601c-4169-9101-784ab9394997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fantasyfootball-maF7LdMm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
